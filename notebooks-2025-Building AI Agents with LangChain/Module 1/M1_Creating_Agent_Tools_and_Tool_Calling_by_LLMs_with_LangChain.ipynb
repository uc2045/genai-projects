{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring Tools in LangChain"
      ],
      "metadata": {
        "id": "-CVPAiNAy9MH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install OpenAI, and LangChain dependencies"
      ],
      "metadata": {
        "id": "L1KvMtf54l0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from warnings import filterwarnings\n",
        "filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "yXgPP-n3lDy6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.3.14\n",
        "!pip install langchain-openai==0.3.0\n",
        "!pip install langchain-community==0.3.14"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2evPp14fy258",
        "outputId": "ab34056e-c484-42bd-f1b7-ae0a05c43059"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain==0.3.14 in /usr/local/lib/python3.11/dist-packages (0.3.14)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.14) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.14) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.14) (3.11.11)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.14) (0.3.29)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.14) (0.3.5)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.14) (0.2.10)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.14) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.14) (2.10.5)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.14) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.14) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.14) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.14) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.14) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.14) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.14) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.14) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.14) (1.18.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain==0.3.14) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain==0.3.14) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain==0.3.14) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.17->langchain==0.3.14) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.17->langchain==0.3.14) (3.10.14)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.17->langchain==0.3.14) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.14) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.14) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.14) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.14) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.14) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.14) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.14) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain==0.3.14) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain==0.3.14) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain==0.3.14) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain==0.3.14) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain==0.3.14) (1.3.1)\n",
            "Collecting langchain-openai==0.3.0\n",
            "  Downloading langchain_openai-0.3.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in /usr/local/lib/python3.11/dist-packages (from langchain-openai==0.3.0) (0.3.29)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.11/dist-packages (from langchain-openai==0.3.0) (1.59.6)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai==0.3.0)\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-openai==0.3.0) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-openai==0.3.0) (1.33)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-openai==0.3.0) (0.2.10)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-openai==0.3.0) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-openai==0.3.0) (2.10.5)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-openai==0.3.0) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-openai==0.3.0) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai==0.3.0) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai==0.3.0) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai==0.3.0) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai==0.3.0) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai==0.3.0) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai==0.3.0) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.3.0) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.3.0) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain-openai==0.3.0) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai==0.3.0) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai==0.3.0) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai==0.3.0) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain-openai==0.3.0) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain-openai==0.3.0) (3.10.14)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain-openai==0.3.0) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.29->langchain-openai==0.3.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.29->langchain-openai==0.3.0) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.3.0) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.3.0) (2.3.0)\n",
            "Downloading langchain_openai-0.3.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, langchain-openai\n",
            "Successfully installed langchain-openai-0.3.0 tiktoken-0.8.0\n",
            "Collecting langchain-community==0.3.14\n",
            "  Downloading langchain_community-0.3.14-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.14) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.14) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.14) (3.11.11)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community==0.3.14)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community==0.3.14)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.14 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.14) (0.3.14)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.14) (0.3.29)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.14) (0.2.10)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.14) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community==0.3.14)\n",
            "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.14) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.14) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.14) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.14) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.14) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.14) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.14) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.14) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.14) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.14)\n",
            "  Downloading marshmallow-3.25.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.14)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.14->langchain-community==0.3.14) (0.3.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.14->langchain-community==0.3.14) (2.10.5)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-community==0.3.14) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-community==0.3.14) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-community==0.3.14) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community==0.3.14) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community==0.3.14) (3.10.14)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community==0.3.14) (1.0.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.14)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.3.14) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.3.14) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.3.14) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.3.14) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.3.14) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.14) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.14) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.14) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain-community==0.3.14) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.14->langchain-community==0.3.14) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.14->langchain-community==0.3.14) (2.27.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.14)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.14) (1.3.1)\n",
            "Downloading langchain_community-0.3.14-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading marshmallow-3.25.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.14 marshmallow-3.25.1 mypy-extensions-1.0.0 pydantic-settings-2.7.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Data Extraction APIs"
      ],
      "metadata": {
        "id": "TlfidBdQZRGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to create custom tools\n",
        "!pip install wikipedia==1.4.0\n",
        "!pip install markitdown\n",
        "# to highlight json\n",
        "!pip install rich"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZKQDgQURhmF",
        "outputId": "e965e299-4f80-4e20-a8c9-4186483f7b0a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia==1.4.0\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from wikipedia==1.4.0) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wikipedia==1.4.0) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (2024.12.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia==1.4.0) (2.6)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11679 sha256=e1ac9f0adbb200cd532ff6143e4cc96b6ca90cb760d4e6a5595e8bfb603b5acf\n",
            "  Stored in directory: /root/.cache/pip/wheels/8f/ab/cb/45ccc40522d3a1c41e1d2ad53b8f33a62f394011ec38cd71c6\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n",
            "Collecting markitdown\n",
            "  Downloading markitdown-0.0.1a3-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from markitdown) (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from markitdown) (3.4.1)\n",
            "Collecting mammoth (from markitdown)\n",
            "  Downloading mammoth-1.9.0-py2.py3-none-any.whl.metadata (24 kB)\n",
            "Collecting markdownify (from markitdown)\n",
            "  Downloading markdownify-0.14.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from markitdown) (1.26.4)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (from markitdown) (1.59.6)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (from markitdown) (3.1.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from markitdown) (2.2.2)\n",
            "Collecting pathvalidate (from markitdown)\n",
            "  Downloading pathvalidate-3.2.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pdfminer-six (from markitdown)\n",
            "  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting puremagic (from markitdown)\n",
            "  Downloading puremagic-1.28-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting pydub (from markitdown)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-pptx (from markitdown)\n",
            "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from markitdown) (2.32.3)\n",
            "Collecting speechrecognition (from markitdown)\n",
            "  Downloading SpeechRecognition-3.14.0-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting youtube-transcript-api (from markitdown)\n",
            "  Downloading youtube_transcript_api-0.6.3-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->markitdown) (2.6)\n",
            "Collecting cobble<0.2,>=0.1.3 (from mammoth->markitdown)\n",
            "  Downloading cobble-0.1.4-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: six<2,>=1.15 in /usr/local/lib/python3.11/dist-packages (from markdownify->markitdown) (1.17.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai->markitdown) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai->markitdown) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai->markitdown) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai->markitdown) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai->markitdown) (2.10.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai->markitdown) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai->markitdown) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai->markitdown) (4.12.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl->markitdown) (2.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->markitdown) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->markitdown) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->markitdown) (2024.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer-six->markitdown) (43.0.3)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.11/dist-packages (from python-pptx->markitdown) (11.1.0)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx->markitdown)\n",
            "  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-pptx->markitdown) (5.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->markitdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->markitdown) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->markitdown) (2024.12.14)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from youtube-transcript-api->markitdown) (0.7.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer-six->markitdown) (1.17.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai->markitdown) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->markitdown) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai->markitdown) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai->markitdown) (2.27.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer-six->markitdown) (2.22)\n",
            "Downloading markitdown-0.0.1a3-py3-none-any.whl (16 kB)\n",
            "Downloading mammoth-1.9.0-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.9/52.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdownify-0.14.1-py3-none-any.whl (11 kB)\n",
            "Downloading pathvalidate-3.2.3-py3-none-any.whl (24 kB)\n",
            "Downloading pdfminer.six-20240706-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading puremagic-1.28-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SpeechRecognition-3.14.0-py3-none-any.whl (32.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading youtube_transcript_api-0.6.3-py3-none-any.whl (622 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.3/622.3 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cobble-0.1.4-py3-none-any.whl (4.0 kB)\n",
            "Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, puremagic, XlsxWriter, speechrecognition, pathvalidate, cobble, youtube-transcript-api, python-pptx, markdownify, mammoth, pdfminer-six, markitdown\n",
            "Successfully installed XlsxWriter-3.2.0 cobble-0.1.4 mammoth-1.9.0 markdownify-0.14.1 markitdown-0.0.1a3 pathvalidate-3.2.3 pdfminer-six-20240706 puremagic-1.28 pydub-0.25.1 python-pptx-1.0.2 speechrecognition-3.14.0 youtube-transcript-api-0.6.3\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (13.9.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enter Open AI API Key"
      ],
      "metadata": {
        "id": "H9c37cLnSrbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "OPENAI_KEY = getpass('Enter Open AI API Key: ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cv3JzCEx_PAd",
        "outputId": "d2368589-e313-4cc8-909c-9399e147ef1d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Open AI API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enter Tavily Search API Key\n",
        "\n",
        "Get a free API key from [here](https://tavily.com/#api)"
      ],
      "metadata": {
        "id": "ucWRRI3QztL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TAVILY_API_KEY = getpass('Enter Tavily Search API Key: ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mK-1WLzOrJdb",
        "outputId": "df080905-75fc-4b9d-96bd-562bce8afa84"
      },
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Tavily Search API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enter WeatherAPI API Key\n",
        "\n",
        "Get a free API key from [here](https://www.weatherapi.com/signup.aspx)"
      ],
      "metadata": {
        "id": "Ce5arICZEEov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WEATHER_API_KEY = getpass('Enter WeatherAPI API Key: ')"
      ],
      "metadata": {
        "id": "XpAMz1XgEEov",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a18b0a1-ffea-4fbf-f185-d9116fbaf6b2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter WeatherAPI API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Environment Variables"
      ],
      "metadata": {
        "id": "1T0s0um5Svfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_KEY\n",
        "os.environ['TAVILY_API_KEY'] = TAVILY_API_KEY"
      ],
      "metadata": {
        "id": "x1YSuHNF_lbh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring Built-in Tools"
      ],
      "metadata": {
        "id": "65C3PellZGYf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring the Wikipedia Tool\n",
        "\n",
        "Enables you to tap into the Wikipedia API to search wikipedia pages for information"
      ],
      "metadata": {
        "id": "howf-v0ARWbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools import WikipediaQueryRun\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "\n",
        "wiki_api_wrapper = WikipediaAPIWrapper(top_k_results=3,\n",
        "                                       doc_content_chars_max=8000)\n",
        "wiki_tool = WikipediaQueryRun(api_wrapper=wiki_api_wrapper, features=\"lxml\")"
      ],
      "metadata": {
        "id": "q2CMhK9Rjk2t"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_tool.description"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "t1Ce8wbYodYO",
        "outputId": "dc4ad085-54a8-450f-d32a-bb261bdfc362"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_tool.args"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2MSVAh2osSE",
        "outputId": "02e23a35-bf58-453b-adf3-64fa5536588c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': {'description': 'query to look up on wikipedia',\n",
              "  'title': 'Query',\n",
              "  'type': 'string'}}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wiki_tool.invoke({\"query\": \"Microsoft\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luhjlzeSkgUq",
        "outputId": "0c781ee6-9a3e-459c-969d-54f6dbe72251"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page: Microsoft\n",
            "Summary: Microsoft Corporation is an American multinational technology conglomerate headquartered in Redmond, Washington. Founded in 1975, the company became highly influential in the rise of personal computers through software like Windows, and the company has since expanded to Internet services, cloud computing, video gaming and other fields. Microsoft is the largest software maker, one of the most valuable public U.S. companies, and one of the most valuable brands globally.\n",
            "Microsoft was founded by Bill Gates and Paul Allen to develop and sell BASIC interpreters for the Altair 8800. It rose to dominate the personal computer operating system market with MS-DOS in the mid-1980s, followed by Windows. During the 41 years from 1980 to 2021 Microsoft released 9 versions of MS-DOS with a median frequency of 2 years, and 13 versions of Microsoft Windows with a median frequency of 3 years. The company's 1986 initial public offering (IPO) and subsequent rise in its share price created three billionaires and an estimated 12,000 millionaires among Microsoft employees. Since the 1990s, it has increasingly diversified from the operating system market. Steve Ballmer replaced Gates as CEO in 2000 which would see the then-largest of Microsoft's corporate acquisitions in Skype Technologies in 2011, and an increased focus on hardware that led to its first in-house PC line, the Surface, in 2012, and the formation of Microsoft Mobile through Nokia. Since Satya Nadella took over as CEO in 2014, the company has changed focus towards cloud computing, as well as its large acquisition of LinkedIn for $26.2 billion in 2016. Under Nadella's direction, the company has also expanded its video gaming business to support the Xbox brand, establishing the Microsoft Gaming division in 2022, which is currently the third-largest gaming company in the world by revenue, following the 2023 acquisition of Activision Blizzard for $68.7 billion.\n",
            "Microsoft has been market-dominant in the IBM PC–compatible operating system market and the office software suite market since the 1990s. Its best-known software products are the Windows line of operating systems and the Microsoft Office and Microsoft 365 suite of productivity applications, which most notably include the Word word processor and Excel spreadsheet editor. Its flagship hardware products are the Surface lineup of personal computers and Xbox video game consoles, the latter of which includes the Xbox network; the company also provides a range of consumer Internet services such as Bing web search, the MSN web portal, the Outlook.com email service and the Microsoft Store. In the enterprise and development fields, Microsoft most notably provides the Azure cloud computing platform, Microsoft SQL Server database software, and Visual Studio.\n",
            "Microsoft is considered one of the Big Five American information technology companies, alongside Alphabet, Amazon, Apple, and Meta. In April 2019, Microsoft reached a trillion-dollar market cap, becoming the third public U.S. company to be valued at over $1 trillion. It has been criticized for its monopolistic practices, and the company's software has been criticized for problems with ease of use, robustness, and security.\n",
            "\n",
            "\n",
            "\n",
            "Page: Microsoft Windows\n",
            "Summary: Windows is a product line of proprietary graphical operating systems developed and marketed by Microsoft. It is grouped into families and subfamilies that cater to particular sectors of the computing industry – Windows (unqualified) for a consumer or corporate workstation, Windows Server for a server and Windows IoT for an embedded system. Windows is sold as either a consumer retail product or licensed to third-party hardware manufacturers who sell products bundled with Windows.\n",
            "The first version of Windows, Windows 1.0, was released on November 20, 1985, as a graphical operating system shell for MS-DOS in response to the growing interest in graphical user interfaces (GUIs). The name \"Windows\" is a reference to the windowing system in GUIs. The 1990 release of Windows 3.0 catapulted its market success and led to various other product families, including the now-defunct Windows 9x, Windows Mobile, Windows Phone, and Windows CE/Embedded Compact. Windows is the most popular desktop operating system in the world, with a 70% market share as of March 2023, according to StatCounter; however when including mobile OSes, it is not the most used, in favor of Android.\n",
            "The most recent version of Windows is Windows 11 for consumer PCs and tablets, Windows 11 Enterprise for corporations, and Windows Server 2025 for servers. Still supported are some editions of Windows 10, Windows Server 2016 or later (and exceptionally with paid support down to Windows Server 2008).\n",
            "\n",
            "Page: Microsoft Word\n",
            "Summary: Microsoft Word is a word processing program developed by Microsoft. It was first released on October 25, 1983, under the name Multi-Tool Word for Xenix systems. Subsequent versions were later written for several other platforms including: IBM PCs running DOS (1983), Apple Macintosh running the Classic Mac OS (1985), AT&T UNIX PC (1985), Atari ST (1988), OS/2 (1989), Microsoft Windows (1989), SCO Unix (1990), Handheld PC (1996), Pocket PC (2000), macOS (2001), Web browsers (2010), iOS (2014), and Android (2015).\n",
            "Microsoft Word has been the de facto standard word processing software since the 1990s when it eclipsed WordPerfect. Commercial versions of Word are licensed as a standalone product or as a component of Microsoft Office, which can be purchased with a perpetual license, or as part of the Microsoft 365 suite as a subscription.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wiki_tool.invoke({\"query\": \"AI\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jglV7GRXjk5O",
        "outputId": "c5982698-fe63-4d74-faa0-f6b5b7ffd2d5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page: Artificial intelligence\n",
            "Summary: Artificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. Such machines may be called AIs.\n",
            "High-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., ChatGPT and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"\n",
            "Various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics. General intelligence—the ability to complete any task performed by a human on an at least equal level—is among the field's long-term goals. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.\n",
            "Artificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters. Funding and interest vastly increased after 2012 when deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture, and by the early 2020s many billions of dollars were being invested in AI and the field experienced rapid ongoing progress in what has become known as the AI boom. The emergence of advanced generative AI in the midst of the AI boom and its ability to create and modify content exposed several unintended consequences and harms in the present and raised concerns about the risks of AI and its long-term effects in the future, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.\n",
            "\n",
            "\n",
            "\n",
            "Page: Perplexity AI\n",
            "Summary: Perplexity AI is a conversational search engine that uses large language models (LLMs) to answer queries using sources from the web and cites links within the text response. Its developer, Perplexity AI, Inc., is based in San Francisco, California.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " You can customize the default tool with its own name, description and so on as follows"
      ],
      "metadata": {
        "id": "LnfMeoXJVn-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import Tool\n",
        "\n",
        "wiki_tool_init = Tool(name=\"Wikipedia\",\n",
        "                      func=wiki_api_wrapper.run,\n",
        "                      description=\"useful when you need a detailed answer about general knowledge\")"
      ],
      "metadata": {
        "id": "1tO5g9Q1jk7x"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_tool_init.description"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZRTfVUuGo7ti",
        "outputId": "e7f5b9e0-d851-472b-ab66-cff39b04f84c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'useful when you need a detailed answer about general knowledge'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_tool_init.args"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TW0emMEIou-L",
        "outputId": "e0fba8f6-1da1-4791-e4af-8913e4f38245"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'tool_input': {'type': 'string'}}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wiki_tool_init.invoke({\"tool_input\": \"AI\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgAwdPrBjk-x",
        "outputId": "fe3765d3-356d-4545-af7f-3a65076578ea"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page: Artificial intelligence\n",
            "Summary: Artificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. Such machines may be called AIs.\n",
            "High-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., ChatGPT and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"\n",
            "Various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics. General intelligence—the ability to complete any task performed by a human on an at least equal level—is among the field's long-term goals. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.\n",
            "Artificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters. Funding and interest vastly increased after 2012 when deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture, and by the early 2020s many billions of dollars were being invested in AI and the field experienced rapid ongoing progress in what has become known as the AI boom. The emergence of advanced generative AI in the midst of the AI boom and its ability to create and modify content exposed several unintended consequences and harms in the present and raised concerns about the risks of AI and its long-term effects in the future, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.\n",
            "\n",
            "\n",
            "\n",
            "Page: Perplexity AI\n",
            "Summary: Perplexity AI is a conversational search engine that uses large language models (LLMs) to answer queries using sources from the web and cites links within the text response. Its developer, Perplexity AI, Inc., is based in San Francisco, California.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring the Tavily Search Tool\n",
        "\n",
        "Tavily Search API is a search engine optimized for LLMs and RAG, aimed at efficient, quick and persistent search results"
      ],
      "metadata": {
        "id": "cnQfnkQeV7Hp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "tavily_tool = TavilySearchResults(max_results=8,\n",
        "                                search_depth='advanced',\n",
        "                                include_raw_content=True)"
      ],
      "metadata": {
        "id": "UjWM95p5pB4k"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tavily_tool.args"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmawzibmjlC6",
        "outputId": "ccd9cdcc-7ece-42bb-ca1a-b6dc0d727d72"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': {'description': 'search query to look up',\n",
              "  'title': 'Query',\n",
              "  'type': 'string'}}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tavily_tool.description"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "eERoFFoPjlGJ",
        "outputId": "a14facad-3ba4-4a0f-a68b-34d12658fc79"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = tavily_tool.invoke(\"Tell me about Microsoft\")\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khqWQVcvrnG9",
        "outputId": "287a0237-f323-4e40-9d35-3df6d100515f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'url': 'https://news.microsoft.com/facts-about-microsoft/',\n",
              "  'content': 'Corporate Address\\nImportant Dates\\nBoard of Directors\\nMicrosoft Business Organization\\nOperation Centers\\nMicrosoft Subsidiaries\\nFinancial Data\\nEmployment Information\\nReal Estate Portfolio\\nAbout Microsoft\\nCorporate Address\\nMicrosoft Corporation\\nOne Microsoft Way\\nRedmond, WA 98052-7329\\nUSA\\nTel: (425) 882-8080\\nFax: (425) 706-7329\\nhttp://www.microsoft.com\\nImportant Dates\\nAs of Sept. 30, 2023\\nBoard of Directors\\nAs of Dec. 21, 2023\\nMicrosoft Business Organization\\n Last updated: Dec. 21, 2023\\nOperation Centers\\nAs of June 30, 2022\\nMicrosoft Subsidiaries\\nAs of Sept. 30, 2023Â\\nCurrent employment headcount\\nAs of June 30, 2023\\nAdditional information and details about the demographics of our workforce can be found on the Microsoft Global Diversity & Inclusion site.\\n Global\\nFacts About Microsoft\\nMicrosoft enables digital transformation for the era of an intelligent cloud and an intelligent edge. Press Tools\\nInnovation Stories\\nDigital Transformation\\nFollow us:\\n About Microsoft\\nMicrosoft (Nasdaq â€œMSFTâ€� @microsoft) enables digital transformation for the era of an intelligent cloud and an intelligent edge.'},\n",
              " {'url': 'https://news.microsoft.com/about/',\n",
              "  'content': 'New Redmond campus plan unveiled\\nMicrosoft announced a major overhaul of its 500-acre campus in Redmond, Washington, that will add 17 new buildings, 6.7 million square feet of revamped work space, trails and sports fields in four â€œvillages.â€�\\nâ€˜Hit Refreshâ€™ is published\\nSatya Nadellaâ€™s first book, â€œHit Refresh: The Quest to Rediscover Microsoftâ€™s Soul and Imagine a Better Future for Everyone,â€� told the CEOâ€™s personal journey from his childhood in India to the helm of one of the worldâ€™s most well-known companies.\\n First version of Windows announced\\nBill Gates announced Microsoft Windows, a graphical user interface for its MS-DOS operating system that he described as â€œunique software designed for the serious PC user.â€�\\nMicrosoft is born\\nInspired by the January cover of Popular Electronics magazine, friends Bill Gates and Paul Allen started Microsoft â€“ sometimes Micro-Soft, for microprocessors and software â€“ to develop software for the Altair 8800, an early personal computer.\\n Global\\nAbout Microsoft\\nExplore a timeline of Microsoftâ€™s journey\\nDigital skills for 25 million people\\nMicrosoft committed to helping 25 million people worldwide acquire the new digital skills needed in a COVID-19 economy.\\n Office expands to the cloud\\nMicrosoft took its flagship suite of productivity and collaboration services to the cloud with the launch of Office 365.\\n Microsoft buys LinkedIn\\nMicrosoftâ€™s $26.2-billion acquisition of LinkedIn aimed to grow the professional networking site and integrate it with Microsoftâ€™s enterprise software, such as Office 365.\\n'},\n",
              " {'url': 'https://www.britannica.com/money/Microsoft-Corporation',\n",
              "  'content': 'Microsoft Corporation is a leading developer of computer software, operating systems, cloud computing, and artificial intelligence'},\n",
              " {'url': 'https://www.quora.com/What-is-so-great-about-Microsoft',\n",
              "  'content': 'It is known for its user-friendly interface, widespread compatibility, and constant updates to meet user needs. Cloud Computing (Azure):'},\n",
              " {'url': 'https://www.microsoft.com/en-us/about',\n",
              "  'content': 'Microsoft Microsoft AI Microsoft Copilot All Microsoft Microsoft 365 Microsoft Security Microsoft 365 for business Microsoft Learn ![Image 5: Three people are smiling and talking around a table with a laptop, with the text \"How can we expand opportunity?\" displayed.](https://cdn-dynmedia-1.microsoft.com/is/image/microsoftcorp/Impact-Summary-Cross-Expand-Opportunity-834x470?wid=406&hei=230&fit=crop) ![Image 6: A man points to a digital world map on a screen, with the question \"How can we earn trust?\" displayed.](https://cdn-dynmedia-1.microsoft.com/is/image/microsoftcorp/Impact-Summary-Cross-Earn-Trust-834x470?wid=406&hei=230&fit=crop) Microsoft Copilot Explore careers at Microsoft ![Image 14: Three female coworkers sitting at a table talking and while looking at a Surface computer and tablet](https://cdn-dynmedia-1.microsoft.com/is/image/microsoftcorp/About-Diversity-Inclusion-three-coworkers-400x400?wid=425&hei=425) Explore Microsoft Microsoft Support Microsoft Copilot Microsoft in education Microsoft 365 Education Microsoft Security Microsoft 365 Microsoft 365 Copilot Microsoft Learn About Microsoft Privacy at Microsoft'},\n",
              " {'url': 'https://en.wikipedia.org/wiki/Microsoft',\n",
              "  'content': 'The European Union imposed another fine of €899\\xa0million ($1.4\\xa0billion) for Microsoft\\'s lack of compliance with the March 2004 judgment on February 27, 2008, saying that the company charged rivals unreasonable prices for key information about its workgroup and backoffice servers.[61] Microsoft stated that it was in compliance and that \"these fines are about the past issues that have been resolved\".[62] 2007 also saw the creation of a multi-core unit at Microsoft, following the steps of server companies such as Sun and IBM.[63]\\nGates retired from his role as Chief Software Architect on June 27, 2008, a decision announced in June 2006, while retaining other positions related to the company in addition to being an advisor for the company on key projects.[64][65] Microsoft eventually became the leading PC operating systems vendor.[23][24]:\\u200a210\\u200a The company expanded into new markets with the release of the Microsoft Mouse in 1983, as well as with a publishing division named Microsoft Press.[13]:\\u200a232\\nPaul Allen resigned from Microsoft in 1983 after developing Hodgkin\\'s lymphoma.[25] Allen claimed in Idea Man: A Memoir by the Co-founder of Microsoft that Gates wanted to dilute his share in the company when he was diagnosed with Hodgkin\\'s disease because he did not think that he was working hard enough.[26] Allen later invested in low-tech sectors, sports teams, commercial real estate, neuroscience, private space flight, and more.[27]\\n1985–1994: Windows and Office\\nMicrosoft released Windows on November 20, 1985, as a graphical extension for MS-DOS,[13]:\\u200a242–243,\\u200a246\\u200a despite having begun jointly developing OS/2 with IBM the previous August.[28] These committees include the Audit Committee, which handles accounting issues with the company including auditing and reporting; the Compensation Committee, which approves compensation for the CEO and other employees of the company; the Governance and Nominating Committee, which handles various corporate matters including the nomination of the board; and the Regulatory and Public Policy Committee, which includes legal/antitrust matters, along with privacy, trade, digital safety, artificial intelligence, and environmental sustainability.[167]\\nOn March 13, 2020, Gates announced that he is leaving the board of directors of Microsoft and Berkshire Hathaway to focus more on his philanthropic efforts. On July 22, 2020, Microsoft announced plans to close its Mixer service, planning to move existing partners to Facebook Gaming.[133]\\nOn July 31, 2020, it was reported that Microsoft was in talks to acquire TikTok after the Trump administration ordered ByteDance to divest ownership of the application to the U.S.[134] On August 3, 2020, after speculation on the deal, Donald Trump stated that Microsoft could buy the application, however, it should be completed by September 15, 2020, and that the United States Department of the Treasury should receive a portion if it were to go through.[135]\\nOn August 5, 2020, Microsoft stopped its xCloud game streaming test for iOS devices. In October 1997, the Justice Department filed a motion in the Federal District Court, stating that Microsoft violated an agreement signed in 1994 and asked the court to stop the bundling of Internet Explorer with Windows.[13]:\\u200a323–324\\nOn January 13, 2000, Bill Gates handed over the CEO position to Steve Ballmer, an old college friend of Gates and employee of the company since 1980, while creating a new position for himself as Chief Software Architect.[13]:\\u200a111,\\u200a228\\u200a[17] Various companies including Microsoft formed the Trusted Computing Platform Alliance in October 1999 to (among other things) increase security and protect intellectual property through identifying changes in hardware and software.'},\n",
              " {'url': 'https://news.microsoft.com/announcement/microsoft-is-born/',\n",
              "  'content': 'Microsoft is born - Stories Microsoft Microsoft On The Issues Inclusion is Innovation All Microsoft Microsoft 365 Surface Microsoft Teams Microsoft Edge Microsoft Cloud Microsoft Security Microsoft 365 for business Microsoft Industry Microsoft Learn Microsoft Rewards Microsoft is born Share on Facebook (opens new window) Microsoft released Windows in 1985, a year before moving its headquarters to Redmond, Washington. By the late 1980s, Microsoft was the world’s largest personal-computer software company. ### Microsoft by the Numbers ### Facts About Microsoft Microsoft Copilot Explore Microsoft products Microsoft Store Microsoft Store support Microsoft in education Microsoft Teams for Education Microsoft 365 Education Microsoft Cloud Microsoft Security Microsoft 365 Microsoft Teams Microsoft 365 Copilot Microsoft Learn About Microsoft Privacy at Microsoft Contact Microsoft © Microsoft 2024'},\n",
              " {'url': 'https://support.microsoft.com/en-us/office/what-is-microsoft-365-847caf12-2589-452c-8aca-1c009797678b',\n",
              "  'content': 'Microsoft 365 is a cloud-powered productivity platform. A subscription to Microsoft 365 provides all of the following: The latest productivity apps,'}]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build your own tools in LangChain\n",
        "\n",
        "Tools are interfaces that an agent, chain, or LLM can use to interact with the world. They combine a few things:\n",
        "\n",
        "- The name of the tool\n",
        "- A description of what the tool is\n",
        "- JSON schema of what the inputs to the tool are\n",
        "- The function to call\n",
        "- Whether the result of a tool should be returned directly to the user\n",
        "\n",
        "It is useful to have all this information because this information can be used to build action-taking systems! The name, description, and JSON schema can be used to prompt the LLM so it knows how to specify what action to take, and then the function to call is equivalent to taking that action."
      ],
      "metadata": {
        "id": "-GNP2J9Dd_nj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building a Simple Math Tool"
      ],
      "metadata": {
        "id": "ZtoArDYfheYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will start by building a simple tool which does some basic math"
      ],
      "metadata": {
        "id": "JvPAmW62eLyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def multiply(a, b):\n",
        "    \"\"\"Multiply two numbers.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "\n",
        "# Let's inspect some of the attributes associated with the tool.\n",
        "print(multiply.name)\n",
        "print(multiply.description)\n",
        "print(multiply.args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wa0mztbQ8Ae3",
        "outputId": "0a94a594-dcec-4710-8756-74f96200d7d2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multiply\n",
            "Multiply two numbers.\n",
            "{'a': {'title': 'A'}, 'b': {'title': 'B'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(multiply)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "tfP2TjeK8FsR",
        "outputId": "60859e65-098c-4ebf-aeb4-69bbf29fcb6e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain_core.tools.structured.StructuredTool"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.tools.structured.StructuredTool</b><br/>def warning_emitting_wrapper(*args: Any, **kwargs: Any) -&gt; Any</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/langchain_core/tools/structured.py</a>Tool that can operate on any number of inputs.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 32);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multiply.invoke({\"a\": 2, \"b\": 3})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsOVBIvX8Agu",
        "outputId": "c30cfbac-b30c-466d-ddaa-0c3b3974f030"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multiply.invoke({\"a\": 2.1, \"b\": 3.2})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cC7o_Mv5-Grc",
        "outputId": "b7ad7932-9a39-465d-88ef-3cff9d508021"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.720000000000001"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multiply.invoke({\"a\": 2, \"b\": 'abc'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vN4tHdDa-Tm2",
        "outputId": "49359bb5-6ae6-4b03-993a-ae04b100a67a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'abcabc'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now build a tool with data type enforcing"
      ],
      "metadata": {
        "id": "XED8giGYeS_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.tools import StructuredTool\n",
        "\n",
        "class CalculatorInput(BaseModel):\n",
        "    a: float = Field(description=\"first number\")\n",
        "    b: float = Field(description=\"second number\")\n",
        "\n",
        "\n",
        "def multiply(a: float, b: float) -> float:\n",
        "    \"\"\"Multiply two numbers.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "# we could also use the @tool decorator from before\n",
        "multiply = StructuredTool.from_function(\n",
        "    func=multiply,\n",
        "    name=\"multiply\",\n",
        "    description=\"use to multiply numbers\",\n",
        "    args_schema=CalculatorInput,\n",
        "    return_direct=True\n",
        "    )\n",
        "\n",
        "# Let's inspect some of the attributes associated with the tool.\n",
        "print(multiply.name)\n",
        "print(multiply.description)\n",
        "print(multiply.args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sKF2zqQ8Aih",
        "outputId": "7508ee9f-3492-4386-9be5-cf870b690d09"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multiply\n",
            "use to multiply numbers\n",
            "{'a': {'description': 'first number', 'title': 'A', 'type': 'number'}, 'b': {'description': 'second number', 'title': 'B', 'type': 'number'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multiply.invoke({\"a\": 2, \"b\": 3})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5taFxZg8Akc",
        "outputId": "c20364a3-8405-4713-f963-d42bdee4b688"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.0"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this code will error out as abc is not a floating point number\n",
        "multiply.invoke({\"a\": 2, \"b\": 'abc'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "3PTl9ezG8Ans",
        "outputId": "2c769158-1a30-4991-8211-0a7471acd4a2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "1 validation error for CalculatorInput\nb\n  Input should be a valid number, unable to parse string as a number [type=float_parsing, input_value='abc', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/float_parsing",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-e6eaaafa51ca>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# this code will error out as abc is not a floating point number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmultiply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'abc'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/tools/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m     ) -> Any:\n\u001b[1;32m    483\u001b[0m         \u001b[0mtool_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prep_run_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtool_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m     async def ainvoke(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/tools/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[0m\n\u001b[1;32m    723\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merror_to_raise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_tool_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_to_raise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_to_raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_format_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martifact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtool_call_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_tool_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/tools/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_set_config_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0mtool_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtool_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_args_and_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtool_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtool_call_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m                 \u001b[0mtool_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtool_kwargs\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/tools/base.py\u001b[0m in \u001b[0;36m_to_args_and_kwargs\u001b[0;34m(self, tool_input, tool_call_id)\u001b[0m\n\u001b[1;32m    609\u001b[0m             \u001b[0;31m# StructuredTool with no args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0mtool_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtool_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtool_call_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m         \u001b[0;31m# For backwards compatibility, if run_input is a string,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;31m# pass as a positional argument.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/tools/base.py\u001b[0m in \u001b[0;36m_parse_input\u001b[0;34m(self, tool_input, tool_call_id)\u001b[0m\n\u001b[1;32m    530\u001b[0m                                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m                             \u001b[0mtool_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtool_call_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtool_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m                     \u001b[0mresult_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseModelV1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36mmodel_validate\u001b[0;34m(cls, obj, strict, from_attributes, context)\u001b[0m\n\u001b[1;32m    625\u001b[0m         \u001b[0;31m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m         \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m         return cls.__pydantic_validator__.validate_python(\n\u001b[0m\u001b[1;32m    628\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_attributes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_attributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         )\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for CalculatorInput\nb\n  Input should be a valid number, unable to parse string as a number [type=float_parsing, input_value='abc', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/float_parsing"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build a Web Search & Information Extraction Tool"
      ],
      "metadata": {
        "id": "QMsvq9sVhjuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tavily_tool = TavilySearchResults(max_results=5,\n",
        "                                  search_depth='advanced',\n",
        "                                  include_raw_content=True)\n",
        "\n",
        "result = tavily_tool.invoke(\"Tell me about Microsoft's Q4 2024 earning call report\")\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cazBMQsghoaB",
        "outputId": "e5c7d31e-1197-46da-b0ac-20ff234e19b0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'url': 'https://www.gurufocus.com/news/2486529/microsoft-corp-msft-q4-2024-earnings-call-transcript-highlights-strong-cloud-growth-and-record-revenue',\n",
              "  'content': 'Release Date: July 30, 2024. For the complete transcript of the earnings call, please refer to the full earnings call transcript. Positive Points . Microsoft Corp (MSFT, Financial) reported annual revenue of over $245 billion, up 15% year over year. Microsoft Cloud revenue surpassed $135 billion, up 23% year over year.'},\n",
              " {'url': 'https://www.marketbeat.com/earnings/reports/2024-7-30-microsoft-co-stock/',\n",
              "  'content': 'In our largest quarter of the year, we again delivered double-digit top and bottom-line growth with continued share gains across many of our businesses and record commitments to our Microsoft Cloud platform. Office 365 Commercial revenue increased 13% and 14% in constant currency with ARPU growth primarily from E5 momentum as well as Copilot for Microsoft 365. Azure and other cloud services revenue grew 29% and 30% in constant currency, in line with expectations and consistent with Q3 when adjusting for the leap year. In Windows Commercial products and Cloud services, customer demand for Microsoft 365 and our advanced security solutions should drive revenue growth in the mid-single digits.'},\n",
              " {'url': 'https://www.gurufocus.com/news/2486427/q4-2024-microsoft-corp-earnings-call-transcript',\n",
              "  'content': 'Jul 30, 2024 / 09:30PM GMT Operator Greetings, and welcome to the Microsoft fiscal year 2024 fourth-quarter earnings conference Call. (Operator Instructions) As a reminder, this conference is being recorded. I would now like to turn the conference over to your host, Brett Iversen, Vice President of Investor Relations.'},\n",
              " {'url': 'https://www.gurufocus.com/stock/MSFT/transcripts/2486427',\n",
              "  'content': 'Jul 30, 2024 / 09:30PM GMTOperator Greetings, and welcome to the Microsoft fiscal year 2024 fourth-quarter earnings conference Call. (Operator Instructions) As 🚀 Enjoy a 7-Day Free Trial Thru Nov 25, 2024!'},\n",
              " {'url': 'https://www.microsoft.com/en-us/investor/earnings/FY-2024-Q4/press-release-webcast',\n",
              "  'content': 'Microsoft Cloud Strength Drives Fourth Quarter Results. REDMOND, Wash. — July 30, 2024 — Microsoft Corp. today announced the following results for the quarter ended June 30, 2024, as compared to the corresponding period of last fiscal year: · Revenue was $64.7 billion and increased 15% (up 16% in constant currency) · Operating income was $27.9 billion and increased 15% (up 16% in'}]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result[0]['url']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xZ-WELThiwjK",
        "outputId": "dddff98f-74ed-43b8-b9bc-bdefc69b2046"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://www.gurufocus.com/news/2486529/microsoft-corp-msft-q4-2024-earnings-call-transcript-highlights-strong-cloud-growth-and-record-revenue'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from markitdown import MarkItDown\n",
        "\n",
        "md = MarkItDown()\n",
        "doc_content = md.convert(result[0]['url'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "IXfm6C2UizLk",
        "outputId": "d3f4702b-388b-4c87-af8b-6d1de29c0901"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<frozen importlib._bootstrap>:1047: ImportWarning: _PyDrive2ImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _GenerativeAIImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _OpenCVImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: APICoreClientInfoImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _BokehImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _AltairImportHook.find_spec() not found; falling back to find_module()\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "403 Client Error: Forbidden for url: https://www.gurufocus.com/news/2486529/microsoft-corp-msft-q4-2024-earnings-call-transcript-highlights-strong-cloud-growth-and-record-revenue",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-184814300e1f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMarkItDown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdoc_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/markitdown/_markitdown.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, source, **kwargs)\u001b[0m\n\u001b[1;32m   1090\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"file://\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m             ):\n\u001b[0;32m-> 1092\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1093\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/markitdown/_markitdown.py\u001b[0m in \u001b[0;36mconvert_url\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# Send a HTTP request to the URL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_requests_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://www.gurufocus.com/news/2486529/microsoft-corp-msft-q4-2024-earnings-call-transcript-highlights-strong-cloud-growth-and-record-revenue"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_content = md.convert(result[1]['url'])"
      ],
      "metadata": {
        "id": "cK2iU3buYDsB"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(doc_content.title.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iNDFm3qk-Em",
        "outputId": "596d2d54-283c-4c05-ee5c-0611ea265cc1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Microsoft  (NASDAQ:MSFT) Q4 2024 Earnings Report on 7/30/2024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(doc_content.text_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7ok0mAqi9Oz",
        "outputId": "7112b92c-32d0-4415-8f33-e48ee5f58442"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Skip to main content](#main)\n",
            "\n",
            "[![MarketBeat home page](/images/master/MarketBeat-logo-r-white.svg?v=2019)](https://www.marketbeat.com \"MarketBeat\")\n",
            "\n",
            "* [Research Tools](/all-access/)\n",
            "  + [All Access Research Tools](/all-access/)\n",
            "    - [Live News Feed](/all-access/live-news/)\n",
            "    - [Momentum Alerts](/manage/momentum-alerts/)\n",
            "    - [Idea Engine](/all-access/idea-engine/)\n",
            "    - [Export Data (CSV)](/all-access/export-data/)\n",
            "    - [See All Research Tools](/all-access/)\n",
            "  + [My MarketBeat](/manage/watchlists/)\n",
            "    - [My Portfolio](/manage/watchlists/)\n",
            "    - [My Newsletter](/manage/watchlists/#newsletter)\n",
            "    - [My Account](/manage/)\n",
            "  + [Calculators](/calculators/)\n",
            "    - [Dividend Calculator](/dividends/calculator/)\n",
            "    - [Dividend Yield Calculator](/dividends/yield-calculator/)\n",
            "    - [Market Cap Calculator](/calculators/market-cap-calculator/)\n",
            "    - [Options Profit Calculator](/calculators/options-profit-calculator/)\n",
            "    - [Stock Average Calculator](/calculators/stock-average-calculator/)\n",
            "    - [Stock Split Calculator](/stock-splits/calculator/)\n",
            "    - [Stock Profit Calculator](/calculators/stock-profit-calculator/)\n",
            "  + [Stock Screeners](/stock-screener/)\n",
            "    - [Stock Screener](/stock-screener/)\n",
            "    - [ETF Screener](/all-access/etf-screener/)\n",
            "  + [Trending Stocks](/stocks/trending/)\n",
            "    - [Trending MarketBeat Stocks](/stocks/trending/)\n",
            "    - [Trending Media Mentions](/stocks/trending/media/)\n",
            "    - [High Media Sentiment Stocks](/stocks/trending/media-sentiment/)\n",
            "    - [Trending WallStreetBets Stocks](/stocks/trending/wallstreetbets/)\n",
            "  + [Premium Reports](/all-access/reports/)\n",
            "    - [10 Best High-Yield Dividend Stocks](/all-access/reports/?report=highyield)\n",
            "    - [7 Magnificent Stocks in 2025](/all-access/reports/?report=Magnificent7)\n",
            "    - [10 Best AI Stocks to Own in 2025](/all-access/reports/?report=10BestAIStocks)\n",
            "    - [7 Stocks to Buy And Hold Forever](/all-access/reports/?report=buyandholdforever)\n",
            "    - [10 Best Stocks to Own in 2025](/all-access/reports/?report=newyear)\n",
            "    - [2025 Gold Forecast: A Perfect Storm](/all-access/reports/?report=gold)\n",
            "    - [Beginner's Guide to Cannabis Stocks](/all-access/reports/?report=pot)\n",
            "    - [Beginner's Guide to 5G Stocks](/all-access/reports/?report=5g)\n",
            "* [Financial Calendars](/market-data/)\n",
            "  + [Calendars and Market Data](/market-data/)\n",
            "* [Market Data](/market-data/)\n",
            "  + [All Market Data and Financial Calendars](/market-data/)\n",
            "  + [Analyst Ratings](/ratings/)\n",
            "    - [Recent Analyst Ratings](/ratings/)\n",
            "    - [Stock Ratings Screener](/all-access/ratings-screener/)\n",
            "    - [Top-Rated Stocks](/stocks/top-rated/)\n",
            "    - [Lowest-Rated Stocks](/stocks/lowest-rated/)\n",
            "    - [Top-Rated Analysts](/all-access/analyst-rankings/)\n",
            "    - [Top-Rated Brokerages](/all-access/brokerage-rankings/)\n",
            "    - [Most-Upgraded Stocks](/stocks/most-upgraded/)\n",
            "    - [Free Ratings Newsletter](/ratings/newsletter/)\n",
            "  + [Congressional Data](/congress-stock-trades/)\n",
            "    - [Recent Trades](/congress-stock-trades/)\n",
            "    - [Most Bought Stocks](/congress-stock-trades/most-purchased-stocks/)\n",
            "    - [Members of Congress](/congress-stock-trades/profiles/)\n",
            "  + [Corporate Events](/market-data/)\n",
            "    - [Corporate Buybacks](/stock-buybacks/)\n",
            "    - [Economic Reports](/economic-reports/upcoming/)\n",
            "    - [Initial Public Offerings (IPOs)](/ipos/)\n",
            "    - [IPO Lockup Expirations](/ipos/lockup-expirations/)\n",
            "    - [SEC Filings](/market-data/sec-filings/)\n",
            "    - [Stock Splits](/stock-splits/)\n",
            "  + [Dividends](/dividends/)\n",
            "    - [Today's Announcements](/dividends/announcements/)\n",
            "    - [Ex-Dividend Calendar](/dividends/ex-dividend-calendar/)\n",
            "    - [Dividend Increases](/dividends/increases/)\n",
            "    - [Dividend Achievers](/dividends/achievers/)\n",
            "    - [Dividend Aristocrats](/dividends/aristocrats/)\n",
            "    - [Best Dividend Stocks](/dividends/best-dividend-stocks/)\n",
            "    - [High-Yield Dividend Stocks](/dividends/high-yield/)\n",
            "    - [Top-Rated Dividend Stocks](/dividends/top-rated-dividend-stocks/)\n",
            "    - [Dividend Screener](/dividends/screener/)\n",
            "    - [Free Dividend Newsletter](/dividends/subscribe/)\n",
            "  + [Earnings](/earnings/)\n",
            "    - [Today's Announcements](/earnings/latest/)\n",
            "    - [Tomorrow's Announcements](/earnings/tomorrow/)\n",
            "    - [Next Week's Announcements](/earnings/next-week/)\n",
            "    - [Upcoming Earnings Calls](/earnings/conference-calls/)\n",
            "    - [Earnings Call Transcripts](/earnings/transcripts/)\n",
            "    - [Earnings Screener](/all-access/earnings-screener/)\n",
            "  + [Insider Trades](/insider-trades/)\n",
            "    - [Today's Insider Trades](/insider-trades/)\n",
            "    - [Top Insider Buying Stocks](/insider-trades/insider-buying-stocks/)\n",
            "    - [Top Insider Selling Stocks](/insider-trades/insider-selling-stocks/)\n",
            "    - [Insider Trades Screener](/all-access/insider-trades-screener/)\n",
            "    - [Insider Trades Newsletter](/insider-trades/subscribe/)\n",
            "  + [Stock Market Holidays](/stock-market-holidays/)\n",
            "  + [Cryptocurrencies](/cryptocurrencies/)\n",
            "    - [All Cryptocurrencies](/cryptocurrencies/)\n",
            "    - [Cryptocurrency Headlines](/cryptocurrencies/news/)\n",
            "    - [Cryptocurrency Newsletter](/cryptocurrencies/newsletter/)\n",
            "  + [Gainers & Decliners](/market-data/biggest-percentage-gainers/)\n",
            "    - [Percentage Gainers](/market-data/biggest-percentage-gainers/)\n",
            "    - [Percentage Decliners](/market-data/biggest-percentage-decliners/)\n",
            "    - [Breakout Stocks](/market-data/breakout-stocks/)\n",
            "  + [High & Low P/E](/market-data/low-pe-stocks/)\n",
            "    - [High P/E Stocks](https://www.marketbeat.com/market-data/high-pe-stocks/)\n",
            "    - [Low P/E Stocks](https://www.marketbeat.com/market-data/low-pe-stocks/)\n",
            "  + [Highs & Lows](/market-data/52-week-highs/)\n",
            "    - [52-Week Highs](/market-data/52-week-highs/)\n",
            "    - [52-Week Lows](/market-data/52-week-lows/)\n",
            "  + [Most Active](/market-data/most-active-stocks/)\n",
            "    - [Most Active Stocks](/market-data/most-active-stocks/)\n",
            "    - [Most Volatile Stocks](/market-data/most-volatile-stocks/)\n",
            "    - [Unusual Trading Volume](/market-data/unusual-volume-stocks/)\n",
            "    - [Trading Halts](/market-data/trading-halts/)\n",
            "  + [Options](/market-data/unusual-call-options-volume/)\n",
            "    - [Unusual Call Volume](/market-data/unusual-call-options-volume/)\n",
            "    - [Unusual Put Volume](/market-data/unusual-put-options-volume/)\n",
            "  + [Sector Performance](/market-data/sector-performance/)\n",
            "  + [Short Interest](/short-interest/)\n",
            "    - [Largest Short Positions](/short-interest/)\n",
            "    - [Short Interest Increases](/short-interest/short-interest-increases/)\n",
            "    - [Short Interest Decreases](/short-interest/short-interest-decreases/)\n",
            "* [Stock Lists](/stocks/)\n",
            "  + [All Stock Lists](/stocks/)\n",
            "  + [Stocks by Interest](/types-of-stock/)\n",
            "    - [5G Stocks](/types-of-stock/5g-stocks/)\n",
            "    - [Blue Chip Stocks](/types-of-stock/blue-chip-stocks/)\n",
            "    - [Biotech Stocks](/types-of-stock/biotech-stocks/)\n",
            "    - [FAANG Stocks](/types-of-stock/faang-stocks/)\n",
            "    - [Gold Stocks](/types-of-stock/gold-stocks/)\n",
            "    - [Large Cap Stocks](/types-of-stock/large-cap-stocks/)\n",
            "    - [Marijuana Stocks](/types-of-stock/marijuana-stocks/)\n",
            "    - [Oil Stocks](/types-of-stock/oil-stocks/)\n",
            "    - [REITs](/types-of-stock/real-estate-investment-trusts-reits/)\n",
            "    - [Russell 2000 Stocks](/types-of-stock/russell-2000-stocks/)\n",
            "    - [Small Cap Stocks](/types-of-stock/small-cap-stocks/)\n",
            "    - [Warren Buffett Stocks](/types-of-stock/warren-buffett-stocks/)\n",
            "  + [Low Priced Stocks](/market-data/low-priced-stocks/)\n",
            "    - [Stocks Under $0.50](/market-data/low-priced-stocks/stocks-under-50-cents/)\n",
            "    - [Stocks Under $1](/market-data/low-priced-stocks/stocks-under-1/)\n",
            "    - [Stocks Under $2](/market-data/low-priced-stocks/stocks-under-2/)\n",
            "    - [Stocks Under $5](/market-data/low-priced-stocks/stocks-under-5/)\n",
            "  + [Penny Stocks](/types-of-stock/penny-stocks/)\n",
            "    - [Most Active Penny Stocks](/market-data/most-active-penny-stocks/)\n",
            "    - [Most Popular Penny Stocks](/types-of-stock/penny-stocks/most-popular/)\n",
            "    - [Top Penny Stocks Today](/types-of-stock/penny-stocks/top-penny-stocks-today/)\n",
            "  + [Stocks by Exchange](/stocks/)\n",
            "    - [NYSE Stocks](/stocks/NYSE/)\n",
            "    - [NASDAQ Stocks](/stocks/NASDAQ/)\n",
            "    - [OTCMKTS Stocks](/stocks/OTCMKTS/)\n",
            "    - [TSX Stocks](/stocks/TSE/)\n",
            "    - [LSE Stocks](/stocks/LON/)\n",
            "  + [Stocks by Sector](/stocks/sectors/)\n",
            "    - [Automotive Stocks](/stocks/sectors/auto-tires-trucks/)\n",
            "    - [Aerospace Stocks](/stocks/sectors/aerospace/)\n",
            "    - [Basic Materials Stocks](/stocks/sectors/basic-materials/)\n",
            "    - [Business Services Stocks](/stocks/sectors/business-services/)\n",
            "    - [Consumer Discretionary Stocks](/stocks/sectors/consumer-discretionary/)\n",
            "    - [Consumer Staples Stocks](/stocks/sectors/consumer-staples/)\n",
            "    - [Construction Stocks](/stocks/sectors/construction/)\n",
            "    - [Energy Stocks](/stocks/sectors/oils-energy/)\n",
            "    - [Finance Stocks](/stocks/sectors/finance/)\n",
            "    - [Industrial Stocks](/stocks/sectors/industrial-products/)\n",
            "    - [Manufacturing Stocks](/stocks/sectors/manufacturing/)\n",
            "    - [Medical Stocks](/stocks/sectors/medical/)\n",
            "    - [Real Estate Stocks](/stocks/sectors/real-estate/)\n",
            "    - [Retail Stocks](/stocks/sectors/retail-wholesale/)\n",
            "    - [Technology Stocks](/stocks/sectors/computer-and-technology/)\n",
            "    - [Transportation Stocks](/stocks/sectors/transportation/)\n",
            "    - [Utilities Stocks](/stocks/sectors/utilities/)\n",
            "  + [Technical Indicators](/stocks/)\n",
            "    - [Death Cross Stocks](/stocks/death-cross-stocks/)\n",
            "    - [Golden Cross Stocks](/stocks/golden-cross-stocks/)\n",
            "    - [RSI Overbought Stocks](/market-data/overbought-stocks-rsi/)\n",
            "    - [RSI Oversold Stocks](/market-data/oversold-stocks-rsi/)\n",
            "  + [Stock Comparisons](/compare-stocks/)\n",
            "  + [Premium Stock Lists](/stocks/top-rated/)\n",
            "    - [Top MarketRank™ Stocks](/stocks/top-marketrank/)\n",
            "    - [Top ESG Stocks](/stocks/top-esg-stocks/)\n",
            "    - [Top-Rated Stocks](/stocks/top-rated/)\n",
            "    - [Top-Rated Dividend Stocks](/dividends/top-rated-dividend-stocks/)\n",
            "    - [Top-Rated Small-Cap Stocks](/stocks/top-rated-small-cap-stocks/)\n",
            "    - [Top-Rated Tech Stocks](/stocks/top-rated-tech-stocks/)\n",
            "* [Headlines](/headlines/)\n",
            "  + [MarketBeat TV](/videos/)\n",
            "    - [![3 Quantum Computing Stocks to Watch in 2025 (That Aren't Rigetti)](https://www.marketbeat.com/logos/videos/thumb_20250122155142_videobeyondrigetti.png)3 Quantum Computing Stocks to Watch in 2025 (That Aren't Rigetti)](/videos/3-quantum-computing-stocks-to-watch-in-2025-that-arent-rigetti/)\n",
            "    - [3 Quantum Computing Stocks to Watch in 2025 (That Aren't Rigetti)](/videos/3-quantum-computing-stocks-to-watch-in-2025-that-arent-rigetti/)\n",
            "    - [![Transportation Stocks to Watch in 2025: Top Picks for Growth](https://www.marketbeat.com/logos/videos/thumb_20250121174839_videoairplane.png)Transportation Stocks to Watch in 2025: Top Picks for Growth](/videos/transportation-stocks-to-watch-in-2025-top-picks-for-growth/)\n",
            "    - [Transportation Stocks to Watch in 2025: Top Picks for Growth](/videos/transportation-stocks-to-watch-in-2025-top-picks-for-growth/)\n",
            "    - [![Crypto Boom 2025: Bitcoin’s Rise and Trump’s Impact on the Market](https://www.marketbeat.com/logos/videos/thumb_20250120152625_videotrumpfuelscryptorally.png)Crypto Boom 2025: Bitcoin’s Rise and Trump’s Impact on the Market](/videos/crypto-boom-2025-bitcoins-rise-and-trumps-impact-on-the-market/)\n",
            "    - [Crypto Boom 2025: Bitcoin’s Rise and Trump’s Impact on the Market](/videos/crypto-boom-2025-bitcoins-rise-and-trumps-impact-on-the-market/)\n",
            "    - [![Goldman Sachs' 2025 Market Outlook: Top 3 Stock Picks](https://www.marketbeat.com/logos/videos/thumb_20250117162003_videotop3stocks.png)Goldman Sachs' 2025 Market Outlook: Top 3 Stock Picks](/videos/goldman-sachs-2025-market-outlook-top-3-stock-picks/)\n",
            "    - [Goldman Sachs' 2025 Market Outlook: Top 3 Stock Picks](/videos/goldman-sachs-2025-market-outlook-top-3-stock-picks/)\n",
            "  + [Featured Articles](/originals/)\n",
            "    - [Oracle Announces Game-Changing News for the AI Industry](/originals/oracle-announces-game-changing-news-for-the-ai-industry/)\n",
            "    - [Netflix Adds 19 Million Subscribers, Growth Is Far From Over](/originals/netflix-adds-19-million-subscribers-growth-is-far-for-over/)\n",
            "    - [Tempus AI: A Game-Changer in AI-Powered Healthcare](/originals/tempus-ai-a-game-changer-in-ai-powered-healthcare/)\n",
            "    - [Pelosi Bets Big on AI: This Is What You Need to Know](/originals/pelosi-bets-big-on-ai-this-is-what-you-need-to-know/)\n",
            "    - [Mobileye's High Short Interest Signals Squeeze Potential](/originals/mobileyes-high-short-interest-signals-squeeze-potential/)\n",
            "    - [Rivian: Weathering the Storm, Poised for Growth?](/originals/rivian-weathering-the-storm-poised-for-growth/)\n",
            "    - [Avient Stock: Manufacturing Play With Double-Digit Upside](/originals/avient-stock-manufacturing-play-with-double-digit-upside/)\n",
            "    - [Ultrasound Weight Loss: GE HealthCare and Novo Nordisk’s Play](/originals/ultrasound-weight-loss-ge-healthcare-and-novo-nordisks-play/)\n",
            "    - [More Featured Articles](/originals/)\n",
            "  + [News](/headlines/)\n",
            "    - [All Headlines](/headlines/)\n",
            "    - [Instant News Alerts](/instant-alerts/)\n",
            "    - [Research Surveys](/surveys/)\n",
            "    - [Real-Time News Feed](/all-access/live-news/)\n",
            "    - [Investing Slideshows](/slideshows/)\n",
            "* [Education](/learn/)\n",
            "  + Featured Topic: Options Trading\n",
            "    - [![NEOS S&P 500 High Income ETF (SPYI)](https://www.marketbeat.com/logos/articles/thumb_20241210135548_neos-sp-500r-high-income-etf-harnasses-the-power-o.png)Neos S&P 500(R) High Income ETF Harnesses the Power of Options](https://www.marketbeat.com/originals/neos-s-and-p-500r-high-income-etf-harnasses-the-power-of-options/)\n",
            "    - [![](https://www.marketbeat.com/logos/articles/thumb_20241104115452_options-traders-bet-big-on-these-3-tech-stocks.jpg)Options Traders Bet Big on These 3 Tech Stocks](https://www.marketbeat.com/originals/3-unusual-call-option-trades-investors-should-be-watching/)\n",
            "    - [![Six bitcoin coins — Stock Editorial Photography](https://www.marketbeat.com/logos/articles/thumb_20241101152430_how-to-play-new-options-trading-with-bitcoin-etfs.jpg)How to Play New Options Trading With Bitcoin ETFs](https://www.marketbeat.com/learn/how-to-play-new-options-trading-with-bitcoin-etfs/)\n",
            "    - [![albemarle lithium ion battery](https://www.marketbeat.com/logos/articles/thumb_20241028111137_albemarle-stock-call-options-surge-what-it-means-f.png)Albemarle Stock Call Options Surge: What It Means for Lithium](https://www.marketbeat.com/originals/albemarle-stock-call-options-surge-what-it-means-for-lithium/)\n",
            "    - [![Photo of a businessman running inside a circle made of money symbolic of the wheel options trading strategy. ](https://www.marketbeat.com/logos/articles/thumb_20240718150215_how-to-execute-the-wheel-strategy-to-generate-opti.jpg)How to Execute the Wheel Strategy to Generate Options Income](https://www.marketbeat.com/learn/options-how-to-execute-the-wheel-strategy-for-options-income/)\n",
            "    - [![Stock options strategies ](https://www.marketbeat.com/logos/articles/thumb_20240626075418_3-options-strategies-to-play-a-stocks-uptrend-if-b.jpg)3 Options Strategies to Play a Stock’s Uptrend if Bullish](https://www.marketbeat.com/learn/3-options-strategies-to-play-a-stocks-uptrend-if-bullish/)\n",
            "  + [Learn](/learn/)\n",
            "    Read investment guides, how-to articles, and explainers.\n",
            "  + [Stock Ideas](/stock-ideas/)\n",
            "    Looking for ideas for stocks to invest in? These stocks are poised to move.\n",
            "  + [Financial Terms](/financial-terms/)\n",
            "    Learn the language of investment with our glossary of over 200 financial terms.\n",
            "  + [Help](/help/)\n",
            "    View our library of help videos to learn how to use the tools on the MarketBeat website.\n",
            "\n",
            "Log in\n",
            "[Free Trial](https://marketbeat.com/subscribe/all-access/?ReferralType=HeaderButton)\n",
            "\n",
            "![](/images/loading-gif.gif?v=2)\n",
            "Search\n",
            "\n",
            "[→ Did You See Trump’s Manhattan Project Bombshell?](https://www.marketbeat.com/scripts/redirect.aspx?TriggeredCampaignID=11477&UserID=0&Placement=NativeDisplay&Source=NativeDisplay&interstitial=1)  (From Banyan Hill Publishing) (Ad)![](https://www.marketbeat.com/scripts/TriggeredCampaignPixel.ashx?TriggeredCampaignID=11477&Placement=NativeDisplay&Source=NativeDisplay)\n",
            "\n",
            "# Microsoft Q4 2024 Earnings Report\n",
            "\n",
            "![Microsoft logo](https://www.marketbeat.com/logos/thumbnail/microsoft-corporation-logo.png)\n",
            "**$446.20**  **+17.70 (+4.13%)** As of 04:00 PM Eastern\n",
            "\n",
            "[Earnings History](/stocks/NASDAQ/MSFT/earnings/)[Forecast](/stocks/NASDAQ/MSFT/forecast/)\n",
            "\n",
            "## Microsoft EPS Results\n",
            "\n",
            "Actual EPS**$2.95**Consensus EPS **$2.90**Beat/Miss**Beat by +$0.05**One Year Ago EPS**$2.69**\n",
            "## Microsoft Revenue Results\n",
            "\n",
            "Actual Revenue**$64.73 billion**Expected Revenue**$64.38 billion**Beat/Miss**Beat by +$346.54 million**YoY Revenue Growth**+15.20%**\n",
            "## Microsoft Announcement Details\n",
            "\n",
            "Quarter**Q4 2024**Date**[7/30/2024](/all-access/earnings-screener/?Date=7/30/2024)**Time**After Market Closes**\n",
            "### MSFT Upcoming Earnings\n",
            "\n",
            "Microsoft will be holding an earnings conference call on Wednesday, January 29 at 5:30 PM Eastern. Interested parties can [register for or listen to the call](http://mmm.wallstreethorizon.com/u.asp?u=655679).\n",
            "\n",
            "## Conference Call Resources\n",
            "\n",
            "* [Conference Call](http://mmm.wallstreethorizon.com/u.asp?u=609208)\n",
            "* [Conference Call Transcript](#transcript)\n",
            "* [Conference Call Audio](#call-audio)\n",
            "* [Slide Deck](https://files.quartr.com/conference-calls/09845-2024-07-30-08-47-24.pdf?ref=bWFya2V0YmVhdA==)\n",
            "* [Press Release](https://files.quartr.com/reports/40031-2024-07-30-08-06-04.pdf?ref=bWFya2V0YmVhdA==)\n",
            "* [MSFT Earnings History](/stocks/NASDAQ/MSFT/earnings/)\n",
            "\n",
            "### Slide Deck\n",
            "\n",
            "[Full Screen Slide Deck](https://files.quartr.com/conference-calls/09845-2024-07-30-08-47-24.pdf?ref=bWFya2V0YmVhdA== \"Open Slide Deck in New Tab\")[Powered by ![Quartr](/images/quartr-logotype-svg.svg)](https://quartr.com)\n",
            "### Microsoft Q4 2024 Earnings Call Transcript\n",
            "\n",
            "![AlphaStreet](https://www.marketbeat.com/logos/authors/thumb_20240829111142_author-alphastreet.png)Provided by AlphaStreetJuly 30, 2024 ShareLink copied to clipboard.\n",
            "#### Presentation\n",
            "\n",
            "[Skip to Questions & Answers](#questions-and-answers)Operator\n",
            "\n",
            "Greetings, and welcome to the Microsoft Fiscal Year 2024 Fourth Quarter Earnings Conference Call. [Operator Instructions] As a reminder, this conference is being recorded.\n",
            "\n",
            "I would now like to turn the conference over to your host, Brett Iversen, Vice President of Investor Relations.\n",
            "\n",
            "Brett IversenVice President of Investor Relations at Microsoft\n",
            "\n",
            "Good afternoon, and thank you for joining us today. On the call with me are Satya Nadella, Chairman and Chief Executive Officer; Amy Hood, Chief Financial Officer; Alice Jolla, Chief Accounting Officer; and Keith Dolliver, Corporate Secretary and Deputy General Counsel.\n",
            "\n",
            "On the Microsoft Investor Relations website, you can find our earnings press release and financial summary slide deck, which is intended to supplement our prepared remarks during today's call, and provides the reconciliation of differences between GAAP and non-GAAP financial measures. More detailed outlook slides will be available on the Microsoft Investor Relations website when we provide outlook commentary on today's call.\n",
            "\n",
            "On this call, we will discuss certain non-GAAP items. The non-GAAP financial measures provided should not be considered as a substitute for or superior to the measures of financial performance prepared in accordance with GAAP. They are included as additional clarifying items to aid investors in further understanding the Company's fourth-quarter performance in addition to the impact these items and events have on the financial results.\n",
            "\n",
            "All growth comparisons we make on the call today relate to the corresponding period of last year unless otherwise noted. We will also provide growth rates in constant currency when available as a framework for assessing how our underlying businesses performed, excluding the effect of foreign currency rate fluctuations. Where growth rates are the same in constant currency, we will refer to the growth rate only. We will post our prepared remarks to our website immediately following the call until the complete transcript is available.\n",
            "\n",
            "Today's call is being webcast live and recorded. If you ask a question. It will be included in our live transmission, in this transcript, and in any future use of the recording. You can replay the call and view the transcript on the Microsoft Investor Relations website. During this call, we'll be making forward-looking statements which are predictions, projections, or other statements about future events. These statements are based on current expectations and assumptions that are subject to risks and uncertainties. Actual results could materially differ because of factors discussed in today's earnings press release, in the comments made during this conference call, and in the Risk Factors section of our Form 10-K, Forms 10-Q, and other reports and filings with the Securities and Exchange Commission. We do not undertake any duty to update any forward-looking statement.\n",
            "\n",
            "And with that, I'll turn the call over to Satya.\n",
            "\n",
            "Satya NadellaChairman and Chief Executive Officer at Microsoft\n",
            "\n",
            "Thank you, Brett. We had a solid close to our fiscal year. All-up annual revenue was more than $245 billion, up 15% year-over-year and Microsoft Cloud revenue surpassed $135 billion, up 23%. Before I dive in, I want to offer some broader perspective on the AI platform shift. Similar to the cloud, this transition involves both knowledge and capital-intensive investments. And as we go through this shift, we are focused on two fundamental things. First, driving innovation across a product portfolio that spans infrastructure and applications so as to ensure that we are maximizing our opportunity, while in parallel continuing to scale our cloud business and prioritizing fundamentals starting with security. Second, using customer demand signal and time to value to manage our cost structure dynamically and generate durable long-term operating leverage.\n",
            "\n",
            "With that, let me highlight examples starting with Azure. Our share gains accelerated this year driven by AI. We expanded our data center footprint, announcing investments across four continents. These are long-term assets around the world to drive growth for the next decade and beyond. We added new AI accelerators from AMD and NVIDIA as well as our own first-party Silicon Azure Maia and we introduced new Cobalt 100, which provides best-in-class performance for customers like Elastic, MongoDB, Siemens, Snowflake, and Teradata.\n",
            "\n",
            "We continue to see sustained revenue growth from migrations. Azure Arc is helping customers in every industry from ABB and Cathay Pacific to La Liga to streamline their cloud migrations. We now have 36,000 ARC customers up 90% year-over-year. We remain the hyperscale cloud of choice for SAP and Oracle workloads, Atos, Kohl's, Daimler Truck AG, Domino's, Helion, for example, all migrated their mission-critical SAP workloads to our cloud and with our Azure VMware solution, we offer the fastest and most cost-effective way for customers to migrate their VMware workloads too.\n",
            "\n",
            "With Azure AI, we are building out the app server for the AI wave, providing access to the most diverse selection of models to meet customers' unique cost, latency, and design considerations. All-up, we now have over 60,000 Azure AI customers, up nearly 60% year-over-year, and average spend per customer continues to grow. Azure OpenAI Service provides access to best-in-class frontier models, including as of this quarter, GPT-4o and GPT-4o mini. It's being used by leading companies in every industry, including H&R Block, Suzuki, Swiss Re, Telstra as well as digital natives like Freshworks, Meesho, and Zomato.\n",
            "\n",
            "With Phi-3 we offer a family of powerful small language models, which are being used by companies like BlackRock, Emirates, Epic, ITC, Navy Federal Credit Union, and others. And with models as a service, we provide API access to third-party models, including as of last week, the latest from Cohere, Meta, and Mistral. The number of paid models as a service customers more than doubled quarter-over-quarter and we are seeing increased usage by leaders in every industry from Adobe and Bridgestone to Novo Nordisk and Palantir.\n",
            "\n",
            "Now on to data. Our Microsoft intelligent data platform provides customers with the broadest capabilities spanning databases, analytics, business intelligence, and governance, along with seamless integration with all of our AI services. The number of Azure AI customers also using our data and analytics tools grew nearly 50% year-over-year. Microsoft Fabric, our AI-powered next-generation data platform now has over 14,000 paid customers, including leaders in every industry from Accenture and Kroger to Rockwell Automation and ZEISS, up 20% quarter-over-quarter. And this quarter, we introduced new first-of-their-kind real-time intelligence capabilities in fabric so customers can unlock insights on high-volume time-sensitive data.\n",
            "\n",
            "Now on to developer tools. GitHub Copilot is by far the most widely adopted AI-powered developer tool. Just over two years since its general availability, more than 77,000 organizations from BBVA, FedEx, and H&M to Infosys and Paytm have adopted Copilot, up 180% year-over-year. And we're going further with Copilot workspace, we offer Copilot native end-to-end developer productivity across plan, build, test, debug, and deploy cycle. Copilot is driving GitHub growth all up. GitHub annual revenue run-rate is now $2 billion. Copilot accounted for over 40% of GitHub revenue growth this year and is already a larger business than all of GitHub was when we acquired it.\n",
            "\n",
            "We are also integrating generative AI across power platform, enabling anyone to use natural language to create apps, automate workflows, or build a website. To date, over 480,000 organizations have used AI-powered capabilities in Power Platform up 45% quarter-over-quarter. In total, we now have 48 million monthly active users of Power Platform up 40% year-over-year. Now on to future of work. Copilot for Microsoft 365 is becoming a daily habit for knowledge workers as it transforms work, workflow, and work artifacts. The number of people who use Copilot daily at work nearly doubled quarter-over-quarter as they use it to complete tasks faster, hold more effective meetings, and automate business workflows and processes.\n",
            "\n",
            "Copilot customers increased more than 60% quarter-over-quarter. Feedback has been positive with the majority of enterprise customers coming back to purchase more seats. All-up, the number of customers with more than 10,000 seats more than doubled quarter-over-quarter, including Capital Group, Disney, Dow, Kyndryl, Novartis, and EY alone will deploy Copilot to 150,000 of its employees and we are going further adding agent capabilities to Copilot. New Team Copilot can facilitate meetings and create and assign tasks. And with Copilot studio, customers can extend Copilot for Microsoft 365 and build custom copilots that proactively respond to data and events using their own first and third-party business data.\n",
            "\n",
            "To date, 50,000 organizations from Carnival Corporations, Cognizant, and Eaton to KPMG, Majesco, and McKinsey have used Copilot Studio up over 70% quarter-over-quarter. We are also extending Copilot to specific industries, including healthcare with DAX Copilot, more than 400 healthcare organizations, including Community Health Network, Intermountain, Northwestern Memorial Healthcare and Ohio State University, Wexner Medical Center have purchased DAX Copilot to date, up 40% quarter-over-quarter and the number of AI-generated clinical reports more than tripled.\n",
            "\n",
            "Copilot is also transforming ERP and CRM business applications. We again took share this quarter as customers like Thermo Fisher Scientific switched to Dynamics. Our new Dynamics 365 contact center is a Copilot-first solution that infuses generative AI throughout the contact center workflow. Companies like 1-800-Flowers, Mediterranean Shipping, Synoptek will rely on it to deliver better customer support. And Dynamics 365 Business Central is now trusted by over 40,000 organizations for Core ERP.\n",
            "\n",
            "Microsoft Teams has become essential to how hundreds of millions of people meet, call, chat, collaborate, and do business. We once again saw year-over-year usage growth. Teams Premium has surpassed 3 million seats, up nearly 400% year-over-year as organizations like Dentsu, Eli Lilly, and Ford chose it for advanced features like end-to-end encryption and real-time translation. When it comes to devices, we introduced our new category of Copilot Plus PCs this quarter. They are the fastest, most intelligent Windows PCs ever. They include a new system architecture designed to deliver best-in-class performance and breakthrough AI experiences.\n",
            "\n",
            "We are delighted by early reviews and we are looking forward to the introduction of more Copilot Plus PCs powered by all of our silicon and OEM partners in the coming months. More broadly, Windows 11 active devices increased 50% year-over-year and we are seeing accelerated adoption of Windows 11 by companies like Carlsberg, AON, National Australia Bank.\n",
            "\n",
            "And now on to security. We continue to prioritize security above all else. We are doubling down on our Secure Future initiative as we implement our principles of secure by design, secure by default, and secure operations. Through this initiative, we are also continually applying what we are learning and translating it into innovation for our customers, including how we approach AI. Over 1,000 paid customers used Copilot for security, including Alaska Airlines, Oregon State University, Petrofac, Wipro, WTW and we are also securing customers' AI deployments with updates to Defender and Purview. All-up, we now have 1.2 million security customers over 800,000, including Dell Technologies, Deutsche Telecom, TomTom use four or more workloads, up 25% year-over-year. And Defender for Cloud, our cloud security solutions surpassed $1 billion in revenue over the past 12 months as we protect customer workloads across multi-cloud and hybrid environments.\n",
            "\n",
            "Now let me turn to our consumer businesses starting with LinkedIn. LinkedIn continues to see accelerated member growth and record engagement, 1.5 million pieces of content are shared every minute on the platform and video is now the fastest-growing format on LinkedIn with uploads up 34% year-over-year. LinkedIn Marketing Solutions continues to be a leader in B2B digital advertising, helping companies deliver the right message to the right audience on a safe, trusted platform. And when it comes to our subscription businesses, premium sign-ups increased 51% this fiscal year and we are adding even more value to our members and customers with new AI tools.\n",
            "\n",
            "Our reimagined AI-powered LinkedIn premium experience is now available for every premium subscriber worldwide, helping them more easily and intuitively connect to opportunity, learn, and get career coaching. Finally, hiring took share for the second consecutive year and now on to search advertising and news. We are ensuring that Bing, Edge, and Copilot collectively are driving more engagement and value to end users, publishers, and advertisers.\n",
            "\n",
            "Our overall revenue ex-TAC increased 19% year-over-year and we again took share across Bing and Edge. We continue to apply generative AI to pioneer new approaches to how people search and browse. Just last week, we announced we are testing a new generative search experience, which creates a dynamic response to users' query while maintaining clickshare to publishers. And we continue to drive record engagement with Copilot for the web, consumers have used Copilot to create over 12 billion images and conduct 13 billion chats to date, up 150% since the start of the calendar year.\n",
            "\n",
            "Thousands of news and entertainment publishers trust us to reach new audiences with Microsoft start and in fact, we have paid them $1 billion over the last five years. We are helping advertisers increase their ROI too. We have seen positive response to Performance Max, which uses AI to dynamically create and optimize ads, and Copilot in Microsoft ad platform helps marketeers create campaigns and troubleshoot using natural language.\n",
            "\n",
            "Now on to gaming. We now have over 500 million monthly active users across platforms and devices, and our content pipeline has never been stronger. We previewed a record 30 new titles at our showcase this quarter, 18 of them such as Call of Duty, Black Op 6 will be available on Game Pass. Game Pass Ultimate subscribers can now stream games directly on devices they already have, including as of last month, Amazon Fire TVs.\n",
            "\n",
            "Finally, we are bringing our IP to new audiences. Fallout, for example, made its debut as a TV show on Amazon Prime this quarter. It was the second most-watched title on the platform ever and hours played on Game Pass for the Fallout franchise increased nearly 5 times quarter-over-quarter. In closing, I'm energized about the opportunities ahead. We are investing for the long-term in our fundamentals, in our innovation, and in our people.\n",
            "\n",
            "With that, let me turn it over to Amy.\n",
            "\n",
            "Amy HoodChief Financial Officer at Microsoft\n",
            "\n",
            "Thank you, Satya, and good afternoon, everyone. This quarter, revenue was $64.7 billion, up 15% and 16% in constant currency. Earnings per share was $2.95, an increased 10% and 11% in constant-currency. In our largest quarter of the year, we again delivered double-digit top and bottom-line growth with continued share gains across many of our businesses and record commitments to our Microsoft Cloud platform. Commercial bookings were significantly ahead of expectations and increased 17% and 19% in constant currency.\n",
            "\n",
            "This record commitment quarter was driven by growth in the number of $10 million-plus and $100 million-plus contracts for both Azure and Microsoft 365 and consistent execution across our core annuity sales motions. Commercial remaining performance obligation increased 20% and 21% in constant-currency to $269 billion. Roughly 40% will be recognized in revenue in the next 12 months, up 18% year-over-year. The remaining portion recognized beyond the next 12 months increased 21%. And this quarter, our annuity mix was 97%. At a Company level, Activision contributed a net impact of approximately 3 points to revenue growth was a 2 point drag on operating income growth, and had a negative $0.06 impact to earnings per share.\n",
            "\n",
            "A reminder that this net impact includes adjusting for the movement of Activision content from our prior relationship as a third-party partner to first-party and includes $938 million from purchase accounting adjustments, integration, and transaction-related costs. FX did not have a significant impact on our results and was roughly in line with our expectations on total company revenue, segment-level revenue, COGS, and operating expense growth. Microsoft Cloud revenue was $36.8 billion and grew 21% and 22% in constant currency, roughly in line with expectations. Microsoft Cloud gross margin percentage decreased roughly 2 points year-over-year to 69%, in line with expectations.\n",
            "\n",
            "Excluding the impact of the change in accounting estimate for useful lives, gross margin percentage decreased slightly, driven by sales mix-shift to Azure, partially offset by improvement in Azure even with the impact of scaling our AI infrastructure. Company gross margin dollars increased 14% and 15% in constant currency and gross margin percentage decreased slightly year-over-year to 70%. Excluding the impact of the change in accounting estimate, gross margin percentage increased slightly even with the impact from purchase accounting adjustments, integration, and transaction-related costs from the Activision acquisition.\n",
            "\n",
            "Operating expenses increased 13% with 9 points from the Activision acquisition. At a total company level, headcount at the end of June was 3% higher than a year ago. Operating income increased 15% and 16% in constant currency, and operating margins were 43%, relatively unchanged year-over-year. Excluding the impact of the change in accounting estimate, operating margins increased slightly driven by the higher gross margin noted earlier, and improved operating leverage through continued cost discipline.\n",
            "\n",
            "Now to our segment results. Revenue from Productivity and Business Processes was $20.3 billion and grew 11% and 12% in constant currency, slightly ahead of expectations, driven by better-than-expected results across all business units. Office Commercial revenue grew 12% and 13% in constant currency. Office 365 Commercial revenue increased 13% and 14% in constant currency with ARPU growth primarily from E5 momentum as well as Copilot for Microsoft 365.\n",
            "\n",
            "Paid Office 365 Commercial seats grew 7% year-over-year, with installed base expansion across all customer segments. Seat growth was again driven by our small and medium business and frontline worker offerings, although both segments continued to moderate. Office Commercial licensing declined 9% and 7% in constant currency with continued customer shift to Cloud offerings.\n",
            "\n",
            "Office consumer revenue increased 3% and 4% in constant currency, with continued momentum in Microsoft 365 subscriptions, which grew 10% to $82.5 million. LinkedIn revenue increased 10% and 9% in constant currency, driven by better-than-expected performance across all businesses. Dynamics revenue grew 16%, driven by Dynamics 365, which grew 19% and 20% in constant currency. We saw continued growth across all workloads and better-than-expected new business. Dynamics 365 now represents roughly 90% of total Dynamics revenue.\n",
            "\n",
            "Segment gross margin dollars increased 9% and 10% in constant currency, and gross margin percentage decreased roughly 1 point year-over-year. Excluding the impact of the change in accounting estimate, gross margin percentage decreased slightly, driven by Office 365 as we scale our AI infrastructure. Operating expenses increased 5%, and operating income increased 12% and 13% in constant currency.\n",
            "\n",
            "Next, the Intelligent Cloud segment. Revenue was $28.5 billion, increasing 19% and 20% in constant currency, in line with expectations. Overall, Server products and Cloud services revenue grew 21% and 22% in constant currency. Azure and other cloud services revenue grew 29% and 30% in constant currency, in line with expectations and consistent with Q3 when adjusting for the leap year.\n",
            "\n",
            "Azure growth included 8 points from AI services, where demand remained higher than our available capacity. In June, we saw slightly lower-than-expected growth in a few European geos. In our per-user business, the enterprise mobility and security installed base grew 10% to over 281 million seats, with continued impact from moderated growth in seats sold outside the Microsoft 365 suite. Therefore, our Azure consumption business continues to grow faster than total Azure.\n",
            "\n",
            "In our on-premises server business, revenue increased 2% and 3% in constant currency. Growth was driven by demand for our hybrid solutions, although with slightly lower-than-expected transactional purchasing. Enterprise and Partner Services revenue decreased 7% on a strong prior year comparable for enterprise support services. Segment gross margin dollars increased 16% and gross margin percentage decreased roughly 2 points year-over-year. Excluding the impact of the change in accounting estimate, gross margin percentage decreased slightly, driven by sales mix shift to Azure, partially offset by the improvement in Azure noted earlier, even with the impact of scaling our AI infrastructure. Operating expenses increased 5%, and operating income grew 22% and 23% in constant currency.\n",
            "\n",
            "Now to More Personal Computing. Revenue was $15.9 billion, increasing 14% and 15% in constant currency, with 12 points of net impact from the Activision acquisition. Results were above expectations, driven by Windows Commercial and Search. The PC market was as expected and Windows OEM revenue increased 4% year-over-year. Windows Commercial products and Cloud services revenue increased 11% and 12% in constant currency, ahead of expectations due to higher in-period revenue recognition from the mix of contracts. Devices revenue decreased 11% and 9% in constant currency, roughly in line with expectations as we remain focused on our higher-margin premium products.\n",
            "\n",
            "While early days, we're excited about the recent launch of our Copilot+ PCs. Search and News advertising revenue ex-TAC increased 19%, ahead of expectations, primarily due to improved execution. Healthy volume growth was driven by Bing and Edge. And in Gaming, revenue increased 44% with 48 points of net impact from the Activision acquisition. Xbox content and services revenue increased 61%, slightly ahead of expectations with 58 points of net impact from the Activision acquisition.\n",
            "\n",
            "Stronger-than-expected performance in first-party content was partially offset by third-party content performance. Xbox hardware revenue decreased 42% and 41% in constant currency. Segment gross margin dollars increased 21% with 10 points of net impact from the Activision acquisition. Gross margin percentage increased roughly 3 points year-over-year, primarily driven by sales mix shift to higher-margin businesses. Operating expenses increased 43% with 41 points from the Activision acquisition, operating income increased 5% and 6% in constant currency.\n",
            "\n",
            "Now back to total company results. Capital expenditures, including finance leases, were $19 billion, in line with expectations and cash paid for PP&E was $13.9 billion. Cloud and AI-related spend represents nearly all of our total capital expenditures. Within that, roughly half is for infrastructure needs where we continue to build and lease data centers that will support monetization over the next 15 years and beyond. The remaining Cloud and AI-related spend is primarily for servers, both CPUs and GPUs to serve customers based on demand signals. For the full fiscal year, the mix of our Cloud and AI-related spend was similar to Q4.\n",
            "\n",
            "Cash flow from operations was $37.2 billion, up 29% driven by strong Cloud billings and collections. Free cash flow was $23.3 billion, up 18% year-over-year, reflecting higher capital expenditures to support our Cloud and AI offerings. For the full year, cash flow from operations surpassed $100 billion for the first time, reaching $119 billion.\n",
            "\n",
            "This quarter, other income expense was negative $675 million more favorable than anticipated with lower-than-expected interest expense and higher-than-expected interest income. Our losses on investments accounted for under the equity method were as expected. Our effective tax rate was approximately 19%, higher than anticipated due to a state tax law signed in June that was affected retroactively.\n",
            "\n",
            "And finally, we returned $8.4 billion to shareholders through dividends and share repurchases, bringing our total cash return to shareholders to over $34 billion for the full fiscal year.\n",
            "\n",
            "Now moving to our outlook. My commentary for both the full year and next quarter is on a U.S. dollar basis unless specifically noted otherwise. Let me start with some full-year commentary for FY '25.\n",
            "\n",
            "First, FX. Assuming current rates remain stable, we expect FX to have no meaningful impact to full-year revenue, COGS, or operating expense growth. Next, we continue to expect double-digit revenue and operating income growth as we focus on delivering differentiated value for our customers. To meet the growing demand signal for our AI and Cloud products, we will scale our infrastructure investments with FY '25 capital expenditures expected to be higher than FY '24.\n",
            "\n",
            "As a reminder, these expenditures are dependent on demand signals and adoption of our services that will be managed through the year. As scaling these investments drives growth in COGS, we will remain disciplined on operating expense management. Therefore, we expect FY '25 OpEx growth to be in the single digits. And given our focused commitment to managing at the operating margin level, we still expect FY '25 operating margins to be down only about 1 point year-over-year. And finally, we expect our FY '25 effective tax rate to be around 19%.\n",
            "\n",
            "Now to the outlook for our first quarter. Based on current rates, we expect FX to decrease total revenue and segment-level revenue growth by less than 1 point. We expect FX to decrease COGS growth by less than 1 point and to have no meaningful impact to operating expense growth.\n",
            "\n",
            "In Commercial bookings, increased long-term commitments to our platform and strong execution across core annuity sales motions should drive healthy growth on a growing expiry base. As a reminder, larger long-term Azure contracts, which are more unpredictable in their timing, can derive increased quarterly volatility in our bookings growth rate.\n",
            "\n",
            "Microsoft Cloud gross margin percentage should be roughly 70%, down year-over-year, driven by the impact of scaling our AI infrastructure. We expect capital expenditures to increase on a sequential basis given our Cloud and AI demand as well as existing AI capacity constraints. As a reminder, there can be quarterly spend variability from Cloud infrastructure build-outs and the timing of delivery of finance leases.\n",
            "\n",
            "Next is segment guidance. In Productivity and Business Processes, we expect revenue to grow between 10% and 11% in constant currency, or $20.3 billion to $20.6 billion. In Office Commercial, revenue growth will again driven by Office 365 with seat growth across customer segments and ARPU growth through E5 and Copilot for Microsoft 365. We expect Office 365 revenue growth to be approximately 14% in constant currency.\n",
            "\n",
            "In our on-premises business, we expect revenue to decline in the mid-to-high teens. In Office Consumer, we expect revenue growth in the low to mid-single digits, driven by Microsoft 365 subscriptions. For LinkedIn, we expect revenue growth in the high single digits driven by continued growth across all businesses. And in Dynamics, we expect revenue growth in the low to mid-teens, driven by Dynamics 365. For Intelligent Cloud, we expect revenue to grow between 18% and 20% in constant currency, or $28.6 billion to $28.9 billion.\n",
            "\n",
            "Revenue will continue to be driven by Azure, which, as a reminder, can have quarterly variability primarily from our per-user business and in-period revenue recognition depending on the mix of contracts. In Azure, we expect Q1 revenue growth to be 28% to 29% in constant currency. Growth will continue to be driven by our consumption business, inclusive of AI, which is growing faster than total Azure. We expect the consumption trends from Q4 to continue through the first half of the year. This includes both AI demand impacted by capacity constraints and non-AI growth trends similar to June. Growth in our per-user business will continue to moderate.\n",
            "\n",
            "And in H2, we expect Azure growth to accelerate as our capital investments create an increase in available AI capacity to serve more of the growing demand.\n",
            "\n",
            "In our on-premises server business, we expect revenue to decline in the low single digits as continued hybrid demand will be more than offset by lower transactional purchasing. And in Enterprise and Partner Services revenue should decline in the low single digits. And More Personal Computing, we expect revenue to grow between 9% and 12% in constant currency or $14.9 billion to $15.3 billion. Windows OEM revenue growth should be relatively flat, roughly in line with the PC market.\n",
            "\n",
            "In Windows Commercial products and Cloud services, customer demand for Microsoft 365 and our advanced security solutions should drive revenue growth in the mid-single digits. As a reminder, our quarterly revenue growth can have variability primarily from in-period revenue recognition depending on the mix of contracts. In Devices, revenue growth should be in the low to mid-single digits. Search and News advertising ex-TAC revenue growth should be in the mid-to-high teens. This will be higher than overall Search and News advertising revenue growth, which we expect to be in the low single digits.\n",
            "\n",
            "And in gaming, we expect revenue growth in the mid-30s, including approximately 40 points of net impact from the Activision acquisition. We expect Xbox content and services revenue growth in the low to mid-50s, driven by the net impact from the Activision acquisition. Hardware revenue will again decline year-over-year.\n",
            "\n",
            "Now back to Company guidance. We expect COGS between $19.95 billion to $20.15 billion, including approximately $700 million from purchase accounting, integration, and transaction-related costs from the Activision acquisition. We expect operating expense of $15.2 billion to $15.3 billion, including approximately $200 million from purchase accounting, integration, and transaction-related costs from the Activision acquisition.\n",
            "\n",
            "Other income and expense should be roughly negative $650 million, driven by losses on investments accounted for under the equity method as interest income will be mostly offset by interest expense. As a reminder, we are required to recognize gains or losses on our equity investments, which can increase quarterly volatility. We expect our Q1 effective tax rate to be approximately 19%.\n",
            "\n",
            "In closing, we remain focused on delivering innovations that matter to our global customers of every size. That focus extends to delivering on our financial commitments as well. We delivered operating margin growth of nearly 3 points year-over-year even as we accelerate our AI investments, completed the Activision acquisition, and had a headwind from the change in useful lives last year.\n",
            "\n",
            "So as we begin FY '25, we will continue to invest in the Cloud and AI opportunity ahead, aligned and if needed, adjusted to the demand signals we see. We are committed to growing our leadership across our Commercial Cloud and within that, the AI platform, and we feel well positioned as we start FY '25.\n",
            "\n",
            "With that, let's go to Q&A, Brett.\n",
            "\n",
            "Brett IversenVice President of Investor Relations at Microsoft\n",
            "\n",
            "Thanks, Amy. We'll now move over to Q&A. Out of respect for others on the call, we request that participants please only ask one question. Operator, can you please repeat your instructions?\n",
            "\n",
            "Read more[Skip to Participants](#participants)\n",
            "#### Questions and Answers\n",
            "\n",
            "Operator\n",
            "\n",
            "[Operator Instructions] Our first question comes from the line of Keith Weiss with Morgan Stanley. Please proceed.\n",
            "\n",
            "Keith Weiss Analyst at Morgan Stanley\n",
            "\n",
            "Excellent. Thank you guys for taking the question and congratulations on another great quarter and really solid overall fiscal year. Right now, there's an industry debate ranging around the capex requirements around Generative AI and whether the monetization is actually going to match with that. And I think the question for you guys, from a Microsoft perspective, is capex still an appropriate leading indicator for Cloud growth? Or does the shift in gross margin profile change that equation? Or said another way, maybe can you give us a little bit more help in understanding the timing between the capex investments and the yield on those investments? Thank you.\n",
            "\n",
            "Satya NadellaChairman and Chief Executive Officer at Microsoft\n",
            "\n",
            "Thank you, Keith. Let me start, and then Amy can add to this. I think I would say we primarily start right now from the demand side. What I mean by that is what's the product -- the shape of the product portfolio, what we learned even from the cloud transition, which, as you know, Keith, was similar in the sense that it was both a knowledge-intensive and a capital-intensive transition. We needed to have the product portfolio where there was the right mix of, I'll call it, infrastructure meters as well as SaaS applications. So that's the first thing that we are looking at.\n",
            "\n",
            "And how is that value landing with customers and what's the growth rate? So when I think about what's happening with M365 Copilot as perhaps the best Office 365 or M365 suite we have had, the fact that we're getting recurring customers, so our customers coming back buying more seats. So GitHub Copilot now being bigger than even GitHub when we bought it. What's happening in the contact center with Dynamics? So I would say -- and obviously, the Azure AI growth, that's the first place we look at. That then drives bulk of the capex spend, basically, that's the demand signal because you got to remember, even in the capital spend, there is land and there is data center build, but 60-plus percent is the kit, that only will be bought for inferencing and everything else if there is demand signal, right? So that's, I think, the key way to think about capital cycle even.\n",
            "\n",
            "The asset, as Amy said, is a long-term asset, which is land and the data center, which, by the way, we don't even construct things fully, we can even have things which are semi-constructive, we call coal, shelves, and so on. So we know how to manage our capex spend to build out a long-term asset and a lot of the hydration of the kit happens when we have the demand signal.\n",
            "\n",
            "There is definitely spend for training. Even there, of course, we will only be scaling training as we see the demand accrue in any given period in time. So I would say it's more important to manage to capture the opportunity with the right product portfolio that's driving value. And on that front, I feel good about the breadth of Microsoft offering, whether it's in consumer side, whether it's on commercial per seat side or on the consumption meters, that's, I think, the fundamental driver.\n",
            "\n",
            "Amy HoodChief Financial Officer at Microsoft\n",
            "\n",
            "And Keith, I do think -- and I really do appreciate how you phrased the question as well because I think the timing and some of the questions you all have had really led to how we were talking even about capital expense in our comments -- in my comments today. Being able to maybe share a little more about that when we talked about roughly half of FY '24's total capital expense as well as half of Q4's expense, it's really on land and build and finance leases, and those things really will be monetized over 15 years and beyond.\n",
            "\n",
            "And they're incredibly flexible because we've built a consistent architecture first with the Commercial Cloud and second with the Azure stack for AI, regardless of whether the demand is at the platform layer or at the app layer or through third parties and partners or, frankly, our first-party SaaS, it uses the same infrastructure. So it's a long-lived flexible asset.\n",
            "\n",
            "And if you think about it, that way, you can see what we're doing and focused on is building out this network in parallel across the globe. Because when we did this last transition, the first transition to the Cloud, which seems a long time ago sometimes. It rolled out quite differently. We rolled out more geo by geo and this one because we have demand on a global basis, we are doing it on a global basis, which is important. We have large customers in every geo. And so hopefully, with that sort of shape of our capital expense, it helps people see how much of that is sort of near-term monetization driver as well as a much longer duration.\n",
            "\n",
            "Keith Weiss Analyst at Morgan Stanley\n",
            "\n",
            "That's super helpful. Thank you very much.\n",
            "\n",
            "Brett IversenVice President of Investor Relations at Microsoft\n",
            "\n",
            "Thanks, Keith. Operator, next question, please.\n",
            "\n",
            "Operator\n",
            "\n",
            "And the next question comes from the line of Mark Moerdler with Bernstein Research. Please proceed.\n",
            "\n",
            "Mark Moerdler Analyst at Bernstein Research\n",
            "\n",
            "Thank you very much. Thank you for taking the question and congrats on a strong year. GenAI has been a bit of a roller coaster of a tech over the last year with periods of acceleration, high expectations, and the expectations drop as reality kicked in. With Azure growth we've seen this quarter and O365 Commercial, not yet fully visible in numbers even though Amy, you gave us a lot of color on it.\n",
            "\n",
            "Two quick parts to the question. Satya, how should we think about what it's going to take for GenAI to become more real across the industry and for it to become more visible within your SaaS offerings?\n",
            "\n",
            "And Amy, with Cloud, it took time for margins to improve. It looks like with AI, it's happening quicker. Can you give us a sense of how you think about the margin impact near term and long term from all the investment on AI? Thank you.\n",
            "\n",
            "Satya NadellaChairman and Chief Executive Officer at Microsoft\n",
            "\n",
            "Yeah. Thanks again, Mark, for the question. So to me, look, at the end of the day, GenAI is just software. So it is really translating into fundamentally growth on what has been our M365 SaaS offering with a newer offering that is the Copilot SaaS offering, which today is on a growth rate that's faster than any other previous generation of software we launched as a suite in M365. That's, I think, the best way to describe it. I mean the numbers I think we shared even this quarter are indicative of this, Mark. So if you look at it, we have both the landing of the seats itself quarter-over-quarter that growing 60%, right? That's a pretty good healthy sign.\n",
            "\n",
            "The most healthy sign for me is the fact that customers are coming back there. That is the same customers with whom we landed the seats coming back and buying more seats. And then the number of customers with 10,000-plus seats doubled, right? It's 2 times quarter-over-quarter. That, to me, is a healthy SaaS core business.\n",
            "\n",
            "And on top of that, some of the things that Amy shared are on Dynamic. That's another exciting place for us, which is one, we are gaining share. We are -- Dynamics with the Gen AI built-in is sort of really biz app, it's probably the category that gets completely transformed with Gen AI. Contact centers being a great example. We ourselves are on course to save hundreds of millions of dollars in our own customer support and contact center operations. I think we can drive that value to our customers.\n",
            "\n",
            "And then on the Azure side, you see the numbers very clearly. In fact, I think last quarter is when we started giving you that. You saw an acceleration of that this quarter. One of the other pieces, Mark, is AI doesn't sit on its own, right? So it's just for -- we have a concept of design wins in Azure. So in fact, 50% of the folks who are using Azure AI are also using a data meter. That's very exciting to us because the most important thing in Azure is to win workloads in the enterprise. And that is starting to happen. And these are generational things once they get going with you. So that's, I think, how we think about it at least when I look at what's happening on our demand side.\n",
            "\n",
            "Amy HoodChief Financial Officer at Microsoft\n",
            "\n",
            "And, Mark, to answer the second half of your question on margin improvement, looking different than it did through the last cloud cycle. That's primarily for a reason I've mentioned a couple of times. We have a consistent platform. So because we're building to one Azure AI stack, we don't have to have multiple infrastructure investments. We're making one. We're using that internally first party, and that's what we're using with customers to build on as well as ISVs. So it does, in fact, make margins start off better and obviously scale consistently.\n",
            "\n",
            "Mark Moerdler Analyst at Bernstein Research\n",
            "\n",
            "Thank you.\n",
            "\n",
            "Brett IversenVice President of Investor Relations at Microsoft\n",
            "\n",
            "Thanks, Mark. Operator, next question, please.\n",
            "\n",
            "Operator\n",
            "\n",
            "The next question comes from the line of Kash Rangan with Goldman Sachs. Please proceed.\n",
            "\n",
            "Kash Rangan Analyst at The Goldman Sachs Group\n",
            "\n",
            "Hi, thank you very much, and congrats on a great year -- fiscal year ending. A question for you, Amy. When you look at the capex, how do you ring efficiencies out of the capex? You've disclosed that 50% of the infrastructure, the other 50% tech, is very useful. So in other words, do you have to keep growing capex at these elevated rates? Or could you slow down capex and still get that consistent revenue growth rate in your Azure and Generative AI? That's the main question in my mind. Thank you so much.\n",
            "\n",
            "Amy HoodChief Financial Officer at Microsoft\n",
            "\n",
            "Thanks, Kash. That's a very good question. There's really two pieces, I think, as I heard your question that I would reflect on. The first is, could we see sort of consistent revenue growth without maybe what you would say is more of this sort of elevated capital expense number or something that continues to accelerate?\n",
            "\n",
            "And the answer to that is yes because there's two different pieces, right? You're seeing half of this go toward long-term builds that Satya mentioned, the pace at which we fill those builds with CPUs or GPUs will be demand-driven. And so if we see differences in demand signal, we can throttle that investment on the CPU side, which we've done for I guess, a long time at this point, as I reflect, and we'll use all that same learning and demand signal understand to do the same thing on the GPU side.\n",
            "\n",
            "And so you're right that you could see relatively consistent revenue patterns and yet see these inconsistencies and capital spend quarter-to-quarter.\n",
            "\n",
            "The other thing I would note, Kash, is you'll also notice there's a growing distinction between our capex number, and on occasion, the cash that we pay for PP&E and you're going to start to see that more often in this period because it happens when we use leases. Leases sort of show up all at once. And so you'll see a little bit more volatility. I've mentioned it back in my comments before, but I mentioned it again just because you're starting to see that distinction in my comments and hopefully that's helpful context.\n",
            "\n",
            "Satya NadellaChairman and Chief Executive Officer at Microsoft\n",
            "\n",
            "Just one other thing, Amy, if I want to add. I think as people think about capital spend, I think it's important to separate out leases from build. And when it comes to build, I think it's important for us to think about -- we think about it in terms of what's the total percentage of cost that goes into each line item, land which obviously has a very different duration and a very different lead time.\n",
            "\n",
            "So those are the other two considerations. We think about lead time and duration of the asset. Land, network, construction, the system or the kit, and then the ongoing cost. And so if you think about it that way, then you know how to even adjust if you will, the capital spend based on demand signal.\n",
            "\n",
            "Kash Rangan Analyst at The Goldman Sachs Group\n",
            "\n",
            "Thank you. It was triggered by the jump in capex. And as Amy pointed out, you're guiding to accelerating -- Azure revenue growth rate, which, I guess, follows the capex surge. Thank you so much once again.\n",
            "\n",
            "Brett IversenVice President of Investor Relations at Microsoft\n",
            "\n",
            "Thanks, Kash. Operator, next question, please.\n",
            "\n",
            "Operator\n",
            "\n",
            "The next question comes from the line of Brent Thill with Jefferies. Please proceed.\n",
            "\n",
            "Brent Thill Analyst at Jefferies Financial Group\n",
            "\n",
            "Thanks. Amy, the magnitude to beat this quarter was a little lower than we've seen in the past. Was there anything unusual on the sales cycle that close rate that you saw off? Thanks.\n",
            "\n",
            "Amy HoodChief Financial Officer at Microsoft\n",
            "\n",
            "Thanks, Brent. Actually, no. As I was talking on the quarter, I mean Commercial bookings were much better than we expected going into the quarter. Commitments were very good execution across both the core sort of annuity renewal motion was good, as expected, the larger long-term commitments were better than we expected. So, Brent, I would not say there was anything really unusual in how I thought about what we saw in our commercial execution through the quarter.\n",
            "\n",
            "Brent Thill Analyst at Jefferies Financial Group\n",
            "\n",
            "Great. Thank you.\n",
            "\n",
            "Brett IversenVice President of Investor Relations at Microsoft\n",
            "\n",
            "Thanks, Brent. Operator, next question, please. The next question comes from the line of Karl Keirstead with UBS. Please proceed.\n",
            "\n",
            "Karl Keirstead Analyst at UBS Group\n",
            "\n",
            "Okay. Great. So maybe I'll direct this to Amy. Amy, I know when you set your Azure guidance, you're always looking to meet or beat the high end the 30% you put up in the June quarter, amazing number given the scale of Azure, but it did come in at the low end of your range. And I'd just love for you to maybe elaborate on the delta. I guess as I reflect on what you said in your comments, there's two things that I heard you say. One, it sounded like there's persistent capacity constraints that you think might get alleviated in the second half. And then secondly, you mentioned perhaps some modest softness in Europe. I presume that's a little bit more economic rather than Azure-specific. Is that the right way to frame the performance in the quarter? Thank you.\n",
            "\n",
            "Amy HoodChief Financial Officer at Microsoft\n",
            "\n",
            "Thanks, Karl. Yes, that's exactly right. Maybe I'll just repeat it, just so people can hear it in my words as well to that 30% to 31% guide for Q4 and coming in at the lower end of 30%. You're exactly right. The distinguishing between being at the higher end or at the lower end really was some softness we saw in a few European geos on non-AI consumption really made the difference in that number. And we've assumed that going forward into H1 inclusive of my guide 28% to 29% going forward.\n",
            "\n",
            "And then let me separate which was your larger point, which is what are the other factors you see ongoing. Number one, you're right, capacity constraints, particularly on AI and Azure will remain in Q4 and will remain in H1. So hopefully, that's helpful.\n",
            "\n",
            "Karl Keirstead Analyst at UBS Group\n",
            "\n",
            "Yeah. Thank you, Amy.\n",
            "\n",
            "Brett IversenVice President of Investor Relations at Microsoft\n",
            "\n",
            "Thanks, Karl. Operator, next question, please.\n",
            "\n",
            "Operator\n",
            "\n",
            "The next question comes from the line of Brad Zelnick with Deutsche Bank. Please proceed.\n",
            "\n",
            "Brad Zelnick Analyst at Deutsche Bank Aktiengesellschaft\n",
            "\n",
            "Great. Thank you very much. Amy, with Azure demand, once again greater than available capacity, I appreciate the capex investments and the build-out and acceleration you expect in the back half. But as we think about Cloud capacity and AI services specifically, can you talk about both the near-term and long-term strategy around the AI partnerships that you're signing with the likes of Oracle and Cohere, for example? Thank you.\n",
            "\n",
            "Amy HoodChief Financial Officer at Microsoft\n",
            "\n",
            "Thanks, Brad. Maybe separate a couple of things. We are -- and we've talked about now for quite a few quarters, we are constrained on AI capacity. And because of that, actually, we've, to your point, have signed up with third parties to help us as we are behind with some leases on AI capacity. We've done that with partners who are happy to help us extend the Azure platform, to be able to serve this Azure AI demand. And you do see us investing quite a bit as we've talked about in builds so that we can get back in a more balanced place.\n",
            "\n",
            "Satya NadellaChairman and Chief Executive Officer at Microsoft\n",
            "\n",
            "Yeah. I mean, to me, it's no different than leases that we would have done in the past. These -- even say sometimes buying from Oracle, maybe even more efficient leases because they're even shorter date.\n",
            "\n",
            "Brad Zelnick Analyst at Deutsche Bank Aktiengesellschaft\n",
            "\n",
            "Excellent. Thanks for the color.\n",
            "\n",
            "Brett IversenVice President of Investor Relations at Microsoft\n",
            "\n",
            "Thanks, Brad. Operator, next question, please.\n",
            "\n",
            "Operator\n",
            "\n",
            "The next question comes from the line of Mark Murphy with JPMorgan. Please proceed.\n",
            "\n",
            "Mark Murphy Analyst at JPMorgan Chase & Co.\n",
            "\n",
            "Thank you very much. With a couple of quarters of Copilot for M365 availability under your belt now, how are you assessing the capability of Copilots to replicate the productivity gains that they've created for developers, which seem to be very high, and to do something similar for the broader population of knowledge workers? For instance, you're mentioning the 10,000 feet deals, the repeat purchases, is it possible to eventually see Copilot penetration rate equally high in Office as they will be in GitHub?\n",
            "\n",
            "Satya NadellaChairman and Chief Executive Officer at Microsoft\n",
            "\n",
            "Yeah, that's a great question. In fact, the GitHub design system and the GitHub Copilot workspace design system, which now, for example, you start with an issue, you create a plan, from a plan, you create a spec, or you create a spec and from a spec, you create a plan and then you go operate across the full repo. That's effectively the design system that is getting replicated inside of even the M365 Copilot.\n",
            "\n",
            "And you see this even now -- for example, you get an email, you're in sales, you want to respond to the customer. The data from the email is essentially context for a prompt but you expand by bringing in all of your CRM data, right? So this customer email is in the context of some order, all of the CRM record gets completed in context and a reply gets generated with the CRM data. That's the type of stuff that's already happening.\n",
            "\n",
            "Then you take something like Copilot Studio, you can start even grounding it in more data and then completing workflows. So you could say if this email comes from this customer whose order date is got a particular issue with it. You can then go and escalate it to somebody else who gets a notification in Teams.\n",
            "\n",
            "And those are the kinds of workflows that are getting built within IT or by end users themselves, what used to be line of business applications to us are Copilot extensions going forward. So we think of this as really a new design system for knowledge and frontline work to drive productivity, which would be very akin to what has happened in software engineering. So when you think about marketing or finance or sales or customer service, we will effectively replicate what you just said, which is the type of productivity we've seen in developers, will come to all of these functions as they think about their work, workflow and workout effect, all being driven by Copilots.\n",
            "\n",
            "Mark Murphy Analyst at JPMorgan Chase & Co.\n",
            "\n",
            "Thank you very much.\n",
            "\n",
            "Brett IversenVice President of Investor Relations at Microsoft\n",
            "\n",
            "Thanks, Mark. Operator, we have time for one last question.\n",
            "\n",
            "Operator\n",
            "\n",
            "And the last question will come from the line of Keith Bachman with BMO Capital Markets. Please proceed.\n",
            "\n",
            "Keith Bachman Analyst at BMO Capital Markets\n",
            "\n",
            "Hi, good evening, and thank you for the opportunity to ask the question. I actually wanted to veer towards gaming, if I could, for a second. Xbox Content Services revenue grew 61%, 58 points held from Activision. So the net is about 3 points of growth. How should investors think about the longer-term growth potential in this area? You've made significant investments, including the Activision deal. But how should investors be thinking about the growth potential of the gaming? Or what are the puts and takes to help make considerations here? Thank you.\n",
            "\n",
            "Satya NadellaChairman and Chief Executive Officer at Microsoft\n",
            "\n",
            "Yeah. For us, our investment in gaming fundamentally was to have, I would say, the right portfolio of both what we love about gaming and always have loved about gaming, which is Xbox, and the content for the console and expand from there so that we have content for everywhere people play games, starting with the PC.\n",
            "\n",
            "So when I think about the Activision portfolio, it comes with great assets for us to cover both the PC and the console. And then, of course, assets to cover mobile sockets, which we never have. So we feel that now we have both the content and the ability to access all the traditional high-scale platforms where people play games, which is the console, PC, and mobile.\n",
            "\n",
            "But we're also excited about these new sockets, right? I mean the fact that even in this last quarter, we expanded X Cloud to Amazon TV, I forget the name of what it's called. But that's the type of new access that really helps us a lot, get reach new gamers or the same gamer everywhere they want to play. And that ultimately will show up in that software plus services and transaction revenue for us, which is really our long-term KPI, and that's what we're building towards. And that was the strategy behind Activision as an asset.\n",
            "\n",
            "Amy, if you want to add to it?\n",
            "\n",
            "Amy HoodChief Financial Officer at Microsoft\n",
            "\n",
            "No, I do think the real goal here is to be able to take a broad set of content to more users in more places, and really build what looks more like to us, the software annuity and subscription business. With enhanced transactions and the ownership of IP, which is quite valuable long term. As Satya mentioned, things where with the ownership of IP, it can be monetized in multiple ways. And I think we're really encouraged by some of the progress and how we're making progress with Game Pass as well with some of the new announcements. Thank you, Keith.\n",
            "\n",
            "Brett IversenVice President of Investor Relations at Microsoft\n",
            "\n",
            "Thanks, Keith. That wraps up the Q&A portion of today's earnings call. Thank you for joining us today, and we look forward to speaking with all of you soon.\n",
            "\n",
            "Amy HoodChief Financial Officer at Microsoft\n",
            "\n",
            "Thank you.\n",
            "\n",
            "Satya NadellaChairman and Chief Executive Officer at Microsoft\n",
            "\n",
            "Thank you all.\n",
            "\n",
            "Operator\n",
            "\n",
            "[Operator Closing Remarks]\n",
            "\n",
            "Read more\n",
            "#### Participants\n",
            "\n",
            "##### Corporate Executives\n",
            "\n",
            "* **Brett Iversen**Vice President of Investor Relations\n",
            "* [Satya Nadella](https://www.insidertrades.com/microsoft-co-stock/satya-nadella/)Chairman and Chief Executive Officer\n",
            "* [Amy Hood](https://www.insidertrades.com/microsoft-co-stock/amy-hood/)Chief Financial Officer\n",
            "##### Analysts\n",
            "\n",
            "* [Keith Weiss](/all-access/analyst-rankings/details/?AnalystName=Keith Weiss&FirmID=71&Type=0), [Morgan Stanley](/all-access/brokerage-rankings/details/?id=71&Type=0)\n",
            "* **Mark Moerdler**, Bernstein Research\n",
            "* [Kash Rangan](/all-access/analyst-rankings/details/?AnalystName=Kash Rangan&FirmID=8&Type=0), [The Goldman Sachs Group, Inc.](/all-access/brokerage-rankings/details/?id=8&Type=0)\n",
            "* [Brent Thill](/all-access/analyst-rankings/details/?AnalystName=Brent Thill&FirmID=149&Type=0), [Jefferies Financial Group Inc.](/all-access/brokerage-rankings/details/?id=149&Type=0)\n",
            "* [Karl Keirstead](/all-access/analyst-rankings/details/?AnalystName=Karl Keirstead&FirmID=2&Type=0), [UBS Group AG](/all-access/brokerage-rankings/details/?id=2&Type=0)\n",
            "* [Brad Zelnick](/all-access/analyst-rankings/details/?AnalystName=Brad Zelnick&FirmID=109&Type=0), [Deutsche Bank Aktiengesellschaft](/all-access/brokerage-rankings/details/?id=109&Type=0)\n",
            "* [Mark Murphy](/all-access/analyst-rankings/details/?AnalystName=Mark Murphy&FirmID=43&Type=0), [JPMorgan Chase & Co.](/all-access/brokerage-rankings/details/?id=43&Type=0)\n",
            "* [Keith Bachman](/all-access/analyst-rankings/details/?AnalystName=Keith Bachman&FirmID=19&Type=0), [BMO Capital Markets](/all-access/brokerage-rankings/details/?id=19&Type=0)\n",
            "\n",
            "[![Alpha Street Logo](/images/alphastreet.png)](https://www.alphastreet.com/)\n",
            "\n",
            "#### Conference Call Audio\n",
            "\n",
            "Your browser does not support the audio element. [Listen Now.](https://files.quartr.com/audio-files/6c725-2024-07-30-11-59-07.mpeg?ref=bWFya2V0YmVhdA==)[Powered by ![Quartr](/images/quartr-logotype-svg.svg)](https://quartr.com)\n",
            "#### Transcript Sections\n",
            "\n",
            "* [Presentation](#presentation)\n",
            "* [Questions and Answers](#questions-and-answers)\n",
            "* [Participants](#participants)\n",
            "\n",
            "## Microsoft Earnings Headlines\n",
            "\n",
            "![Ciena network hardware](https://www.marketbeat.com/logos/articles/thumb_20241223122954_ciena-rebounds-ai-and-strong-guidance-drive-post-e.png)[Ciena Rebounds: AI and Strong Guidance Drive Post-Earnings Surge (MSFT)](https://www.marketbeat.com/originals/ciena-rebounds-ai-and-strong-guidance-drive-post-earnings-surge/)Ciena stock jumped after issuing strong revenue growth guidance through 2027. Find out how AI and network upgrades are driving the momentum.December 25, 2024 | marketbeat.com![](/images/news-sites/tipranks.png)[Microsoft EVP Christopher D. Young Resigns](https://www.tipranks.com/news/company-announcements/microsoft-evp-christopher-d-young-resigns?utm_source=marketbeat.com&utm_medium=referral)January 22 at 6:10 PM | tipranks.com![](https://www.marketbeat.com/images/webpush/files/thumb_2057push_trump_new-9.jpg)[Stunning Trump Manhattan Project Now Underway](https://www.marketbeat.com/scripts/redirect.aspx?TriggeredCampaignID=11478&UserID=0&Placement=NativeDisplay&Source=NativeDisplay&interstitial=1)Just as I predicted, President Trump announced his #1 move during his first 100 days...\n",
            "His allies are calling it Trump’s Manhattan Project.![](https://www.marketbeat.com/scripts/TriggeredCampaignPixel.ashx?TriggeredCampaignID=11478&Placement=NativeDisplay&Source=NativeDisplay)January 22, 2025 | Banyan Hill Publishing (Ad)![](/images/news-sites/nytimes.jpg)[As D.E.I. Programs Come Under Attack, Companies Like Costco and Microsoft Forge Ahead](https://www.nytimes.com/2025/01/22/business/dei-programs-initiatives-costco-microsoft.html)January 22 at 6:10 PM | nytimes.com![](/images/news-sites/cnbc.jpg)[Microsoft's business development chief Chris Young resigns](https://www.cnbc.com/2025/01/22/microsofts-business-development-chief-chris-young-resigns.html)January 22 at 6:05 PM | cnbc.com[Microsoft's relationship with OpenAI cracked when it hired Mustafa Suleyman, rival Marc Benioff says](https://techcrunch.com/2025/01/22/microsofts-relationship-with-openai-cracked-when-it-hired-mustafa-suleyman-rival-marc-benioff-says/)January 22 at 4:37 PM | techcrunch.com[See More Microsoft Headlines](/stocks/NASDAQ/MSFT/news/)\n",
            "\n",
            "Get Earnings Announcements in your inbox\n",
            "\n",
            "Want to stay updated on the latest earnings announcements and upcoming reports for companies like Microsoft? Sign up for **Earnings360's daily newsletter** to receive timely earnings updates on Microsoft and other key companies, straight to your email.\n",
            "\n",
            "Email Address\n",
            "\n",
            "## About Microsoft\n",
            "\n",
            "[Microsoft (NASDAQ:MSFT)](/stocks/NASDAQ/MSFT/) was founded in 1975 in Albuquerque, New Mexico by Bill Gates and Paul Allen. The two quit their respective Harvard schooling and programming jobs to start a software company focused on the then-popular Altair 8800. Originally named Micro-Soft, Microsoft is a portmanteau of the words microprocessor and software. The company quickly took off and was relocated to Washington State where it is headquartered today.\n",
            "\n",
            "Microsoft launched a game called Flight Simulator in 1982 that has since become the longest-running video game franchise. The company’s first major breakthrough came in the early 80s when it licensed MS-DOS to IBM for their personal computer and then another came in 1985 the company altered the way computers were used when it launched Windows. Windows used a graphical interface to display information that included drop-down menus, scroll bars, and other features commonly found in operating systems today.\n",
            "\n",
            "Microsoft went public in 1986 making founder Bill Gates the world’s youngest billionaire. Other innovations that helped make the company’s name include Windows 95 which included many upgrades to the original and, when the Internet took off, Internet Explorer. Bill Gates gave up his role as CEO in 2000 and the company is now run by Satya Nadella. Mr. Nadella took over the role of CEO in 2014 and then the role of chairman in 2021.\n",
            "\n",
            "Today the company develops, licenses, and supports software, services, devices, and solutions worldwide. The company operates in three segments that include Productivity and Business Processes, Intelligent Cloud, and More Personal Computing. As of 2022, Microsoft’s Azure powered more than 20% of the Cloud putting it in second place globally.\n",
            "\n",
            "The Productivity and Business Processes segment offers several software solutions including Office, Exchange, SharePoint, Microsoft Teams, Office 365 Security and Compliance, Microsoft Viva, and Skype for Business. Microsoft also operates Skype, Outlook.com, OneDrive, and LinkedIn for business professionals as well as Dynamics 365. Dynamics 365 is a set of cloud-based and on-premises business solutions for organizations and enterprises of all sizes.\n",
            "\n",
            "The Intelligent Cloud segment licenses SQL, Windows Servers, Visual Studio, System Center, and related Client Access Licenses. Microsoft Intelligent Cloud also operates GitHub which is a collaboration platform and code hosting service for developers. Intelligent Cloud also hosts Nuance healthcare and enterprise AI solutions, and Azure, a cloud platform, as well as associated services.\n",
            "\n",
            "The More Personal Computing segment provides Windows original equipment manufacturer (OEM) licensing and other licensing of the Windows family of operating systems. This includes Windows Commercial, Windows cloud services, and Windows Internet of Things.\n",
            "\n",
            "The More Personal Computing segment also offers Surface, PC accessories, PCs, tablets, gaming and entertainment consoles, Xbox hardware, and Xbox content and services including video games and third-party video game royalties. The company sells its products through OEMs, distributors, and resellers; and directly through digital marketplaces, online stores, and retail stores.\n",
            "\n",
            "*Written by [Jeffrey Neal Johnson](https://www.marketbeat.com/authors/jeffrey-neal-johnson/)*[View Microsoft Profile](/stocks/NASDAQ/MSFT/)Read more\n",
            "\n",
            "## More Earnings Resources from MarketBeat\n",
            "\n",
            "**Earnings Tools**\n",
            "\n",
            "* [Today's Earnings](/earnings/latest/)\n",
            "* [Tomorrow's Earnings](/earnings/tomorrow/)\n",
            "* [Next Week's Earnings](/earnings/next-week/)\n",
            "* [Upcoming Earnings Calls](/earnings/conference-calls/)\n",
            "* [Earnings Newsletter](/earnings/subscribe/)\n",
            "* [Earnings Call Transcripts](/earnings/transcripts/)\n",
            "* [Earnings Beats & Misses](/earnings/beats-and-misses/)\n",
            "* [Corporate Guidance](/earnings/guidance/)\n",
            "* [Earnings Screener](/all-access/earnings-screener/)\n",
            "\n",
            "**Earnings By Country**\n",
            "\n",
            "* [U.S. Earnings Reports](/earnings/latest/)\n",
            "* [Canadian Earnings Reports](/earnings/canada/)\n",
            "* [U.K. Earnings Reports](/earnings/uk/)\n",
            "\n",
            "**Latest Articles**\n",
            "\n",
            "* [Why Amazon’s Next Earnings Could Trigger a Stock Breakout](https://www.marketbeat.com/originals/why-amazons-next-earnings-could-trigger-a-stock-breakout/)\n",
            "* [S&P 500 Earnings Set to Shine: January's Critical Market Test](https://www.marketbeat.com/originals/s-and-p-500-earnings-set-to-shine-januarys-critical-market-test/)\n",
            "* [Ciena Rebounds: AI and Strong Guidance Drive Post-Earnings Surge](https://www.marketbeat.com/originals/ciena-rebounds-ai-and-strong-guidance-drive-post-earnings-surge/)\n",
            "* [History Hints at a Rebound for Toll Brothers After Earnings](https://www.marketbeat.com/originals/let-history-be-your-guide-with-tol-stock-after-earnings/)\n",
            "* [Alibaba Stock: Why Earnings Make It a Buy Despite the Recent Dip](https://www.marketbeat.com/originals/alibaba-stock-why-earnings-make-it-a-buy-despite-the-recent-dip/)\n",
            "* [MercadoLibre Down 23% After Missed Earnings: Time to Buy the Dip?](https://www.marketbeat.com/originals/mercadolibre-down-after-missed-earnings-time-to-buy-the-dip/)\n",
            "* [Roblox Stock Soars 22% After Q3 Earnings – A Turning Point Ahead?](https://www.marketbeat.com/originals/gaming-stock-soars-after-q3-earnings-a-turning-point-ahead/)\n",
            "\n",
            "**Upcoming Earnings**\n",
            "\n",
            "* [CSX (1/23/2025)](/stocks/NASDAQ/CSX/earnings/)\n",
            "* [Intel (1/23/2025)](/stocks/NASDAQ/INTC/earnings/)\n",
            "* [Texas Instruments (1/23/2025)](/stocks/NASDAQ/TXN/earnings/)\n",
            "* [Elevance Health (1/23/2025)](/stocks/NYSE/ELV/earnings/)\n",
            "* [Freeport-McMoRan (1/23/2025)](/stocks/NYSE/FCX/earnings/)\n",
            "* [General Electric (1/23/2025)](/stocks/NYSE/GE/earnings/)\n",
            "* [Union Pacific (1/23/2025)](/stocks/NYSE/UNP/earnings/)\n",
            "* [American Express (1/24/2025)](/stocks/NYSE/AXP/earnings/)\n",
            "* [HCA Healthcare (1/24/2025)](/stocks/NYSE/HCA/earnings/)\n",
            "* [NextEra Energy (1/24/2025)](/stocks/NYSE/NEE/earnings/)\n",
            "\n",
            "![Web Analytics Made Easy - StatCounter](https://c.statcounter.com/12590395/0/c4ac327f/1/)\n",
            "\n",
            "Get 30 Days of MarketBeat All Access for Free\n",
            "\n",
            "Sign up for MarketBeat All Access to gain access to MarketBeat's full suite of research tools.\n",
            "\n",
            "[Start Your 30-Day Trial](/subscribe/all-access/?referraltype=masterfooter)\n",
            "\n",
            "## MarketBeat All Access Features\n",
            "\n",
            "[### Best-in-Class Portfolio Monitoring\n",
            "\n",
            "* Get personalized stock ideas.\n",
            "* Compare portfolio to indices.\n",
            "* Check stock news, ratings, SEC filings, and more.](/subscribe/all-access/?referraltype=masterfooter)\n",
            "\n",
            "[### Stock Ideas and Recommendations\n",
            "\n",
            "* See daily stock ideas from top analysts.\n",
            "* Receive short-term trading ideas from MarketBeat.\n",
            "* Identify trending stocks on social media.](/subscribe/all-access/?referraltype=masterfooter)\n",
            "\n",
            "[### Advanced Stock Screeners and Research Tools\n",
            "\n",
            "* Use our seven stock screeners to find suitable stocks.\n",
            "* Stay informed with MarketBeat's real-time news.\n",
            "* Export data to Excel for personal analysis.](/subscribe/all-access/?referraltype=masterfooter)\n",
            "\n",
            "Sign in to your free account to enjoy these benefits\n",
            "\n",
            "* In-depth profiles and analysis for 20,000 public companies.\n",
            "* Real-time analyst ratings, insider transactions, earnings data, and more.\n",
            "* Our daily ratings and market update email newsletter.\n",
            "\n",
            "Sign in to your free account to enjoy all that MarketBeat has to offer.\n",
            "\n",
            "* [Sign In](#pnlLoginOnModal)\n",
            "* [Create Account](#pnlCreate)\n",
            "\n",
            "Your Email Address:\n",
            "\n",
            "Your Password:\n",
            "\n",
            "Log In\n",
            "\n",
            "---\n",
            "\n",
            "or\n",
            "\n",
            "![Facebook icon](/images/facebook-logo-2.png)\n",
            "![Google icon](/images/google-g-logo.png)Sign in with Google\n",
            "\n",
            "[Forgot your password?](/forgot-password/)\n",
            "\n",
            "Your Email Address:\n",
            "\n",
            "Choose a Password:\n",
            "\n",
            "Create My Account (Free)\n",
            "\n",
            "---\n",
            "\n",
            "or\n",
            "\n",
            "![Facebook icon](/images/facebook-logo-2.png)Sign in with Facebook\n",
            "![Google icon](/images/google-g-logo.png)Sign in with Google\n",
            "\n",
            "By creating a free account, you agree to our [terms of service.](/terms/) This site is protected by reCAPTCHA and the Google [Privacy Policy](https://policies.google.com/privacy) and [Terms of Service](https://policies.google.com/terms) apply.\n",
            "\n",
            "Featured By\n",
            "\n",
            "![MarketBeat - Stock Market News and Research Tools](/images/master/MarketBeat-logo-r-white.svg?v=2019)\n",
            "\n",
            "Empowering Individual Investors\n",
            "\n",
            "345 N Reid Place, Suite 620, Sioux Falls, SD 57103\n",
            "\n",
            "contact@marketbeat.com\n",
            "\n",
            "(844) 978-6257\n",
            "\n",
            "* [Twitter](https://twitter.com/MarketBeatCom)\n",
            "* [Facebook](https://www.facebook.com/marketbeatcom/)\n",
            "* [YouTube](https://www.youtube.com/marketbeatcom?sub_confirmation=1)\n",
            "* [LinkedIn](https://www.linkedin.com/company/marketbeat \"Follow MarketBeat on LinkedIn\")\n",
            "\n",
            "## About MarketBeat\n",
            "\n",
            "* [About](/about/)\n",
            "* [Editorial Guidelines](/editorial-guidelines/)\n",
            "* [Authors](/authors/)\n",
            "* [Press Room](/press-room/)\n",
            "* [Careers](/careers/)\n",
            "* [Contact](/contact/)\n",
            "* [FAQ](/faq/)\n",
            "* [Help](/help/)\n",
            "\n",
            "## MarketBeat Products\n",
            "\n",
            "* [Compare Products](/compare-products/)\n",
            "* [MarketBeat All Access](https://www.marketbeat.com/subscribe/all-access/?ProductCode=aap1997t&ReferralType=WebNavigation)\n",
            "* [Customer Reviews](/reviews/)\n",
            "* [MarketBeat Daily Ratings](/ratings/newsletter/)\n",
            "* [MarketBeat Daily Canada](/ratings/newsletter-canada/)\n",
            "* [MarketBeat CryptoBeat](/cryptocurrencies/newsletter/)\n",
            "* [MarketBeat Mobile App](/mobileapp/)\n",
            "\n",
            "## Popular Tools\n",
            "\n",
            "* [Stock Lists](/stocks/)\n",
            "* [Compare Stocks](/compare-stocks/)\n",
            "* [Dividend Calculator](/dividends/calculator/)\n",
            "* [My MarketBeat](/manage/watchlists/)\n",
            "* [Stock Screener](/stock-screener/)\n",
            "\n",
            "## Financial Calendars\n",
            "\n",
            "* [Analyst Ratings](/ratings/)\n",
            "* [Dividends](/dividends/)\n",
            "* [Earnings](/earnings/latest/)\n",
            "* [Insider Trades](/insider-trades/)\n",
            "* [Stock Market Holidays](/stock-market-holidays/)\n",
            "\n",
            "## Terms & Info\n",
            "\n",
            "* [Advertising](/advertising/)\n",
            "* [Accessibility Statement](/accessibility/)\n",
            "* [Do Not Sell My Information](/do-not-sell-my-information/)\n",
            "* [Privacy Policy](/terms/#privacy-policy)\n",
            "* [RSS Feeds](/rss-feeds/)\n",
            "* [Terms of Service](/terms/)\n",
            "* [Sitemap](/sitemap/)\n",
            "\n",
            "© MarketBeat Media, LLC 2010-2025. All rights reserved.\n",
            "\n",
            "© 2025   Fair market value prices are updated every minute and are provided by [Polygon.io](https://polygon.io/). Other market data provided is at least 10-minutes delayed and hosted by Barchart Solutions. Information is provided 'as-is' and solely for informational purposes, not for trading purposes or advice, and is delayed. To see all exchange delays and terms of use please see [Barchart's disclaimer](https://www.barchartmarketdata.com/terms).\n",
            "\n",
            "[My Account -](/manage/)\n",
            "\n",
            "* [My MarketBeat](/manage/watchlists/#portfolio)\n",
            "* [My Newsletter](/manage/watchlists/#newsletter)\n",
            "* [My Alerts](/manage/alerts/)\n",
            "* [My Subscriptions](/manage/subscriptions/)\n",
            "* [My Account Settings](/manage/)\n",
            "* [My Payment Settings](/manage/payments/)\n",
            "* [Log Out](/login/?cmd=logout)\n",
            "\n",
            "×\n",
            "\n",
            "![Web Analytics](//c.statcounter.com/7602069/0/74cbf3e8/1/)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from markitdown import MarkItDown\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "\n",
        "tavily_tool = TavilySearchResults(max_results=5,\n",
        "                                  search_depth='advanced',\n",
        "                                  include_answer=False,\n",
        "                                  include_raw_content=True)\n",
        "md = MarkItDown()\n",
        "\n",
        "@tool\n",
        "def search_web_extract_info(query: str) -> list:\n",
        "    \"\"\"Search the web for a query and extracts useful information from the search links\"\"\"\n",
        "    results = tavily_tool.invoke(query)\n",
        "    docs = []\n",
        "    for result in tqdm(results):\n",
        "        # Extracting all text content from the URL\n",
        "        try:\n",
        "            extracted_info = md.convert(result['url'])\n",
        "            text_title = extracted_info.title.strip()\n",
        "            text_content = extracted_info.text_content.strip()\n",
        "            docs.append(text_title + '\\n' + text_content)\n",
        "        except:\n",
        "            print('Extraction blocked for url: ', result['url'])\n",
        "            pass\n",
        "\n",
        "    return docs"
      ],
      "metadata": {
        "id": "0G6V3Kv1jwU0"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = search_web_extract_info('OpenAI GPT-4o')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qNB0fSmlhLQ",
        "outputId": "ff852d1e-aaf1-4b69-e516-744d923b6092"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-37-d65e085f7f43>:1: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  docs = search_web_extract_info('OpenAI GPT-4o')\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _PyDrive2ImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _GenerativeAIImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _OpenCVImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: APICoreClientInfoImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _BokehImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _AltairImportHook.find_spec() not found; falling back to find_module()\n",
            " 40%|████      | 2/5 [00:00<00:00, 13.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction blocked for url:  https://openai.com/index/hello-gpt-4o/\n",
            "Extraction blocked for url:  https://openai.com/index/gpt-4o-and-more-tools-to-chatgpt-free/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:01<00:00,  4.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction blocked for url:  https://openai.com/index/gpt-4/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "display(Markdown(docs[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "resources": {
            "http://localhost:8080/_next/static/media/openai-logomark.e026557a.svg": {
              "data": "",
              "ok": false,
              "headers": [
                [
                  "content-length",
                  "0"
                ]
              ],
              "status": 404,
              "status_text": ""
            },
            "http://localhost:8080/_next/image?url=https%3A%2F%2Favatars.githubusercontent.com%2Fu%2F96567547%3Fs%3D400%26u%3D08b9757200906ab12e3989b561cff6c4b95a12cb%26v%3D4&w=64&q=75": {
              "data": "",
              "ok": false,
              "headers": [
                [
                  "content-length",
                  "0"
                ]
              ],
              "status": 404,
              "status_text": ""
            }
          }
        },
        "id": "jHJb63LQm2If",
        "outputId": "2ebcd166-88f6-4af1-de57-540b2c08567d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Introduction to GPT-4o and GPT-4o mini | OpenAI Cookbook\n[Cookbook](/)Topics[About](/about)[API Docs](https://platform.openai.com/docs/introduction)[Contribute](https://github.com/openai/openai-cookbook)Toggle themeToggle themeSearch...⌘K\n# Introduction to GPT-4o and GPT-4o mini\n\n![Juston Forte](/_next/image?url=https%3A%2F%2Favatars.githubusercontent.com%2Fu%2F96567547%3Fs%3D400%26u%3D08b9757200906ab12e3989b561cff6c4b95a12cb%26v%3D4&w=64&q=75)![Verified](/_next/static/media/openai-logomark.e026557a.svg)Juston Forte(OpenAI)Jul 18, 2024[Open in Github](https://github.com/openai/openai-cookbook/blob/main/examples/gpt4o/introduction_to_gpt4o.ipynb)\n\n---\n\nGPT-4o (\"o\" for \"omni\") and GPT-4o mini are natively multimodal models designed to handle a combination of text, audio, and video inputs, and can generate outputs in text, audio, and image formats. GPT-4o mini is the lightweight version of GPT-4o.\n\n### [Background](#background)\n\nBefore GPT-4o, users could interact with ChatGPT using Voice Mode, which operated with three separate models. GPT-4o integrates these capabilities into a single model that's trained across text, vision, and audio. This unified approach ensures that all inputs — whether text, visual, or auditory — are processed cohesively by the same neural network.\n\nGPT-4o mini is the next iteration of this omni model family, available in a smaller and cheaper version. This model offers higher accuracy than GPT-3.5 Turbo while being just as fast and supporting multimodal inputs and outputs.\n\n### [Current API Capabilities](#current-api-capabilities)\n\nCurrently, the `gpt-4o-mini` model supports `{text, image}`, with `{text}` outputs, the same modalities as `gpt-4-turbo`. As a preview, we will also be using the `gpt-4o-audio-preview` model to showcase transcription though the GPT4o model.\n\n## [Getting Started](#getting-started)\n\n### [Install OpenAI SDK for Python](#install-openai-sdk-for-python)\n\n```\n%pip install --upgrade openai\n```\n### [Configure the OpenAI client and submit a test request](#configure-the-openai-client-and-submit-a-test-request)\n\nTo setup the client for our use, we need to create an API key to use with our request. Skip these steps if you already have an API key for usage.\n\nYou can get an API key by following these steps:\n\n1. [Create a new project](https://help.openai.com/en/articles/9186755-managing-your-work-in-the-api-platform-with-projects)\n2. [Generate an API key in your project](https://platform.openai.com/api-keys)\n3. (RECOMMENDED, BUT NOT REQUIRED) [Setup your API key for all projects as an env var](https://platform.openai.com/docs/quickstart/step-2-set-up-your-api-key)\n\nOnce we have this setup, let's start with a simple {text} input to the model for our first request. We'll use both `system` and `user` messages for our first request, and we'll receive a response from the `assistant` role.\n\n```\nfrom openai import OpenAI\nimport os\n\n## Set the API key and model name\nMODEL=\"gpt-4o-mini\"\nclient = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as an env var>\"))\n```\n```\ncompletion = client.chat.completions.create(\n  model=MODEL,\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Help me with my math homework!\"}, # <-- This is the system message that provides context to the model\n    {\"role\": \"user\", \"content\": \"Hello! Could you solve 2+2?\"}  # <-- This is the user message for which the model will generate a response\n  ]\n)\n\nprint(\"Assistant: \" + completion.choices[0].message.content)\n```\n```\nAssistant: Of course! \\( 2 + 2 = 4 \\).\n\n```\n## [Image Processing](#image-processing)\n\nGPT-4o mini can directly process images and take intelligent actions based on the image. We can provide images in two formats:\n\n1. Base64 Encoded\n2. URL\n\nLet's first view the image we'll use, then try sending this image as both Base64 and as a URL link to the API\n\n```\nfrom IPython.display import Image, display, Audio, Markdown\nimport base64\n\nIMAGE_PATH = \"data/triangle.png\"\n\n# Preview image for context\ndisplay(Image(IMAGE_PATH))\n```\n![image generated by notebook](data:image/png;base64...)\n#### [Base64 Image Processing](#base64-image-processing)\n\n```\n# Open the image file and encode it as a base64 string\ndef encode_image(image_path):\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n\nbase64_image = encode_image(IMAGE_PATH)\n\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown. Help me with my math homework!\"},\n        {\"role\": \"user\", \"content\": [\n            {\"type\": \"text\", \"text\": \"What's the area of the triangle?\"},\n            {\"type\": \"image_url\", \"image_url\": {\n                \"url\": f\"data:image/png;base64,{base64_image}\"}\n            }\n        ]}\n    ],\n    temperature=0.0,\n)\n\nprint(response.choices[0].message.content)\n```\n```\nTo find the area of the triangle, you can use the formula:\n\n\\[\n\\text{Area} = \\frac{1}{2} \\times \\text{base} \\times \\text{height}\n\\]\n\nIn the triangle you provided:\n\n- The base is \\(9\\) (the length at the bottom).\n- The height is \\(5\\) (the vertical line from the top vertex to the base).\n\nNow, plug in the values:\n\n\\[\n\\text{Area} = \\frac{1}{2} \\times 9 \\times 5\n\\]\n\nCalculating this:\n\n\\[\n\\text{Area} = \\frac{1}{2} \\times 45 = 22.5\n\\]\n\nThus, the area of the triangle is **22.5 square units**.\n\n```\n#### [URL Image Processing](#url-image-processing)\n\n```\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown. Help me with my math homework!\"},\n        {\"role\": \"user\", \"content\": [\n            {\"type\": \"text\", \"text\": \"What's the area of the triangle?\"},\n            {\"type\": \"image_url\", \"image_url\": {\n                \"url\": \"https://upload.wikimedia.org/wikipedia/commons/e/e2/The_Algebra_of_Mohammed_Ben_Musa_-_page_82b.png\"}\n            }\n        ]}\n    ],\n    temperature=0.0,\n)\n\nprint(response.choices[0].message.content)\n```\n```\nTo find the area of the triangle, you can use the formula:\n\n\\[\n\\text{Area} = \\frac{1}{2} \\times \\text{base} \\times \\text{height}\n\\]\n\nIn the triangle you provided:\n\n- The base is \\(9\\) (the length at the bottom).\n- The height is \\(5\\) (the vertical line from the top vertex to the base).\n\nNow, plug in the values:\n\n\\[\n\\text{Area} = \\frac{1}{2} \\times 9 \\times 5\n\\]\n\nCalculating this gives:\n\n\\[\n\\text{Area} = \\frac{1}{2} \\times 45 = 22.5\n\\]\n\nThus, the area of the triangle is **22.5 square units**.\n\n```\n## [Video Processing](#video-processing)\n\nWhile it's not possible to directly send a video to the API, GPT-4o can understand videos if you sample frames and then provide them as images.\n\nSince GPT-4o mini in the API does not yet support audio-in (as of July 2024), we'll use a combination of GPT-4o mini and Whisper to process both the audio and visual for a provided video, and showcase two usecases:\n\n1. Summarization\n2. Question and Answering\n### [Setup for Video Processing](#setup-for-video-processing)\n\nWe'll use two python packages for video processing - opencv-python and moviepy.\n\nThese require [ffmpeg](https://ffmpeg.org/about.html), so make sure to install this beforehand. Depending on your OS, you may need to run `brew install ffmpeg` or `sudo apt install ffmpeg`\n\n```\n%pip install opencv-python\n%pip install moviepy\n```\n### [Process the video into two components: frames and audio](#process-the-video-into-two-components-frames-and-audio)\n\n```\nimport cv2\nfrom moviepy import *\nimport time\nimport base64\n\n# We'll be using the OpenAI DevDay Keynote Recap video. You can review the video here: https://www.youtube.com/watch?v=h02ti0Bl6zk\nVIDEO_PATH = \"data/keynote_recap.mp4\"\n```\n```\ndef process_video(video_path, seconds_per_frame=2):\n    base64Frames = []\n    base_video_path, _ = os.path.splitext(video_path)\n\n    video = cv2.VideoCapture(video_path)\n    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n    fps = video.get(cv2.CAP_PROP_FPS)\n    frames_to_skip = int(fps * seconds_per_frame)\n    curr_frame=0\n\n    # Loop through the video and extract frames at specified sampling rate\n    while curr_frame < total_frames - 1:\n        video.set(cv2.CAP_PROP_POS_FRAMES, curr_frame)\n        success, frame = video.read()\n        if not success:\n            break\n        _, buffer = cv2.imencode(\".jpg\", frame)\n        base64Frames.append(base64.b64encode(buffer).decode(\"utf-8\"))\n        curr_frame += frames_to_skip\n    video.release()\n\n    # Extract audio from video\n    audio_path = f\"{base_video_path}.mp3\"\n    clip = VideoFileClip(video_path)\n    clip.audio.write_audiofile(audio_path, bitrate=\"32k\")\n    clip.audio.close()\n    clip.close()\n\n    print(f\"Extracted {len(base64Frames)} frames\")\n    print(f\"Extracted audio to {audio_path}\")\n    return base64Frames, audio_path\n\n# Extract 1 frame per second. You can adjust the `seconds_per_frame` parameter to change the sampling rate\nbase64Frames, audio_path = process_video(VIDEO_PATH, seconds_per_frame=1)\n\n```\n```\nMoviePy - Writing audio in data/keynote_recap.mp3\n\n```\n```\n\n```\n```\nMoviePy - Done.\nExtracted 218 frames\nExtracted audio to data/keynote_recap.mp3\n\n```\n```\n\n```\n```\n## Display the frames and audio for context\ndisplay_handle = display(None, display_id=True)\nfor img in base64Frames:\n    display_handle.update(Image(data=base64.b64decode(img.encode(\"utf-8\")), width=600))\n    time.sleep(0.025)\n\nAudio(audio_path)\n```\n![image generated by notebook](data:image/png;base64...)\n\nYour browser does not support the audio element.\n\n### [Example 1: Summarization](#example-1-summarization)\n\nNow that we have both the video frames and the audio, let's run a few different tests to generate a video summary to compare the results of using the models with different modalities. We should expect to see that the summary generated with context from both visual and audio inputs will be the most accurate, as the model is able to use the entire context from the video.\n\n1. Visual Summary\n2. Audio Summary\n3. Visual + Audio Summary\n\n#### [Visual Summary](#visual-summary)\n\nThe visual summary is generated by sending the model only the frames from the video. With just the frames, the model is likely to capture the visual aspects, but will miss any details discussed by the speaker.\n\n```\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n    {\"role\": \"system\", \"content\": \"You are generating a video summary. Please provide a summary of the video. Respond in Markdown.\"},\n    {\"role\": \"user\", \"content\": [\n        \"These are the frames from the video.\",\n        *map(lambda x: {\"type\": \"image_url\",\n                        \"image_url\": {\"url\": f'data:image/jpg;base64,{x}', \"detail\": \"low\"}}, base64Frames)\n        ],\n    }\n    ],\n    temperature=0,\n)\nprint(response.choices[0].message.content)\n```\n```\n# OpenAI Dev Day Summary\n\n## Overview\nThe video captures highlights from OpenAI's Dev Day, showcasing new advancements and features in AI technology, particularly focusing on the latest developments in the GPT-4 model and its applications.\n\n## Key Highlights\n\n### Event Introduction\n- The event is branded as \"OpenAI Dev Day,\" setting the stage for discussions on AI advancements.\n\n### Keynote Recap\n- The keynote features a recap of significant updates and innovations in AI, particularly around the GPT-4 model.\n\n### New Features\n- **GPT-4 Turbo**: Introduction of a faster and more efficient version of GPT-4, emphasizing improved performance and reduced costs.\n- **DALL-E 3**: Updates on the image generation model, showcasing its capabilities and integration with other tools.\n- **Custom Models**: Introduction of features allowing users to create tailored AI models for specific tasks.\n\n### Technical Innovations\n- **Function Calling**: Demonstration of how the model can handle complex instructions and execute functions based on user queries.\n- **JSON Mode**: A new feature that allows for structured data handling, enhancing the model's ability to process and respond to requests.\n\n### User Experience Enhancements\n- **Threading and Retrieval**: New functionalities that improve how users can interact with the model, making it easier to manage conversations and retrieve information.\n- **Code Interpreter**: Introduction of a tool that allows the model to execute code, expanding its utility for developers.\n\n### Community Engagement\n- The event emphasizes community involvement, encouraging developers to explore and utilize the new features in their applications.\n\n### Conclusion\n- The event wraps up with a call to action for developers to engage with the new tools and features, fostering innovation in AI applications.\n\n## Closing Remarks\nThe OpenAI Dev Day serves as a platform for showcasing the latest advancements in AI technology, encouraging developers to leverage these innovations for enhanced applications and user experiences.\n\n```\n\nThe results are as expected - the model is able to capture the high level aspects of the video visuals, but misses the details provided in the speech.\n\n#### [Audio Summary](#audio-summary)\n\nThe audio summary is generated by sending the model the audio transcript. With just the audio, the model is likely to bias towards the audio content, and will miss the context provided by the presentations and visuals.\n\n`{audio}` input for GPT-4o is currently in preview, but will be incorporated into the base model in the near future. Because of this, we will use the `gpt-4o-audio-preview` model to process the audio.\n\n```\n#transcribe the audio\nwith open(audio_path, 'rb') as audio_file:\n    audio_content = base64.b64encode(audio_file.read()).decode('utf-8')\n\nresponse = client.chat.completions.create(\n            model='gpt-4o-audio-preview',\n            modalities=[\"text\"],\n            messages=[\n                    {   \"role\": \"system\",\n                        \"content\":\"You are generating a transcript. Create a transcript of the provided audio.\"\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": [\n                            {\n                                \"type\": \"text\",\n                                \"text\": \"this is the audio.\"\n                            },\n                            {\n                                \"type\": \"input_audio\",\n                                \"input_audio\": {\n                                    \"data\": audio_content,\n                                    \"format\": \"mp3\"\n                                }\n                            }\n                        ]\n                    },\n                ],\n            temperature=0,\n        )\n\n# Extract and return the transcription\ntranscription = response.choices[0].message.content\nprint (transcription)\n```\n\nLooking good. Now let's summarize this and format in markdown.\n\n```\n#summarize the transcript\nresponse = client.chat.completions.create(\n            model=MODEL,\n            modalities=[\"text\"],\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are generating a transcript summary. Create a summary of the provided transcription. Respond in Markdown.\"},\n                {\"role\": \"user\", \"content\": f\"Summarize this text: {transcription}\"},\n            ],\n            temperature=0,\n        )\ntranscription_summary = response.choices[0].message.content\nprint (transcription_summary)\n```\n```\n# OpenAI Dev Day Summary\n\nOn the inaugural OpenAI Dev Day, several significant updates and features were announced:\n\n- **Launch of GPT-4 Turbo**: This new model supports up to 128,000 tokens of context and is designed to follow instructions more effectively.\n\n- **JSON Mode**: A new feature that ensures the model responds with valid JSON.\n\n- **Function Calling**: Users can now call multiple functions simultaneously, enhancing the model's capabilities.\n\n- **Retrieval Feature**: This allows models to access external knowledge from documents or databases, improving their contextual understanding.\n\n- **Knowledge Base**: GPT-4 Turbo has knowledge up to April 2023, with plans for ongoing improvements.\n\n- **Dolly 3 and New Models**: The introduction of Dolly 3, GPT-4 Turbo with Vision, and a new Text-to-Speech model, all available via the API.\n\n- **Custom Models Program**: A new initiative where researchers collaborate with companies to create tailored models for specific use cases.\n\n- **Increased Rate Limits**: Established GPT-4 customers will see a doubling of tokens per minute, with options to request further changes in API settings.\n\n- **Cost Efficiency**: GPT-4 Turbo is significantly cheaper than its predecessor, with a 3x reduction for prompt tokens and 2x for completion tokens.\n\n- **Introduction of GPTs**: Tailored versions of ChatGPT designed for specific purposes, allowing users to create and share private or public GPTs easily, even without coding skills.\n\n- **Upcoming GPT Store**: A platform for users to share their GPT creations.\n\n- **Assistance API**: Features persistent threads, built-in retrieval, a code interpreter, and improved function calling to streamline user interactions.\n\nThe event concluded with excitement about the future of AI technology and an invitation for attendees to return next year to see further advancements.\n\n```\n\nThe audio summary is biased towards the content discussed during the speech, but comes out with much less structure than the video summary.\n\n#### [Audio + Visual Summary](#audio--visual-summary)\n\nThe Audio + Visual summary is generated by sending the model both the visual and the audio from the video at once. When sending both of these, the model is expected to better summarize since it can perceive the entire video at once.\n\n```\n## Generate a summary with visual and audio\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n    {\"role\": \"system\", \"content\":\"\"\"You are generating a video summary. Create a summary of the provided video and its transcript. Respond in Markdown\"\"\"},\n    {\"role\": \"user\", \"content\": [\n        \"These are the frames from the video.\",\n        *map(lambda x: {\"type\": \"image_url\",\n                        \"image_url\": {\"url\": f'data:image/jpg;base64,{x}', \"detail\": \"low\"}}, base64Frames),\n        {\"type\": \"text\", \"text\": f\"The audio transcription is: {transcription}\"}\n        ],\n    }\n],\n    temperature=0,\n)\nprint(response.choices[0].message.content)\n```\n```\n# OpenAI Dev Day Summary\n\n## Overview\nThe first-ever OpenAI Dev Day introduced several exciting updates and features, primarily focusing on the launch of **GPT-4 Turbo**. This new model enhances capabilities and expands the potential for developers and users alike.\n\n## Key Announcements\n\n### 1. **GPT-4 Turbo**\n- **Token Support**: Supports up to **128,000 tokens** of context.\n- **JSON Mode**: A new feature that ensures responses are in valid JSON format.\n- **Function Calling**: Improved ability to call multiple functions simultaneously and better adherence to instructions.\n\n### 2. **Knowledge Retrieval**\n- **Enhanced Knowledge Access**: Users can now integrate external documents or databases, allowing models to access updated information beyond their training cut-off (April 2023).\n\n### 3. **DALL-E 3 and Other Models**\n- Launch of **DALL-E 3**, **GPT-4 Turbo with Vision**, and a new **Text-to-Speech model** in the API.\n\n### 4. **Custom Models Program**\n- Introduction of a program where OpenAI researchers collaborate with companies to create tailored models for specific use cases.\n\n### 5. **Rate Limits and Pricing**\n- **Increased Rate Limits**: Doubling tokens per minute for established GPT-4 customers.\n- **Cost Efficiency**: GPT-4 Turbo is **3x cheaper** for prompt tokens and **2x cheaper** for completion tokens compared to GPT-4.\n\n### 6. **Introduction of GPTs**\n- **Tailored Versions**: GPTs are customized versions of ChatGPT designed for specific tasks, combining instructions, expanded knowledge, and actions.\n- **User-Friendly Creation**: Users can create GPTs through conversation, making it accessible even for those without coding skills.\n- **GPT Store**: A new platform for sharing and discovering GPTs, launching later this month.\n\n### 7. **Assistance API Enhancements**\n- Features include persistent threads, built-in retrieval, a code interpreter, and improved function calling.\n\n## Conclusion\nThe event highlighted OpenAI's commitment to enhancing AI capabilities and accessibility for developers. The advancements presented are expected to empower users to create innovative applications and solutions. OpenAI looks forward to future developments and encourages ongoing engagement with the community.\n\nThank you for attending!\n\n```\n\nAfter combining both the video and audio, we're able to get a much more detailed and comprehensive summary for the event which uses information from both the visual and audio elements from the video.\n\n### [Example 2: Question and Answering](#example-2-question-and-answering)\n\nFor the Q&A, we'll use the same concept as before to ask questions of our processed video while running the same 3 tests to demonstrate the benefit of combining input modalities:\n\n1. Visual Q&A\n2. Audio Q&A\n3. Visual + Audio Q&A\n```\nQUESTION = \"Question: Why did Sam Altman have an example about raising windows and turning the radio on?\"\n```\n```\nqa_visual_response = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n    {\"role\": \"system\", \"content\": \"Use the video to answer the provided question. Respond in Markdown.\"},\n    {\"role\": \"user\", \"content\": [\n        \"These are the frames from the video.\",\n        *map(lambda x: {\"type\": \"image_url\", \"image_url\": {\"url\": f'data:image/jpg;base64,{x}', \"detail\": \"low\"}}, base64Frames),\n        QUESTION\n        ],\n    }\n    ],\n    temperature=0,\n)\nprint(\"Visual QA:\\n\" + qa_visual_response.choices[0].message.content)\n```\n```\nVisual QA:\nSam Altman used the example of raising windows and turning the radio on to illustrate the concept of function calling in AI. This example demonstrates how AI can interpret natural language commands and translate them into specific function calls, making interactions more intuitive and user-friendly. By showing a relatable scenario, he highlighted the advancements in AI's ability to understand and execute complex tasks based on simple instructions.\n\n```\n```\nqa_audio_response = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n    {\"role\": \"system\", \"content\":\"\"\"Use the transcription to answer the provided question. Respond in Markdown.\"\"\"},\n    {\"role\": \"user\", \"content\": f\"The audio transcription is: {transcription}. \\n\\n {QUESTION}\"},\n    ],\n    temperature=0,\n)\nprint(\"Audio QA:\\n\" + qa_audio_response.choices[0].message.content)\n```\n```\nAudio QA:\nThe transcription provided does not include any mention of Sam Altman discussing raising windows or turning the radio on. Therefore, I cannot provide an answer to that specific question based on the given text. If you have more context or another transcription that includes that example, please share it, and I would be happy to help!\n\n```\n```\nqa_both_response = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n    {\"role\": \"system\", \"content\":\"\"\"Use the video and transcription to answer the provided question.\"\"\"},\n    {\"role\": \"user\", \"content\": [\n        \"These are the frames from the video.\",\n        *map(lambda x: {\"type\": \"image_url\",\n                        \"image_url\": {\"url\": f'data:image/jpg;base64,{x}', \"detail\": \"low\"}}, base64Frames),\n                        {\"type\": \"text\", \"text\": f\"The audio transcription is: {transcription}\"},\n        QUESTION\n        ],\n    }\n    ],\n    temperature=0,\n)\nprint(\"Both QA:\\n\" + qa_both_response.choices[0].message.content)\n```\n```\nBoth QA:\nSam Altman used the example of raising windows and turning the radio on to illustrate the new function calling feature in GPT-4 Turbo. This example demonstrates how the model can interpret natural language commands and translate them into specific function calls, making it easier for users to interact with the model in a more intuitive way. It highlights the model's ability to understand context and perform multiple actions based on user instructions.\n\n```\n\nComparing the three answers, the most accurate answer is generated by using both the audio and visual from the video. Sam Altman did not discuss the raising windows or radio on during the Keynote, but referenced an improved capability for the model to execute multiple functions in a single request while the examples were shown behind him.\n\n## [Conclusion](#conclusion)\n\nIntegrating many input modalities such as audio, visual, and textual, significantly enhances the performance of the model on a diverse range of tasks. This multimodal approach allows for more comprehensive understanding and interaction, mirroring more closely how humans perceive and process information.\n\nCurrently, GPT-4o and GPT-4o mini in the API support text and image inputs, with audio capabilities coming soon. For the time being, use the `gpt-4o-audio-preview` for audio inputs."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build a Weather Tool"
      ],
      "metadata": {
        "id": "3km3-7WcnYk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "@tool\n",
        "def get_weather(query: str) -> list:\n",
        "    \"\"\"Search weatherapi to get the current weather.\"\"\"\n",
        "    base_url = \"http://api.weatherapi.com/v1/current.json\"\n",
        "    complete_url = f\"{base_url}?key={WEATHER_API_KEY}&q={query}\"\n",
        "\n",
        "    response = requests.get(complete_url)\n",
        "    data = response.json()\n",
        "    if data.get(\"location\"):\n",
        "        return data\n",
        "    else:\n",
        "        return \"Weather Data Not Found\""
      ],
      "metadata": {
        "id": "ZM8R-JgOnXdN"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_weather.invoke(\"Bangalore\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12xdLSJ3ZUiv",
        "outputId": "cbf2b782-48b1-4417-960d-b599278bb199"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'location': {'name': 'Bangalore',\n",
              "  'region': 'Karnataka',\n",
              "  'country': 'India',\n",
              "  'lat': 12.9833,\n",
              "  'lon': 77.5833,\n",
              "  'tz_id': 'Asia/Kolkata',\n",
              "  'localtime_epoch': 1737589266,\n",
              "  'localtime': '2025-01-23 05:11'},\n",
              " 'current': {'last_updated_epoch': 1737588600,\n",
              "  'last_updated': '2025-01-23 05:00',\n",
              "  'temp_c': 18.2,\n",
              "  'temp_f': 64.8,\n",
              "  'is_day': 0,\n",
              "  'condition': {'text': 'Mist',\n",
              "   'icon': '//cdn.weatherapi.com/weather/64x64/night/143.png',\n",
              "   'code': 1030},\n",
              "  'wind_mph': 7.4,\n",
              "  'wind_kph': 11.9,\n",
              "  'wind_degree': 99,\n",
              "  'wind_dir': 'E',\n",
              "  'pressure_mb': 1015.0,\n",
              "  'pressure_in': 29.97,\n",
              "  'precip_mm': 0.0,\n",
              "  'precip_in': 0.0,\n",
              "  'humidity': 94,\n",
              "  'cloud': 50,\n",
              "  'feelslike_c': 18.2,\n",
              "  'feelslike_f': 64.8,\n",
              "  'windchill_c': 17.0,\n",
              "  'windchill_f': 62.6,\n",
              "  'heatindex_c': 17.0,\n",
              "  'heatindex_f': 62.6,\n",
              "  'dewpoint_c': 15.6,\n",
              "  'dewpoint_f': 60.0,\n",
              "  'vis_km': 2.0,\n",
              "  'vis_miles': 1.0,\n",
              "  'uv': 0.0,\n",
              "  'gust_mph': 12.4,\n",
              "  'gust_kph': 19.9}}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import rich\n",
        "\n",
        "result = get_weather.invoke(\"Zurich\")\n",
        "rich.print_json(data=result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 914
        },
        "id": "dyL1L1ifn0mj",
        "outputId": "9fee08e8-31de-4671-c2d4-d43b1f6ef554"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<frozen importlib._bootstrap>:1047: ImportWarning: _PyDrive2ImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _GenerativeAIImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _OpenCVImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: APICoreClientInfoImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _BokehImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _AltairImportHook.find_spec() not found; falling back to find_module()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m{\u001b[0m\n",
              "  \u001b[1;34m\"location\"\u001b[0m: \u001b[1m{\u001b[0m\n",
              "    \u001b[1;34m\"name\"\u001b[0m: \u001b[32m\"Zurich\"\u001b[0m,\n",
              "    \u001b[1;34m\"region\"\u001b[0m: \u001b[32m\"\"\u001b[0m,\n",
              "    \u001b[1;34m\"country\"\u001b[0m: \u001b[32m\"Switzerland\"\u001b[0m,\n",
              "    \u001b[1;34m\"lat\"\u001b[0m: \u001b[1;36m47.3667\u001b[0m,\n",
              "    \u001b[1;34m\"lon\"\u001b[0m: \u001b[1;36m8.55\u001b[0m,\n",
              "    \u001b[1;34m\"tz_id\"\u001b[0m: \u001b[32m\"Europe/Zurich\"\u001b[0m,\n",
              "    \u001b[1;34m\"localtime_epoch\"\u001b[0m: \u001b[1;36m1737589281\u001b[0m,\n",
              "    \u001b[1;34m\"localtime\"\u001b[0m: \u001b[32m\"2025-01-23 00:41\"\u001b[0m\n",
              "  \u001b[1m}\u001b[0m,\n",
              "  \u001b[1;34m\"current\"\u001b[0m: \u001b[1m{\u001b[0m\n",
              "    \u001b[1;34m\"last_updated_epoch\"\u001b[0m: \u001b[1;36m1737588600\u001b[0m,\n",
              "    \u001b[1;34m\"last_updated\"\u001b[0m: \u001b[32m\"2025-01-23 00:30\"\u001b[0m,\n",
              "    \u001b[1;34m\"temp_c\"\u001b[0m: \u001b[1;36m0.1\u001b[0m,\n",
              "    \u001b[1;34m\"temp_f\"\u001b[0m: \u001b[1;36m32.2\u001b[0m,\n",
              "    \u001b[1;34m\"is_day\"\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
              "    \u001b[1;34m\"condition\"\u001b[0m: \u001b[1m{\u001b[0m\n",
              "      \u001b[1;34m\"text\"\u001b[0m: \u001b[32m\"Fog\"\u001b[0m,\n",
              "      \u001b[1;34m\"icon\"\u001b[0m: \u001b[32m\"//cdn.weatherapi.com/weather/64x64/night/248.png\"\u001b[0m,\n",
              "      \u001b[1;34m\"code\"\u001b[0m: \u001b[1;36m1135\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[1;34m\"wind_mph\"\u001b[0m: \u001b[1;36m2.2\u001b[0m,\n",
              "    \u001b[1;34m\"wind_kph\"\u001b[0m: \u001b[1;36m3.6\u001b[0m,\n",
              "    \u001b[1;34m\"wind_degree\"\u001b[0m: \u001b[1;36m169\u001b[0m,\n",
              "    \u001b[1;34m\"wind_dir\"\u001b[0m: \u001b[32m\"S\"\u001b[0m,\n",
              "    \u001b[1;34m\"pressure_mb\"\u001b[0m: \u001b[1;36m1006.0\u001b[0m,\n",
              "    \u001b[1;34m\"pressure_in\"\u001b[0m: \u001b[1;36m29.71\u001b[0m,\n",
              "    \u001b[1;34m\"precip_mm\"\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
              "    \u001b[1;34m\"precip_in\"\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
              "    \u001b[1;34m\"humidity\"\u001b[0m: \u001b[1;36m100\u001b[0m,\n",
              "    \u001b[1;34m\"cloud\"\u001b[0m: \u001b[1;36m75\u001b[0m,\n",
              "    \u001b[1;34m\"feelslike_c\"\u001b[0m: \u001b[1;36m-0.7\u001b[0m,\n",
              "    \u001b[1;34m\"feelslike_f\"\u001b[0m: \u001b[1;36m30.7\u001b[0m,\n",
              "    \u001b[1;34m\"windchill_c\"\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
              "    \u001b[1;34m\"windchill_f\"\u001b[0m: \u001b[1;36m32.0\u001b[0m,\n",
              "    \u001b[1;34m\"heatindex_c\"\u001b[0m: \u001b[1;36m0.6\u001b[0m,\n",
              "    \u001b[1;34m\"heatindex_f\"\u001b[0m: \u001b[1;36m33.0\u001b[0m,\n",
              "    \u001b[1;34m\"dewpoint_c\"\u001b[0m: \u001b[1;36m-2.3\u001b[0m,\n",
              "    \u001b[1;34m\"dewpoint_f\"\u001b[0m: \u001b[1;36m27.9\u001b[0m,\n",
              "    \u001b[1;34m\"vis_km\"\u001b[0m: \u001b[1;36m10.0\u001b[0m,\n",
              "    \u001b[1;34m\"vis_miles\"\u001b[0m: \u001b[1;36m6.0\u001b[0m,\n",
              "    \u001b[1;34m\"uv\"\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
              "    \u001b[1;34m\"gust_mph\"\u001b[0m: \u001b[1;36m4.2\u001b[0m,\n",
              "    \u001b[1;34m\"gust_kph\"\u001b[0m: \u001b[1;36m6.8\u001b[0m\n",
              "  \u001b[1m}\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
              "  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"location\"</span>: <span style=\"font-weight: bold\">{</span>\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"name\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Zurich\"</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"region\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"\"</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"country\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Switzerland\"</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"lat\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">47.3667</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"lon\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.55</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"tz_id\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Europe/Zurich\"</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"localtime_epoch\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1737589281</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"localtime\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"2025-01-23 00:41\"</span>\n",
              "  <span style=\"font-weight: bold\">}</span>,\n",
              "  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"current\"</span>: <span style=\"font-weight: bold\">{</span>\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"last_updated_epoch\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1737588600</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"last_updated\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"2025-01-23 00:30\"</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"temp_c\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"temp_f\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32.2</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"is_day\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"condition\"</span>: <span style=\"font-weight: bold\">{</span>\n",
              "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"text\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Fog\"</span>,\n",
              "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"icon\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"//cdn.weatherapi.com/weather/64x64/night/248.png\"</span>,\n",
              "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"code\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1135</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"wind_mph\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.2</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"wind_kph\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.6</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"wind_degree\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">169</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"wind_dir\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"S\"</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"pressure_mb\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1006.0</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"pressure_in\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29.71</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"precip_mm\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"precip_in\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"humidity\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"cloud\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">75</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"feelslike_c\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.7</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"feelslike_f\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30.7</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"windchill_c\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"windchill_f\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32.0</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"heatindex_c\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"heatindex_f\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">33.0</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"dewpoint_c\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.3</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"dewpoint_f\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27.9</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"vis_km\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10.0</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"vis_miles\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.0</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"uv\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"gust_mph\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.2</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"gust_kph\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.8</span>\n",
              "  <span style=\"font-weight: bold\">}</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explore LLM tool calling with custom tools\n",
        "\n",
        "An agent is basically an LLM which has the capability to automatically call relevant functions to perform complex or tool-based tasks based on input human prompts.\n",
        "\n",
        "Tool calling also popularly known as function calling is the ability to reliably enable such LLMs to call external tools and APIs.\n",
        "\n",
        "We will leverate the custom tools we created earlier in the previous section and try to see if the LLM can automatically call the right tools based on input prompts"
      ],
      "metadata": {
        "id": "bIOhB430gpW9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tool calling for LLMs with native support for tool or function calling"
      ],
      "metadata": {
        "id": "4Y26Ohn3P54j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tool calling allows a model to respond to a given prompt by generating output that matches a user-defined schema. While the name implies that the model is performing some action, this is actually not the case! The model is coming up with the arguments to a tool, and actually running the tool (or not) is up to the user or agent defined by the user.\n",
        "\n",
        "Many LLM providers, including Anthropic, Cohere, Google, Mistral, OpenAI, and others, support variants of a tool calling feature. These features typically allow requests to the LLM to include available tools and their schemas, and for responses to include calls to these tools.\n",
        "\n"
      ],
      "metadata": {
        "id": "1nACq0NgL5yM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chatgpt = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
      ],
      "metadata": {
        "id": "s0wCNpzCBvtS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cf3776a-5fe1-4bbb-fcde-70ff7bca9b1f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<frozen importlib._bootstrap>:1047: ImportWarning: _PyDrive2ImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _GenerativeAIImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _OpenCVImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: APICoreClientInfoImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _BokehImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _AltairImportHook.find_spec() not found; falling back to find_module()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [multiply, search_web_extract_info, get_weather]\n",
        "chatgpt_with_tools = chatgpt.bind_tools(tools)"
      ],
      "metadata": {
        "id": "rjKWxFNgB2t_"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLMs are still not perfect in tool calling so you might need to play around with the following prompt\n",
        "prompt = \"\"\"\n",
        "            Given only the tools at your disposal, mention tool calls for the following tasks:\n",
        "            Do not change the query given for any search tasks\n",
        "            1. What is 2.1 times 3.5\n",
        "            2. What is the current weather in Greenland today\n",
        "            3. What are the 4 major Agentic AI Design Patterns\n",
        "         \"\"\"\n",
        "\n",
        "results = chatgpt_with_tools.invoke(prompt)"
      ],
      "metadata": {
        "id": "hJ271K_tB9K7"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "kLoZWknjCNlC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba479c17-587e-419a-ce56-0c3a16b09dbf"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_ZnAT5ZuL6BpedH7FEvzSiydD', 'function': {'arguments': '{\"a\": 2.1, \"b\": 3.5}', 'name': 'multiply'}, 'type': 'function'}, {'id': 'call_nR7pKlXmDvBAJDz8Y9pNHjaW', 'function': {'arguments': '{\"query\": \"Greenland\"}', 'name': 'get_weather'}, 'type': 'function'}, {'id': 'call_ekGIBMjz1Hk3O2cYGRt9fAEb', 'function': {'arguments': '{\"query\": \"4 major Agentic AI Design Patterns\"}', 'name': 'search_web_extract_info'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 75, 'prompt_tokens': 182, 'total_tokens': 257, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_4691090a87', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-7dc3d55f-6fcf-42b6-a557-85deea052a72-0', tool_calls=[{'name': 'multiply', 'args': {'a': 2.1, 'b': 3.5}, 'id': 'call_ZnAT5ZuL6BpedH7FEvzSiydD', 'type': 'tool_call'}, {'name': 'get_weather', 'args': {'query': 'Greenland'}, 'id': 'call_nR7pKlXmDvBAJDz8Y9pNHjaW', 'type': 'tool_call'}, {'name': 'search_web_extract_info', 'args': {'query': '4 major Agentic AI Design Patterns'}, 'id': 'call_ekGIBMjz1Hk3O2cYGRt9fAEb', 'type': 'tool_call'}], usage_metadata={'input_tokens': 182, 'output_tokens': 75, 'total_tokens': 257, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results.tool_calls"
      ],
      "metadata": {
        "id": "ckZz5pcpCQau",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e96c257-c347-45cd-f5ec-6e1c071d9f86"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'multiply',\n",
              "  'args': {'a': 2.1, 'b': 3.5},\n",
              "  'id': 'call_ZnAT5ZuL6BpedH7FEvzSiydD',\n",
              "  'type': 'tool_call'},\n",
              " {'name': 'get_weather',\n",
              "  'args': {'query': 'Greenland'},\n",
              "  'id': 'call_nR7pKlXmDvBAJDz8Y9pNHjaW',\n",
              "  'type': 'tool_call'},\n",
              " {'name': 'search_web_extract_info',\n",
              "  'args': {'query': '4 major Agentic AI Design Patterns'},\n",
              "  'id': 'call_ekGIBMjz1Hk3O2cYGRt9fAEb',\n",
              "  'type': 'tool_call'}]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multiply"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLxNMPTegDed",
        "outputId": "780eca46-d94f-45c7-9311-d9bd0388b0f7"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructuredTool(name='multiply', description='use to multiply numbers', args_schema=<class '__main__.CalculatorInput'>, return_direct=True, func=<function multiply at 0x7b9c20f2efc0>)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "toolkit = {\n",
        "    \"multiply\": multiply,\n",
        "    \"search_web_extract_info\": search_web_extract_info,\n",
        "    \"get_weather\": get_weather\n",
        "}\n",
        "\n",
        "for tool_call in results.tool_calls:\n",
        "    selected_tool = toolkit[tool_call[\"name\"].lower()]\n",
        "    print(f\"Calling tool: {tool_call['name']}\")\n",
        "    tool_output = selected_tool.invoke(tool_call[\"args\"])\n",
        "    print(tool_output)\n",
        "    print()"
      ],
      "metadata": {
        "id": "POvGp_xZCpSg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56aef3e3-78d7-416e-e0a9-553ae3f7e9c8"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling tool: multiply\n",
            "7.3500000000000005\n",
            "\n",
            "Calling tool: get_weather\n",
            "{'location': {'name': 'Nuuk', 'region': 'Vestgronland', 'country': 'Greenland', 'lat': 64.183, 'lon': -51.75, 'tz_id': 'America/Nuuk', 'localtime_epoch': 1737591145, 'localtime': '2025-01-22 22:12'}, 'current': {'last_updated_epoch': 1737590400, 'last_updated': '2025-01-22 22:00', 'temp_c': -6.7, 'temp_f': 19.9, 'is_day': 0, 'condition': {'text': 'Light snow', 'icon': '//cdn.weatherapi.com/weather/64x64/night/326.png', 'code': 1213}, 'wind_mph': 4.0, 'wind_kph': 6.5, 'wind_degree': 177, 'wind_dir': 'S', 'pressure_mb': 983.0, 'pressure_in': 29.03, 'precip_mm': 0.13, 'precip_in': 0.01, 'humidity': 79, 'cloud': 100, 'feelslike_c': -10.0, 'feelslike_f': 14.1, 'windchill_c': -11.5, 'windchill_f': 11.2, 'heatindex_c': -8.1, 'heatindex_f': 17.5, 'dewpoint_c': -9.0, 'dewpoint_f': 15.9, 'vis_km': 10.0, 'vis_miles': 6.0, 'uv': 0.0, 'gust_mph': 5.3, 'gust_kph': 8.5}}\n",
            "\n",
            "Calling tool: search_web_extract_info\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 2/5 [00:00<00:00,  4.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction blocked for url:  https://www.otechtalks.tv/introduction-to-4-agentic-ai-design-patterns/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:06<00:00,  1.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Introduction to 4 Agentic AI Design Patterns\\n[![](https://substackcdn.com/image/fetch/w_96,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad51036a-09b8-40d6-8ee9-cc1c865c9be9_500x500.png)](/)\\n# [AI Tech Circle](/)\\n\\nSubscribeSign in\\n#### Share this post\\n\\n[![](https://substackcdn.com/image/fetch/w_520,h_272,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff36e923f-1489-4c3b-82b4-1c5f5b7aecf3_1916x1038.png)![AI Tech Circle](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad51036a-09b8-40d6-8ee9-cc1c865c9be9_500x500.png)AI Tech CircleIntroduction to 4 Agentic AI Design Patterns](https://substack.com/home/post/p-155165327?utm_campaign=post&utm_medium=web)Copy linkFacebookEmailNotesMore\\n# Introduction to 4 Agentic AI Design Patterns\\n\\n[![](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b7437ae-1100-4983-aa9a-20fc7bb6bb66_144x144.png)](https://substack.com/%40aitechcircle)[AI Tech Circle](https://substack.com/%40aitechcircle)Jan 19, 2025\\n#### Share this post\\n\\n[![](https://substackcdn.com/image/fetch/w_520,h_272,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff36e923f-1489-4c3b-82b4-1c5f5b7aecf3_1916x1038.png)![AI Tech Circle](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad51036a-09b8-40d6-8ee9-cc1c865c9be9_500x500.png)AI Tech CircleIntroduction to 4 Agentic AI Design Patterns](https://substack.com/home/post/p-155165327?utm_campaign=post&utm_medium=web)Copy linkFacebookEmailNotesMoreShare\\n\\nWelcome to your weekly AI Newsletter from [AITechCircle](https://aitechcircle.com/)!\\n\\nI\\'m Building, Implementing AI solutions, and sharing everything I learn along the way...\\n\\nCheck out the updates from this week! Please take a moment to share them with a friend or colleague who might benefit from these valuable insights!\\n\\n### **Today at a Glance:**\\n\\n* AI Agentic Design Patterns\\n* Generative AI Use cases\\n* AI Weekly news and updates covering newly released LLMs\\n* Courses and events to attend\\n\\n### **Agentic AI Design Patterns**\\n\\nLast week, we covered how AI is evolving, and it is in the [Agentic AI phase in 2025](https://aitechcircle.kit.com/posts/from-perception-ai-to-generative-to-agentic-to-physical-ai)\\u200b\\n\\nContinuously to this topic, I got a question during this week\\'s session with a few tech engineers about whether there are common patterns for Agentic AI.\\n\\nBefore you go into details of the patterns, let\\'s have a look at the following:\\n\\n### **What is Agentic AI**\\n\\nAgentic AI systems feature intelligent, independent agents collaborating to tackle complex problems. These agents surpass traditional AI, like just prompts or pre-defined tasks, by integrating advanced capabilities like “chaining,” enabling them to manage intricate, sequential tasks. They can perceive environments, make decisions, and learn from outcomes to achieve goals.\\n\\nThey have a spectrum of agent-like qualities and cover various systems and methodologies. Large language models (LLMs) are a key part of Agentic AI.\\n\\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff36e923f-1489-4c3b-82b4-1c5f5b7aecf3_1916x1038.png)](https://substackcdn.com/image/fetch/f_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%3A//substack-post-media.s3.amazonaws.com/public/images/f36e923f-1489-4c3b-82b4-1c5f5b7aecf3_1916x1038.png)\\n\\nI worked on exploring these patterns during this week, and here are some common **Agentic AI** design patterns.\\n\\nThey describe how autonomous agents perceive, reason, communicate, and act in a multi-agent or distributed environment.\\n\\nEach pattern has distinct benefits and trade-offs;\\n\\nYou can combine them to build more complex agentic systems.\\n\\nEach pattern offers a way to structure decision-making, communication, and execution in complex scenarios.\\n\\n**Design Patterns for Agentic AI:**\\n\\n**1 - Reflection:**\\n\\nThe Reflection design pattern in Agentic AI involves the system\\'s ability to analyze its own performance and decision-making processes. This self-awareness allows the agent to adjust its behaviour based on past actions and outcomes, enhancing its effectiveness over time.\\n\\n**Use Cases:** This pattern is particularly useful in dynamic environments where conditions change rapidly, such as in automated trading systems where an agent must evaluate its trading strategies and adapt to new market conditions without human intervention.\\n\\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9454656c-3315-4d79-87ca-e01930c53284_506x338.png)](https://substackcdn.com/image/fetch/f_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%3A//substack-post-media.s3.amazonaws.com/public/images/9454656c-3315-4d79-87ca-e01930c53284_506x338.png)\\n\\n**2 - Planning:**\\n\\nPlanning is another design pattern in which the AI agent can foresee potential future states and devise a series of actions to achieve its goals. This involves complex problem-solving and decision-making processes based on predicted outcomes.\\n\\n**Use Cases:** Planning is essential in logistics and supply chain management, where AI agents need to optimize routes and schedules for delivery vehicles based on traffic, weather conditions, and customer delivery windows.\\n\\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9cfc692f-fcc7-4eef-9cae-20d061654c7c_576x314.png)](https://substackcdn.com/image/fetch/f_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%3A//substack-post-media.s3.amazonaws.com/public/images/9cfc692f-fcc7-4eef-9cae-20d061654c7c_576x314.png)\\n\\n\\u200b\\n\\n**3. Tool Use:**\\n\\nThe tool-use design pattern enables AI agents to identify, select, and use tools or resources within their environment to accomplish specific tasks. This extends the agent’s capabilities beyond built-in functions to leveraging external tools or integrating with other systems.\\n\\n**Use Cases:** In manufacturing, AI agents equipped with the Tool Use pattern can autonomously operate machinery, adjust parameters for different production runs, or switch between tools to efficiently handle varying materials and assembly processes.\\n\\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f97dde3-dc0d-46aa-b0c0-6eee31f8f00d_731x315.png)](https://substackcdn.com/image/fetch/f_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%3A//substack-post-media.s3.amazonaws.com/public/images/3f97dde3-dc0d-46aa-b0c0-6eee31f8f00d_731x315.png)\\n\\n**4. Multi-Agent:**\\n\\nThe multi-agent design pattern involves multiple AI agents working collaboratively to solve problems or complete tasks that are too complex for a single agent. This pattern focuses on coordination and cooperation among agents to optimize overall system performance.\\n\\n**Use Cases:** Multi-agent systems are highly effective in smart city applications, such as coordinating traffic lights and public transportation schedules to optimize traffic flow and reduce congestion during peak times.\\n\\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04d27bf5-da31-4edc-b612-5f28bc522a31_802x314.png)](https://substackcdn.com/image/fetch/f_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%3A//substack-post-media.s3.amazonaws.com/public/images/04d27bf5-da31-4edc-b612-5f28bc522a31_802x314.png)\\n\\nGenerative artificial intelligence facilitates the development and implementation of agents. These agents can leverage distinguished reasoning and language processing capabilities to take a proactive, autonomous role in pursuing business process goals\\n\\n### **Decision Tree**\\n\\nThe paper \"*[Agent Design Pattern Catalogue: A Collection of Architectural Patterns for Foundation Model-based Agent](https://arxiv.org/abs/2405.10467)s\"* covers **18 patterns.** The best part is that you can decide which model you want to use below the decision tree to achieve a specific business task.\\u200b\\n\\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ba9b28d-a9c4-4620-891e-c148e2e2c054_1223x473.png)](https://substackcdn.com/image/fetch/f_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%3A//substack-post-media.s3.amazonaws.com/public/images/4ba9b28d-a9c4-4620-891e-c148e2e2c054_1223x473.png)\\n\\n*Source: <https://arxiv.org/abs/2405.10467>\\u200b*\\n\\n### **Call for Action**\\n\\nFor organizations looking to implement Agentic AI, understanding these design patterns can support the development of robust AI systems that can operate autonomously and adapt to new challenges.\\n\\nEach pattern provides a framework for designing AI agents to perform designated tasks, improve their IT ecosystem, and interact intelligently with it.\\n\\nBy leveraging these patterns, businesses can enhance efficiency, reduce operational costs, and improve service delivery across various domains.\\n\\n---\\n\\n## Weekly News & Updates...\\n\\nLast week\\'s AI breakthroughs marked another leap forward in the tech revolution.\\n\\n1. Mistral has announced Codestral 25.01, a new SOTA coding model. It is Lightweight, fast, proficient in over 80 programming languages, and Optimized for low-latency, high-frequency use cases. [Link](https://mistral.ai/news/codestral-2501/)\\u200b\\n2. Snowflake-Llama models with SwiftKV optimizations developed and integrated into vLLM improve LLM inference throughput to lower costs. Snowflake-derived models, based on Meta’s Llama 3.3 70B and Llama 3.1 405B base models, [link](https://www.snowflake.com/en/blog/up-to-75-lower-inference-cost-llama-meta-llm/)\\u200b\\n3. Kokoro Voice Mixer Studio from Gradio is released, [link](https://huggingface.co/spaces/ysharma/Make_Custom_Voices_With_KokoroTTS)\\u200b\\n\\n### The Cloud: *the backbone* of the *AI revolution*\\n\\n* NVIDIA Releases NIM Microservices to Safeguard Applications for Agentic AI [link](https://blogs.nvidia.com/blog/nemo-guardrails-nim-microservices/)\\u200b\\n* Worker safety and PPE compliance using AI [link](https://blogs.oracle.com/ai-and-datascience/post/worker-safety-and-ppe-compliance-using-ai)\\u200b\\n\\n### Generative AI **Use Case of the Week:**\\n\\nUrban planning is evolving with the integration of Generative AI, offering innovative solutions to design challenges. Public Works Departments can now capitalize on the power of large language models (LLMs) to generate tailored urban design proposals for parks, housing layouts, and public spaces. These AI-driven suggestions streamline planning processes while aligning with community needs and sustainability goals.\\n\\nTo access the library of Gen AI Use cases, [link here](https://github.com/kashifmannzoor/Generative-AI-use-cases):\\n\\n### Chief **AI Officer (CAIO) Corner:**\\n\\nThree tips for CAIOs\\n\\n1. **Invest in Scalable Data Infrastructure:** Prioritize a flexible, secure infra for AI. This enables faster AI experimentation and deployment across diverse use cases.\\n2. **Focus on Responsible AI:** Adopt clear guidelines for data privacy, bias mitigation, and model explainability. This will build trust with stakeholders and reduce regulatory risks.\\n3. **AI Literacy in Leadership:** Promote education and training so executives can understand AI\\'s potential, limitations, and ethical implications. This drives strategic alignment and informed decision-making.\\n\\n### Favorite **Tip Of The Week:**\\n\\nHere\\'s my favorite resource of the week.\\n\\nUsing the DINOv2 open source model from Meta FAIR, EndoDINO, a foundation model that delivers SOTA performance across a range of GI endoscopy tasks, details to read over: How Virgo is using DINOv2 to analyze endoscopy videos for precision medicine. [link](https://ai.meta.com/blog/virgo-dino-endoscopy-video/)\\u200b\\n\\n## Potential of AI\\n\\nOpenAI has published an Economic Blueprint outlining policy proposals to maximize AI’s benefits, bolster national security, and drive economic growth in the United States. It discusses the importance of building a democratic AI ecosystem that supports entrepreneurship and personal freedom, ensuring AI development aligns with American values and safeguards. The full details of the proposals can be found on OpenAI’s [website](https://openai.com/global-affairs/openais-economic-blueprint/). Below are the few examples presented in the report where it is being used.\\n\\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26c87f7b-d845-457c-9fdb-c2b021e1d27e_1272x668.png)](https://substackcdn.com/image/fetch/f_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%3A//substack-post-media.s3.amazonaws.com/public/images/26c87f7b-d845-457c-9fdb-c2b021e1d27e_1272x668.png)\\n\\n*Source: OpenAI’s Economic Blueprint*\\n\\n## Things to Know...\\n\\nThe UK’s Prime Minister has recently outlined an ambitious [blueprint](https://www.gov.uk/government/speeches/pm-speech-on-ai-opportunities-action-plan-13-january-2025) aimed at harnessing the power of AI for economic and social advancement. The plan emphasizes substantial investment in AI technology and infrastructure to position the UK as a leading AI innovator on the global stage. This includes enhancing public services through AI\\n\\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdbac856b-b896-406a-99f9-0d1653bb8e71_1000x524.png)](https://substackcdn.com/image/fetch/f_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%3A//substack-post-media.s3.amazonaws.com/public/images/dbac856b-b896-406a-99f9-0d1653bb8e71_1000x524.png)\\n\\n*UK’s Prime Minister has recently outlined an ambitious [blueprint](https://www.gov.uk/government/speeches/pm-speech-on-ai-opportunities-action-plan-13-january-2025)*\\n\\nApplications, addressing safety risks associated with AI technologies, and establishing the UK as a prime destination for AI firms and talent. This strategic move not only aims to boost the national economy but also to ensure that AI advancements are safe and beneficial across various sectors\\n\\n---\\n\\n## The Opportunity...\\n\\n**Podcast:**\\n\\n* This week\\'s Open Tech Talks episode 153 is **\"AI and Software Development: What Engineers Need to Know with Mayank Jindal,\"** He is a Software Development Engineer at Amazon.\\n\\n[Apple](https://podcasts.apple.com/sg/podcast/ai-and-software-development-what-engineers-need-to/id980891268?i=1000682217324) | [Spotify](https://open.spotify.com/episode/0212eCY4ETEgO5xU8sq8Fx) | [Amazon Music](https://music.amazon.com.mx/podcasts/5ebfc419-67fc-4d25-9373-c2e391719bf2/episodes/1065f49a-482a-4040-9afa-896fce0ca69a/open-tech-talks-technology-worth-talking-artificial-intelligence-tools-tips-ai-and-software-development-what-engineers-need-to-know-with-mayank-jindal) \\u200b\\n\\n**Courses to attend:**\\n\\n* \\u200b[One Million Prompters](https://dub.ai/en/omp/): This course consists of four modules\\n\\n**Events:**\\n\\n* \\u200b[Oracle CloudWorld](https://www.oracle.com/ae/cloudworld-tour/), Dubai, Jan 22, 2025\\n* \\u200b[AI in Dubai Conference](https://aidubaiconference.com/), Jan 20 - 24, 2025, Dubai, UAE\\n* \\u200b[AI Everything GLOBAL](https://aieverythingglobal.com/home), Feb 4-6, 2025, Dubai, UAE\\n* \\u200b[Web Summit](https://qatar.websummit.com/), Feb 23-26, Qatar\\n\\n## Tech and Tools...\\n\\n* \\u200b[MiniCPM-o](https://github.com/OpenBMB/MiniCPM-o) is the latest series of end-side multimodal LLMs (MLLMs) ungraded from MiniCPM-V. The models can now take images, video, text, and audio as inputs and provide high-quality text and speech outputs\\n* \\u200b[Parlant](https://github.com/emcie-co/parlant): The Behavior Guidance Framework for Customer-Facing Agents\\n\\n### And that’s a wrap!\\n\\nThank you, as always, for taking the time to read.\\n\\n**I’d love to hear your thoughts. Please reply and let me know what you find most valuable this week. Your feedback means a lot.**\\n\\nUntil next week,\\n\\nKashif Manzoor\\n\\n---\\n\\n*The opinions expressed here are solely my conjecture based on experience, practice, and observation. They do not represent the thoughts, intentions, plans, or strategies of my current or previous employers or their clients/customers. The objective of this newsletter is to share and learn with the community.*\\n\\n#### Share this post\\n\\n[![](https://substackcdn.com/image/fetch/w_520,h_272,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff36e923f-1489-4c3b-82b4-1c5f5b7aecf3_1916x1038.png)![AI Tech Circle](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad51036a-09b8-40d6-8ee9-cc1c865c9be9_500x500.png)AI Tech CircleIntroduction to 4 Agentic AI Design Patterns](https://substack.com/home/post/p-155165327?utm_campaign=post&utm_medium=web)Copy linkFacebookEmailNotesMoreShare\\n#### Discussion about this post\\n\\nCommentsRestacks![](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Fdefault-light.png)TopLatest\\n\\nNo posts\\n\\nReady for more?\\n\\nSubscribe© 2025 AI Tech Circle[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected) [Start Writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)[Substack](https://substack.com) is the home for great culture\\n#### Share\\n\\nCopy linkFacebookEmailNotesMore\\n\\nThis site requires JavaScript to run correctly. Please [turn on JavaScript](https://enable-javascript.com/) or unblock scripts', \"Top 4 Agentic AI Design Patterns\\n[![Analytics Vidhya](https://av-public-assets.s3.ap-south-1.amazonaws.com/logos/av-logo-svg.svg)](https://www.analyticsvidhya.com/blog/)\\n\\n* [Free Courses](https://www.analyticsvidhya.com/all-free-courses?ref=Navbar)\\n* [Learning Paths](https://www.analyticsvidhya.com/blog/category/learning-path/?ref=navbar)\\n* [GenAI Pinnacle Program](https://www.analyticsvidhya.com/genaipinnacle?ref=navbar)\\n* [Agentic AI Pioneer Program](https://www.analyticsvidhya.com/agenticaipioneer/?ref=blognavbar)\\n  New\\n\\n* Login\\n\\n  + Switch Mode\\n  + Logout\\n\\n[Interview Prep](https://www.analyticsvidhya.com/blog/category/interview-questions/?ref=category)[Career](https://www.analyticsvidhya.com/blog/category/career/?ref=category)[GenAI](https://www.analyticsvidhya.com/blog/category/generative-ai/?ref=category)[Prompt Engg](https://www.analyticsvidhya.com/blog/category/prompt-engineering/?ref=category)[ChatGPT](https://www.analyticsvidhya.com/blog/category/chatgpt/?ref=category)[LLM](https://www.analyticsvidhya.com/blog/category/llms/?ref=category)[Langchain](https://www.analyticsvidhya.com/blog/category/langchain/?ref=category)[RAG](https://www.analyticsvidhya.com/blog/category/rag/?ref=category)[AI Agents](https://www.analyticsvidhya.com/blog/category/ai-agent/?ref=category)[Machine Learning](https://www.analyticsvidhya.com/blog/category/machine-learning/?ref=category)[Deep Learning](https://www.analyticsvidhya.com/blog/category/deep-learning/?ref=category)[GenAI Tools](https://www.analyticsvidhya.com/blog/category/ai-tools/?ref=category)[LLMOps](https://www.analyticsvidhya.com/blog/category/llmops/?ref=category)[Python](https://www.analyticsvidhya.com/blog/category/python/?ref=category)[NLP](https://www.analyticsvidhya.com/blog/category/nlp/?ref=category)[SQL](https://www.analyticsvidhya.com/blog/category/sql/?ref=category)[AIML Projects](https://www.analyticsvidhya.com/blog/category/project/?ref=category)\\n\\n#### Reading list\\n\\n##### Data analyst Learning Path\\n\\n[How to Become a Data Analyst in 2025: A Complete RoadMap](https://www.analyticsvidhya.com/blog/2023/12/step-by-step-guide-to-becoming-a-data-analyst/)\\n\\n##### Tableau Learning Path\\n\\n[A Comprehensive Learning Path to Tableau in 2025](https://www.analyticsvidhya.com/blog/2015/07/learning-path-tableau-worlds-fastest-growing-visualization-tool/)\\n\\n##### NLP Learning Path\\n\\n[A Comprehensive NLP Learning Path 2025](https://www.analyticsvidhya.com/blog/2023/12/nlp-learning-path/)\\n\\n##### Data Scientist Learning Path\\n\\n[Learning Path to Become a Data Scientist in 2025](https://www.analyticsvidhya.com/blog/2020/12/a-comprehensive-learning-path-to-become-a-data-scientist/)\\n\\n##### Data Engineer Learning Path\\n\\n[Step-by-Step Roadmap to Become a Data Engineer in 2025](https://www.analyticsvidhya.com/blog/2023/01/step-by-step-roadmap-to-become-a-data-engineer/)\\n\\n##### MLOps Learning Path\\n\\n[A Comprehensive MLOps Learning Path: 2025 Edition](https://www.analyticsvidhya.com/blog/2023/12/a-comprehensive-mlops-learning-path/)\\n\\n##### AI Engineer Learning Path\\n\\n[Roadmap to Become an AI Engineer in 2025](https://www.analyticsvidhya.com/blog/2024/04/roadmap-to-become-an-ai-engineer/)\\n\\n##### Computer Vision Learning Path\\n\\n[A Comprehensive Learning Path to Master Computer Vision in 2025](https://www.analyticsvidhya.com/blog/2020/01/computer-vision-learning-path/)\\n\\n##### Generative AI Learning Path\\n\\n[Best Roadmap to Learn Generative AI in 2025](https://www.analyticsvidhya.com/blog/2023/05/from-novice-to-pro-the-epic-journey-of-mastering-generative-ai/)\\n\\n##### Generative AI Roadmap for Enterprises\\n\\n[GenAI Roadmap for Enterprises](https://www.analyticsvidhya.com/blog/2024/05/genai-roadmap-for-enterprises/)\\n\\n##### LLMs Roadmap\\n\\n[Large Language Models Demystified: A Beginner’s Roadmap](https://www.analyticsvidhya.com/blog/2023/10/large-language-models-demystified/)\\n\\n##### Prompt Engineer Leaning Path\\n\\n[Learning Path to Become a Prompt Engineering Specialist](https://www.analyticsvidhya.com/blog/2024/07/prompt-engineer/)\\n\\n1. [Home](https://www.analyticsvidhya.com/blog/)\\n2. [AI Agents](https://www.analyticsvidhya.com/blog/category/ai-agent/)\\n3. Top 4 Agentic AI Design Patterns for Architecting AI Systems\\n\\n# Top 4 Agentic AI Design Patterns for Architecting AI Systems\\n\\n[![Pankaj Singh](https://av-eks-lekhak.s3.amazonaws.com/media/lekhak-profile-images/converted_image_Lb7Lh0T.webp)](https://www.analyticsvidhya.com/blog/author/pankaj9786/)\\n\\n[Pankaj Singh](https://www.analyticsvidhya.com/blog/author/pankaj9786/)\\n Last Updated :\\n06 Jan, 2025\\n\\n  10  min read\\n\\nLearning is a continuous journey, whether you’re human or an AI model. However, one question that often comes up is, can these AI models learn themselves just like humans do? As per the recent developments – **They can**. To understand this in a better way, let’s go back to our college days when C++, Java, and Python were the primary languages we needed to master to excel in computer science. Learning these languages requires understanding syntax, semantics, practical application, and problem-solving. So, to get a strong hold on these languages, we practised continuously (or you can say get trained). Also, we learned a lot from our classmates and professors. Right? Similarly, just like humans can learn from their own thinking, expertise and other mediums, perhaps LLMs can, too.\\n\\nHowever, gaining expertise or becoming a subject matter expert is quite a rigorous journey for both humans and LLMs. We know about the human learning process and reasoning capabilities for making decisions and completing tasks, but what does LLM training look like?\\n\\nCan I say?\\n\\n1. **Firstly, pre-training of LLM:** In this step, you help the model learn patterns, such as grammar, sentence structure, and even relationships between words and concepts.\\n2. **Instruction-tuning (or Fine-Tuning):** To fine-tune the model, a curated dataset containing examples of instructions and desired responses is used.\\n3. **Reinforcement Learning with Human Feedback (RLHF):** Human evaluators rank model responses, which is used further to improve the model’s alignment with user expectations.\\n\\nThat makes sense, right? But what if we build an agentic workflow to make the model learn and give the output while doing all the checks independently? It would be like having your own assistant who can do all the work without any human intervention. Further, in this article we will talk about the 4 Agentic AI Design Patterns for Architecting AI Systems.\\n\\n* *[**What is Agentic AI Reflection Pattern?**](https://www.analyticsvidhya.com/blog/2024/10/agentic-ai-reflection-pattern/)*\\n* *[**What is Agentic AI Tool Use Pattern?**](https://www.analyticsvidhya.com/blog/2024/10/agentic-ai-tool-use-pattern/)*\\n* *[**What is Agentic AI Planning Pattern?**](https://www.analyticsvidhya.com/blog/2024/11/agentic-ai-planning-pattern/)*\\n* [**What is Agentic AI Multi-Agent Pattern?**](https://www.analyticsvidhya.com/blog/2024/11/agentic-ai-multi-agent-pattern/)\\n\\n![Agentic AI design pattern](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/10/Artboard-1-copy-4.webp)\\n\\nSource: Author\\n\\n### Overview\\n\\n* The article discusses how AI models, particularly large language models (LLMs) like [**GPT**](https://www.analyticsvidhya.com/blog/2024/05/applications-of-gpt-4o/), can learn autonomously by adopting agentic workflows, which mimic human-like iterative problem-solving.\\n* Agentic workflows enhance AI performance by refining tasks step-by-step, similar to how humans review and improve their work repeatedly for better results.\\n* Four key Agentic Design Patterns—Reflection, Tool Use, Planning, and Multi-Agent Collaboration—are introduced as strategies that make AI systems more autonomous and capable.\\n\\n## Table of contents\\n\\n* [Overview](#h-overview)\\n* [What is Agentic Design Patterns?](#h-what-is-agentic-design-patterns)\\n* [Agentic Design Patterns: Evaluations](#h-agentic-design-patterns-evaluations)\\n* [4 Types of Agentic Design Patterns that You Must Know](#h-4-types-of-agentic-design-patterns-that-you-must-know)\\n  + [Reflection Pattern](#h-1-reflection-pattern)\\n  + [Tool Use Pattern](#h-2-tool-use-pattern)\\n  + [Planning Pattern](#h-3-planning-pattern)\\n  + [Multi-Agent Pattern](#h-4-multi-agent-pattern)\\n* [Conclusion](#h-conclusion)\\n* [Frequently Asked Questions](#h-frequently-asked-questions)\\n## What is Agentic Design Patterns?\\n\\nThe agentic design pattern is introduced as a solution for making LLMs more autonomous. Instead of just giving the model one prompt and expecting a final answer (like writing an essay in one go), an agent-like approach involves prompting the LLM multiple times, step by step. Each step refines the task, with the model improving its output iteratively.\\n\\nTo understand this better, let’s look at it like this:\\n\\nWhen we prompt an LLM in zero-shot mode, it’s like asking someone to write a story in one go without revising. LLMs do well at this, but they can do even better. By using an agent-like workflow, we can prompt the LLM multiple times in steps. Each step builds on the previous one, refining the response. Think of it like asking the LLM to go over the essay multiple times, improving it with each pass.\\n\\nBy each step, I meant:\\n\\nLet’s take the example of writing a code using Agentic workflow:\\n\\n1. **Plan an outline for the code:** Break down the task into smaller modules or functions.\\n2. **Gather information and content:** Research libraries, algorithms, or existing solutions. Do web searches or check the documentation if needed.\\n3. **Write the first draft of the code:** Implement the basic functionality, focusing on structure over perfection.\\n4. **Review the code for inefficiencies or errors:** Check for unnecessary code, bugs, or logic flaws.\\n5. **Revise the code:** Refactor, optimise, or add comments for clarity.\\n\\nRinse and repeat until the code is efficient and clean.\\n\\nBy allowing the model to work through these steps independently, the agentic design pattern enhances both human-like reasoning and efficiency. This is similar to how humans break down complex tasks, gather information, make improvements, and iterate until the final result is satisfactory. Now, let us understand the Agentic design pattern in detail.\\n\\n## Agentic Design Patterns: Evaluations\\n\\n![](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/10/unnamed-68.webp)\\n\\nSource:\\xa0 Andrew Ng | [Deeplearning.ai](https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/?ref=dl-staging-website.ghost.io)\\n\\nAndrew Ng’s analysis, shared in a letter on\\xa0[**Deeplearning.ai**](https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/?ref=dl-staging-website.ghost.io), noted advancements in AI-driven code generation, particularly focusing on the performance of models like GPT-3.5 and GPT-4. The evaluation was centred on these models’ capabilities to perform on the widely recognized HumanEval coding benchmark, a common standard for assessing an algorithm’s proficiency in writing code.\\n\\nThe data presented shows the evolution in AI coding abilities using AI agents. [**GPT-3.5**](https://www.analyticsvidhya.com/blog/2023/08/generative-ais-shift-from-gpt-3-5-to-gpt-4-journey/), when tested in a zero-shot setting (i.e., without any prior examples), achieved a correctness rate of 48.1%. [**GPT-4**](https://openai.com/index/gpt-4/), also evaluated in a zero-shot manner, demonstrated a significant improvement, with a 67.0% success rate. However, what stood out in the analysis was how integrating these models into an iterative agent workflow (Agentic workflow) drastically boosted their performance. When [**GPT-3.5**](https://platform.openai.com/docs/models) was wrapped in such an agent loop, its accuracy soared to an impressive 95.1%, far surpassing its baseline and even approaching human-level coding proficiency.\\n\\nThis finding underscores the transformative potential of iterative workflows (Agentic workflow) in enhancing [**AI model**](https://www.analyticsvidhya.com/blog/2024/08/github-models/) performance, suggesting that the future of AI-assisted coding may heavily rely on these more advanced, adaptive frameworks rather than on model size or architecture improvements alone.\\n\\nBut what are Agentic design patterns that complete the delegation of autonomy to AI systems, enabling them to act more independently and effectively? These patterns structure AI agents to perform tasks, make decisions, and communicate with other systems in a more human-like and autonomous manner, ultimately creating both savvy and dependable applications.\\x7f\\n\\n## 4 Types of Agentic Design Patterns that You Must Know\\n\\nIn Agentic AI and the key design patterns, it’s essential to understand how each pattern empowers large language models (LLMs) like GPT to behave more autonomously and effectively. These design patterns push the boundaries of what AI can do by encouraging self-evaluation, tool integration, strategic thinking, and collaboration. Let’s explore four vital agentic design patterns that shape how these models operate and perform complex tasks.\\n\\nHere are the types of agentic design patterns:\\n\\n### 1. Reflection Pattern\\n\\n![Agentic Design Patterns  - Relection Pattern](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/10/Artboard-1-1.webp)\\n\\nSource: Author\\n\\nThe [**Reflection Pattern**](https://www.analyticsvidhya.com/blog/2024/10/agentic-ai-reflection-pattern/) focuses on improving AI’s ability to evaluate and refine its own outputs. Imagine an LLM reviewing its generated content or code as if it were a human reviewer, identifying errors, gaps, or areas that need improvement and then offering suggestions for how to improve.\\n\\nThis self-critique loop is not limited to a single iteration. The AI can repeat the reflection process as many times as necessary to achieve a refined, polished result. For example, if tasked with writing software, the LLM can generate an initial version, critique its own logic and structure, and revise the code. The iterative nature of reflection leads to stronger, more reliable outputs over time.\\n\\nThis pattern is particularly useful in tasks that require precision, such as content creation, problem-solving, or code generation. Employing this approach can enhance the model’s accuracy and reliability through self-guided corrections.\\n\\nOne interesting example is Self-Reflective RAG. [**SELF-RAG**](https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_self_rag/) is a framework designed to improve language models’ quality and factual accuracy by integrating retrieval and self-reflection into the text generation process. Traditional [**Retrieval-Augmented Generation (RAG)**](https://www.analyticsvidhya.com/blog/2023/09/retrieval-augmented-generation-rag-in-ai/) models enhance responses by incorporating relevant retrieved passages but often retrieve a fixed number of documents regardless of their relevance, which can introduce noise or irrelevant content. SELF-RAG addresses these limitations through an adaptive approach that retrieves information on demand and uses reflection tokens to assess the generation’s quality.\\n\\n#### How SELF-RAG Uses Reflection?\\n\\nSELF-RAG incorporates self-reflection mechanisms via “reflection tokens,” which serve to evaluate various aspects of the text generation, such as relevance, support, and overall utility. During the generation process, the model evaluates whether retrieval is necessary and assesses the quality of the generated content by critiquing itself at different stages.\\n\\nHere’s the diagram for better understanding:\\n\\n![SELF-RAG Uses Reflection](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/10/unnamed-69-1.webp)\\n\\n**Source: [SELF-RAG](https://arxiv.org/pdf/2310.11511)**\\n\\n* **Traditional RAG** retrieves a fixed number of documents first, while **Self-RAG** performs retrieval dynamically based on the content being generated.\\n* **Self-RAG** evaluates multiple generated segments, critiques their quality, and selectively combines the most accurate information.\\n* **Self-RAG’s iterative process** enables refining the generation step by step, improving the accuracy and relevance of the output.\\n\\nIn a nutshell, Self-RAG adds an extra layer of self-reflection and refinement, leading to more reliable and precise answers.\\n\\n### 2. Tool Use Pattern\\n\\n![Toll Use Pattern](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/10/Artboard-1-copy.webp)\\n\\n **Source: Author**\\n\\nThe Tool Use Pattern significantly broadens an LLM’s capability by allowing it to interact with external tools and resources to enhance its problem-solving abilities. Instead of relying solely on internal computations or knowledge, an AI following this pattern can access databases, search the web, or even execute complex functions via programming languages like Python.\\n\\nFor instance, an [**LLM**](https://www.analyticsvidhya.com/blog/2024/07/how-to-fine-tune-large-language-models-with-monsterapi/) could be prompted to retrieve data from the web for a specific query, analyze it, and integrate it into its output. Alternatively, it might be tasked with calculating statistical results, generating images, or manipulating spreadsheets—actions that go beyond simple text generation. By incorporating the use of tools, LLMs evolve from static knowledge banks into dynamic agents capable of interacting with external systems to achieve goals.\\n\\nThis pattern is powerful because it allows [**AI systems**](https://www.analyticsvidhya.com/blog/2018/07/google-unveils-ai-chips-on-device-machine-learning/) to tackle more complex, multifaceted tasks where internal knowledge alone isn’t sufficient, expanding their utility into real-world applications.\\n\\n### 3. Planning Pattern\\n\\n![Planning Pattern](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/10/Artboard-1-copy-5.webp)\\n\\n**Source: Author**\\n\\nThe Planning Pattern enables an LLM to break down large, complicated tasks into smaller, more manageable components. Planning equips an agent with the ability to react to requests and strategically structure the steps needed to achieve a goal.\\n\\nInstead of tackling a problem linearly, ad hocly, an LLM using the Planning Pattern will create a roadmap of subtasks, determining the most efficient path to completion. For example, when coding, the LLM would first outline the overall structure before implementing individual functions. This avoids confusion or meandering logic and keeps the AI focused on the main objective.\\n\\n**[ReAct (Reasoning and Acting)](https://research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/)** and [**ReWOO (Reasoning With Open Ontology)**](https://billxbf.github.io/works/ReWOO_preprint.pdf) further extend this approach by integrating decision-making and contextual reasoning into the planning process. ReAct enables the LLM to dynamically alternate between reasoning (thinking through the problem) and acting (performing specific tasks), allowing for more adaptive and flexible planning. By combining these two steps, the LLM can refine its approach iteratively, addressing unexpected challenges as they arise.\\n\\nReWOO, on the other hand, enhances the planning pattern by using an open-world ontology to guide reasoning. This means the LLM can incorporate broader contextual information and knowledge from various domains, leading to more informed decision-making. With ReWOO, the AI can adjust the plan in real-time based on newly acquired information or changing requirements, ensuring a more robust and comprehensive problem-solving approach.\\n\\nTogether, the Planning Pattern, ReAct, and ReWOO enable an LLM to handle complex tasks in a structured yet adaptive manner, resulting in efficient and goal-oriented execution.\\n\\nMoreover, generating a structured plan (or a “user\\\\_request\\\\_summary”) ensures that the AI keeps track of all steps and doesn’t lose sight of the broader task. This method ensures higher quality and consistency in the results, especially in complex problem-solving or multi-phase projects.\\n\\n### 4. Multi-Agent Pattern\\n\\n![MultiAgent Pattern](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/10/Artboard-1-copy-3.webp)\\n\\n**Source: Author**\\n\\nThe Multi-Agent Pattern builds upon the concept of delegation, akin to project management in human teams. This pattern involves assigning different agents (which are instances of an LLM with specific roles or functions) to handle various subtasks. These agents can work independently on their assignments while also communicating and collaborating to achieve a unified outcome.\\n\\nThere are several types of multi-agent patterns:\\n\\n1. **Collaborative Agents**: Multiple agents work together on different parts of a task, sharing progress and building toward a unified result. Each agent may specialize in a different domain.\\n2. **Supervised Agents**: A central supervisor agent manages other agents, coordinating their activities and verifying results to ensure quality.\\n3. **Hierarchical Teams**: A structured system where higher-level agents oversee lower-level agents, with decision-making cascaded through levels to accomplish complex tasks.\\n\\n***For more details on this, explore: [Multi-agent Collaboration](https://langchain-ai.github.io/langgraph/tutorials/multi_agent/multi-agent-collaboration/).***\\n\\nFor instance, in a scenario requiring both text analysis and numerical computation, two separate agents can handle each task, sharing their results to form a comprehensive solution. One agent might focus on understanding the context, while another processes data, and together they deliver a holistic response. This pattern is particularly powerful for tackling large-scale or complex problems that require diverse skill sets.\\n\\nIn short, the Multiagent Pattern mirrors how humans collaborate across specialities, ensuring that each agent focuses on its strengths while contributing to a greater, coordinated effort.\\n\\nBy mastering these four agentic design patterns, developers and users alike can unlock the full potential of AI systems. The Reflection Pattern improves accuracy and quality through self-evaluation, Tool Use enables dynamic, real-world interactions, Planning provides a roadmap for solving complex tasks, and Multiagent Collaboration ensures that multiple agents work together effectively. Together, these patterns create a foundation for building more intelligent, autonomous AI systems capable of addressing real-world challenges.\\n\\n## Conclusion\\n\\nAgentic Design Patterns emphasize the transformative potential of agentic workflows in making AI models, particularly [**large language models (LLMs)**](https://www.analyticsvidhya.com/blog/2024/08/hallucination-in-llms/), more autonomous and efficient. It explains that while models like GPT-3.5 and GPT-4 perform well in zero-shot tasks, their accuracy and effectiveness significantly improve when adopting an iterative, agentic workflow. This method allows the model to break down tasks, self-evaluate, leverage external tools, plan strategically, and collaborate with other agents, enhancing their problem-solving capabilities.\\n\\nThe article introduces four key design patterns—Reflection, Tool Use, Planning, and Multiagent—that form the foundation of these agentic workflows. These patterns push the boundaries of what AI can do and enable AI systems to behave more independently and intelligently, much like humans handling complex tasks. This signals that future AI advancements will depend on increasing model size and developing more adaptive and strategic workflows.\\n\\nIn this series on Agentic Design Patterns, we will further explore each design pattern in detail: Reflection, Tool Use, Planning, and Multiagent, uncovering how they empower AI systems to become even more autonomous and capable.\\n\\nStay tuned!!!!\\n\\n***Explore the [The Agentic AI Pioneer Program](https://www.analyticsvidhya.com/agenticaipioneer/?utm_source=blog_page&utm_medium=blog&utm_campaign=seo) to deepen your understanding of Agent AI and unlock its full potential. Join us on this journey to discover innovative insights and applications!***\\n\\n## Frequently Asked Questions\\n\\n**Q1. **What are Agentic Design Patterns in AI?****\\n\\nAns. Agentic Design Patterns are strategies used to make AI systems, especially large language models (LLMs), more autonomous and effective. These patterns allow AI to perform tasks, make decisions, and interact with other systems more independently by simulating human-like problem-solving and reasoning processes. The key patterns include Reflection, Tool Use, Planning, and Multi-Agent collaboration.\\n\\n **Q2. **How does the Reflection Pattern improve AI performance?****\\n\\nAns. The Reflection Pattern enhances AI’s ability to self-evaluate and refine its output. By repeatedly reviewing its own work, the AI identifies errors, gaps, or areas for improvement and makes corrections in an iterative loop. This pattern proves particularly useful for tasks requiring precision, such as code generation or content creation, as it helps produce more accurate and reliable results.\\n\\n **Q3. **What is the benefit of using the Tool Use Pattern in AI workflows?****\\n\\nAns. The Tool Use Pattern expands an AI’s capabilities by allowing it to interact with external tools and resources. Instead of solely relying on internal knowledge, the AI can access databases, perform web searches, or execute functions using programming languages like Python. This makes the AI more versatile and able to tackle complex tasks that require information or computations beyond its pre-existing data.\\n\\n **Q4. **How does the Planning Pattern help LLMs handle complex tasks?****\\n\\nAns. The Planning Pattern enables an AI model to break down complicated tasks into smaller, manageable steps, creating a roadmap for solving the problem. This approach helps maintain focus on the main objective and ensures efficient task execution. Variations like ReAct (Reasoning and Acting) and ReWOO (Reasoning With Open Ontology) incorporate decision-making and adaptive strategies, allowing the AI to refine its approach dynamically as new information becomes available.\\n\\n[![Pankaj Singh](https://av-eks-lekhak.s3.amazonaws.com/media/lekhak-profile-images/converted_image_Lb7Lh0T.webp)](https://www.analyticsvidhya.com/blog/author/pankaj9786/)\\n\\n[Pankaj Singh](https://www.analyticsvidhya.com/blog/author/pankaj9786/)\\n\\nHi, I am Pankaj Singh Negi - Senior Content Editor | Passionate about storytelling and crafting compelling narratives that transform ideas into impactful content. I love reading about technology revolutionizing our lifestyle.\\n\\n[Advanced](https://www.analyticsvidhya.com/blog/category/advanced/)[AI Agents](https://www.analyticsvidhya.com/blog/category/ai-agent/)[Artificial Intelligence](https://www.analyticsvidhya.com/blog/category/artificial-intelligence/)[Large Language Models](https://www.analyticsvidhya.com/blog/category/large-language-models/)\\n\\n## Free Courses\\n\\n[![Generative AI](https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/Generative-AI---A-Way-of-Life---Free-Course.webp)\\n\\n4.7\\n\\n#### Generative AI - A Way of Life\\n\\nExplore Generative AI for beginners: create text and images, use top AI tools, learn practical skills, and ethics.](https://courses.analyticsvidhya.com/courses/genai-a-way-of-life/?ref=blog_freecourse)\\n\\n[![Large Language Models](https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/Getting-Started-with-Large-Language-Models.webp)\\n\\n4.5\\n\\n#### Getting Started with Large Language Models\\n\\nMaster Large Language Models (LLMs) with this course, offering clear guidance in NLP and model training made simple.](https://courses.analyticsvidhya.com/courses/getting-started-with-llms?ref=blog_freecourse)\\n\\n[![Prompt Engineering](https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/Building-LLM-Applications-using-Prompt-Engineering---Free-Course.webp)\\n\\n4.6\\n\\n#### Building LLM Applications using Prompt Engineering\\n\\nThis free course guides you on building LLM apps, mastering prompt engineering, and developing chatbots with enterprise data.](https://courses.analyticsvidhya.com/courses/building-llm-applications-using-prompt-engineering-free?ref=blog_freecourse)\\n\\n[![LlamaIndex](https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/Real-World-RAG-Systems.webp)\\n\\n4.8\\n\\n#### Improving Real World RAG Systems: Key Challenges & Practical Solutions\\n\\nExplore practical solutions, advanced retrieval strategies, and agentic RAG systems to improve context, relevance, and accuracy in AI-driven applications.](https://courses.analyticsvidhya.com/courses/improving-real-world-rag-systems-key-challenges/?ref=blog_freecourse)\\n\\n[![RAG systems using LlamaIndex](https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/excel.webp)\\n\\n4.7\\n\\n#### Microsoft Excel: Formulas & Functions\\n\\nMaster MS Excel for data analysis with key formulas, functions, and LookUp tools in this comprehensive course.](https://courses.analyticsvidhya.com/courses/microsoft-excel-formulas-functions?ref=blog_freecourse)\\n\\n#### Recommended Articles\\n\\n* [Comprehensive Guide to Build AI Agents from Scr...](https://www.analyticsvidhya.com/blog/2024/07/build-ai-agents-from-scratch/)\\n* [Evolution of Agentic AI Design Patterns in LLM-...](https://www.analyticsvidhya.com/blog/2024/09/agentic-ai-design-patterns/)\\n* [What is Agentic AI Tool Use Pattern?](https://www.analyticsvidhya.com/blog/2024/10/agentic-ai-tool-use-pattern/)\\n* [What is Agentic AI Reflection Pattern?](https://www.analyticsvidhya.com/blog/2024/10/agentic-ai-reflection-pattern/)\\n* [The Rise of LLM Agents: Revolutionizing AI with...](https://www.analyticsvidhya.com/blog/2024/07/llm-agents-with-iterative-workflows/)\\n* [Agentic Frameworks for Generative AI Applications](https://www.analyticsvidhya.com/blog/2024/09/agentic-frameworks-for-generative-ai-applications/)\\n* [How to Become an Agentic AI Expert in 2025?](https://www.analyticsvidhya.com/blog/2024/10/learning-path-for-ai-agents/)\\n* [What is Agentic AI Planning Pattern?](https://www.analyticsvidhya.com/blog/2024/11/agentic-ai-planning-pattern/)\\n* [Agentic AI Demystified: The Ultimate Guide to A...](https://www.analyticsvidhya.com/blog/2024/05/agentic-ai-demystified-the-ultimate-guide-to-autonomous-agents/)\\n\\n### Responses From Readers\\n\\n[Cancel reply](/blog/2024/10/agentic-design-patterns/#respond)\\n\\nClearSubmit reply\\n\\nΔ\\n\\n[## Write for us\\n\\nWrite, captivate, and earn accolades and rewards for your work](https://datahack.analyticsvidhya.com/blogathon/)\\n[* Reach a Global Audience\\n* Get Expert Feedback\\n* Build Your Brand & Audience\\n\\n* Cash In on Your Knowledge\\n* Join a Thriving Community\\n* Level Up Your Data Science Game](https://datahack.analyticsvidhya.com/blogathon/)\\n\\n[![imag](https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/Write-for-us.webp)](https://datahack.analyticsvidhya.com/blogathon/)\\n\\nWe use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our [Privacy Policy](https://www.analyticsvidhya.com/privacy-policy/?utm_source=courses&utm_medium=footer) & [Cookies Policy](https://www.analyticsvidhya.com/cookies-policy/).\\n\\nShow details\\n\\nAccept all cookies\\n\\nUse necessary cookies\\n\\n##### Powered By Av Logo White\\n\\n* Consent\\n* Details\\n* About\\n\\n###### Cookies\\n\\nThis site uses cookies to ensure that you get the best experience possible. To learn more about how we use cookies, please refer to our [Privacy Policy](/privacy-policy) & [Cookies Policy](/cookies-policy).\\n\\n## Necessary (2) Necessary cookies help make a website usable by enabling basic functions like page navigation and access to secure areas of the website. The website cannot function properly without these cookies.\\n\\n## Analytics Vidhya (4)[learn more about analytics vidhya privacy](https://www.analyticsvidhya.com/privacy-policy/)\\n\\n###### brahmaid\\n\\nIt is needed for personalizing the website.\\n\\nExpiry: Session\\n\\nType: HTTP\\n\\n###### csrftoken\\n\\nThis cookie is used to prevent Cross-site request forgery (often abbreviated as CSRF) attacks of the website\\n\\nExpiry: Session\\n\\nType: HTTPS\\n\\n###### Identityid\\n\\nPreserves the login/logout state of users across the whole site.\\n\\nExpiry: Session\\n\\nType: HTTPS\\n\\n###### sessionid\\n\\nPreserves users' states across page requests.\\n\\nExpiry: Session\\n\\nType: HTTPS\\n\\n## Google (1)[learn more about google privacy](https://policies.google.com/privacy)\\n\\n###### g\\\\_state\\n\\nGoogle One-Tap login adds this g\\\\_state cookie to set the user status on how they interact with the One-Tap modal.\\n\\nExpiry: 365 days\\n\\nType: HTTP\\n\\n## Statistics (4) Statistic cookies help website owners to understand how visitors interact with websites by collecting and reporting information anonymously.\\n\\n## Microsoft (7)[learn more about microsoft policy](https://privacy.microsoft.com/en-us/privacystatement)\\n\\n###### MUID\\n\\nUsed by Microsoft Clarity, to store and track visits across websites.\\n\\nExpiry: 1 Year\\n\\nType: HTTP\\n\\n###### \\\\_clck\\n\\nUsed by Microsoft Clarity, Persists the Clarity User ID and preferences, unique to that site, on the browser. This ensures that behavior in subsequent visits to the same site will be attributed to the same user ID.\\n\\nExpiry: 1 Year\\n\\nType: HTTP\\n\\n###### \\\\_clsk\\n\\nUsed by Microsoft Clarity, Connects multiple page views by a user into a single Clarity session recording.\\n\\nExpiry: 1 Day\\n\\nType: HTTP\\n\\n###### SRM\\\\_I\\n\\nCollects user data is specifically adapted to the user or device. The user can also be followed outside of the loaded website, creating a picture of the visitor's behavior.\\n\\nExpiry: 2 Years\\n\\nType: HTTP\\n\\n###### SM\\n\\nUse to measure the use of the website for internal analytics\\n\\nExpiry: 1 Years\\n\\nType: HTTP\\n\\n###### CLID\\n\\nThe cookie is set by embedded Microsoft Clarity scripts. The purpose of this cookie is for heatmap and session recording.\\n\\nExpiry: 1 Year\\n\\nType: HTTP\\n\\n###### SRM\\\\_B\\n\\nCollected user data is specifically adapted to the user or device. The user can also be followed outside of the loaded website, creating a picture of the visitor's behavior.\\n\\nExpiry: 2 Months\\n\\nType: HTTP\\n\\n## Google (7)[learn more about google privacy](https://policies.google.com/privacy)\\n\\n###### \\\\_gid\\n\\nThis cookie is installed by Google Analytics. The cookie is used to store information of how visitors use a website and helps in creating an analytics report of how the website is doing. The data collected includes the number of visitors, the source where they have come from, and the pages visited in an anonymous form.\\n\\nExpiry: 399 Days\\n\\nType: HTTP\\n\\n###### \\\\_ga\\\\_#\\n\\nUsed by Google Analytics, to store and count pageviews.\\n\\nExpiry: 399 Days\\n\\nType: HTTP\\n\\n###### \\\\_gat\\\\_#\\n\\nUsed by Google Analytics to collect data on the number of times a user has visited the website as well as dates for the first and most recent visit.\\n\\nExpiry: 1 Day\\n\\nType: HTTP\\n\\n###### collect\\n\\nUsed to send data to Google Analytics about the visitor's device and behavior. Tracks the visitor across devices and marketing channels.\\n\\nExpiry: Session\\n\\nType: PIXEL\\n\\n###### AEC\\n\\ncookies ensure that requests within a browsing session are made by the user, and not by other sites.\\n\\nExpiry: 6 Months\\n\\nType: HTTP\\n\\n###### G\\\\_ENABLED\\\\_IDPS\\n\\nuse the cookie when customers want to make a referral from their gmail contacts; it helps auth the gmail account.\\n\\nExpiry: 2 Years\\n\\nType: HTTP\\n\\n###### test\\\\_cookie\\n\\nThis cookie is set by DoubleClick (which is owned by Google) to determine if the website visitor's browser supports cookies.\\n\\nExpiry: 1 Year\\n\\nType: HTTP\\n\\n## Webengage (2)[Learn more about webengage privacy](https://webengage.com/privacy-policy/)\\n\\n###### \\\\_we\\\\_us\\n\\nthis is used to send push notification using webengage.\\n\\nExpiry: 1 Year\\n\\nType: HTTP\\n\\n###### WebKlipperAuth\\n\\nused by webenage to track auth of webenagage.\\n\\nExpiry: Session\\n\\nType: HTTP\\n\\n## LinkedIn (16)[learn more about linkedin privacy](https://www.linkedin.com/legal/privacy-policy)\\n\\n###### ln\\\\_or\\n\\nLinkedin sets this cookie to registers statistical data on users' behavior on the website for internal analytics.\\n\\nExpiry: 1 Day\\n\\nType: HTTP\\n\\n###### JSESSIONID\\n\\nUse to maintain an anonymous user session by the server.\\n\\nExpiry: 1 Year\\n\\nType: HTTP\\n\\n###### li\\\\_rm\\n\\nUsed as part of the LinkedIn Remember Me feature and is set when a user clicks Remember Me on the device to make it easier for him or her to sign in to that device.\\n\\nExpiry: 1 Year\\n\\nType: HTTP\\n\\n###### AnalyticsSyncHistory\\n\\nUsed to store information about the time a sync with the lms\\\\_analytics cookie took place for users in the Designated Countries.\\n\\nExpiry: 6 Months\\n\\nType: HTTP\\n\\n###### lms\\\\_analytics\\n\\nUsed to store information about the time a sync with the AnalyticsSyncHistory cookie took place for users in the Designated Countries.\\n\\nExpiry: 6 Months\\n\\nType: HTTP\\n\\n###### liap\\n\\nCookie used for Sign-in with Linkedin and/or to allow for the Linkedin follow feature.\\n\\nExpiry: 6 Months\\n\\nType: HTTP\\n\\n###### visit\\n\\nallow for the Linkedin follow feature.\\n\\nExpiry: 1 Year\\n\\nType: HTTP\\n\\n###### li\\\\_at\\n\\noften used to identify you, including your name, interests, and previous activity.\\n\\nExpiry: 2 Months\\n\\nType: HTTP\\n\\n###### s\\\\_plt\\n\\nTracks the time that the previous page took to load\\n\\nExpiry: Session\\n\\nType: HTTP\\n\\n###### lang\\n\\nUsed to remember a user's language setting to ensure LinkedIn.com displays in the language selected by the user in their settings\\n\\nExpiry: Session\\n\\nType: HTTP\\n\\n###### s\\\\_tp\\n\\nTracks percent of page viewed\\n\\nExpiry: Session\\n\\nType: HTTP\\n\\n###### AMCV\\\\_14215E3D5995C57C0A495C55%40AdobeOrg\\n\\nIndicates the start of a session for Adobe Experience Cloud\\n\\nExpiry: Session\\n\\nType: HTTP\\n\\n###### s\\\\_pltp\\n\\nProvides page name value (URL) for use by Adobe Analytics\\n\\nExpiry: Session\\n\\nType: HTTP\\n\\n###### s\\\\_tslv\\n\\nUsed to retain and fetch time since last visit in Adobe Analytics\\n\\nExpiry: 6 Months\\n\\nType: HTTP\\n\\n###### li\\\\_theme\\n\\nRemembers a user's display preference/theme setting\\n\\nExpiry: 6 Months\\n\\nType: HTTP\\n\\n###### li\\\\_theme\\\\_set\\n\\nRemembers which users have updated their display / theme preferences\\n\\nExpiry: 6 Months\\n\\nType: HTTP\\n\\n## Preferences (0) Preference cookies enable a website to remember information that changes the way the website behaves or looks, like your preferred language or the region that you are in.\\n\\nWe do not use cookies of this type.\\n\\n## Marketing (4) Marketing cookies are used to track visitors across websites. The intention is to display ads that are relevant and engaging for the individual user and thereby more valuable for publishers and third party advertisers.\\n\\n## Google (11)[learn more about google privacy](https://policies.google.com/privacy)\\n\\n###### \\\\_gcl\\\\_au\\n\\nUsed by Google Adsense, to store and track conversions.\\n\\nExpiry: 3 Months\\n\\nType: HTTP\\n\\n###### SID\\n\\nSave certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search.\\n\\nExpiry: 2 Years\\n\\nType: HTTP\\n\\n###### SAPISID\\n\\nSave certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search.\\n\\nExpiry: 2 Years\\n\\nType: HTTP\\n\\n###### \\\\_\\\\_Secure-#\\n\\nSave certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search.\\n\\nExpiry: 2 Years\\n\\nType: HTTP\\n\\n###### APISID\\n\\nSave certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search.\\n\\nExpiry: 2 Years\\n\\nType: HTTP\\n\\n###### SSID\\n\\nSave certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search.\\n\\nExpiry: 2 Years\\n\\nType: HTTP\\n\\n###### HSID\\n\\nSave certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search.\\n\\nExpiry: 2 Years\\n\\nType: HTTP\\n\\n###### DV\\n\\nThese cookies are used for the purpose of targeted advertising.\\n\\nExpiry: 6 Hours\\n\\nType: HTTP\\n\\n###### NID\\n\\nThese cookies are used for the purpose of targeted advertising.\\n\\nExpiry: 1 Month\\n\\nType: HTTP\\n\\n###### 1P\\\\_JAR\\n\\nThese cookies are used to gather website statistics, and track conversion rates.\\n\\nExpiry: 1 Month\\n\\nType: HTTP\\n\\n###### OTZ\\n\\nAggregate analysis of website visitors\\n\\nExpiry: 6 Months\\n\\nType: HTTP\\n\\n## Facebook (2)[learn more about facebook privacy](https://www.facebook.com/privacy/policy/)\\n\\n###### \\\\_fbp\\n\\nThis cookie is set by Facebook to deliver advertisements when they are on Facebook or a digital platform powered by Facebook advertising after visiting this website.\\n\\nExpiry: 4 Months\\n\\nType: HTTP\\n\\n###### fr\\n\\nContains a unique browser and user ID, used for targeted advertising.\\n\\nExpiry: 2 Months\\n\\nType: HTTP\\n\\n## LinkedIn (6)[Learn about linkedin policy](https://www.linkedin.com/legal/privacy-policy)\\n\\n###### bscookie\\n\\nUsed by LinkedIn to track the use of embedded services.\\n\\nExpiry: 1 Year\\n\\nType: HTTP\\n\\n###### lidc\\n\\nUsed by LinkedIn for tracking the use of embedded services.\\n\\nExpiry: 1 Day\\n\\nType: HTTP\\n\\n###### bcookie\\n\\nUsed by LinkedIn to track the use of embedded services.\\n\\nExpiry: 6 Months\\n\\nType: HTTP\\n\\n###### aam\\\\_uuid\\n\\nUse these cookies to assign a unique ID when users visit a website.\\n\\nExpiry: 6 Months\\n\\nType: HTTP\\n\\n###### UserMatchHistory\\n\\nThese cookies are set by LinkedIn for advertising purposes, including: tracking visitors so that more relevant ads can be presented, allowing users to use the 'Apply with LinkedIn' or the 'Sign-in with LinkedIn' functions, collecting information about how visitors use the site, etc.\\n\\nExpiry: 6 Months\\n\\nType: HTTP\\n\\n###### li\\\\_sugr\\n\\nUsed to make a probabilistic match of a user's identity outside the Designated Countries\\n\\nExpiry: 90 Days\\n\\nType: HTTP\\n\\n## Microsoft (2)[Learn more about microsoft privacy.](https://privacy.microsoft.com/en-us/privacystatement)\\n\\n###### MR\\n\\nUsed to collect information for analytics purposes.\\n\\nExpiry: 1 year\\n\\nType: HTTP\\n\\n###### ANONCHK\\n\\nUsed to store session ID for a users session to ensure that clicks from adverts on the Bing search engine are verified for reporting purposes and for personalisation\\n\\nExpiry: 1 Day\\n\\nType: HTTP\\n\\n## UnclassNameified (0) UnclassNameified cookies are cookies that we are in the process of classNameifying, together with the providers of individual cookies.\\n\\nWe do not use cookies of this type.\\n\\nCookie declaration last updated on 24/03/2023 by Analytics Vidhya.\\n\\nCookies are small text files that can be used by websites to make a user's experience more efficient. The law states that we can store cookies on your device if they are strictly necessary for the operation of this site. For all other types of cookies, we need your permission. This site uses different types of cookies. Some cookies are placed by third-party services that appear on our pages. Learn more about who we are, how you can contact us, and how we process personal data in our [Privacy Policy](/privacy-policy).\\n\\nAccept all cookies\\nUse necessary cookies\\n\\n## Flagship Courses\\n\\n[GenAI Pinnacle Program](https://www.analyticsvidhya.com/genaipinnacle/?ref=footer)| [AI/ML BlackBelt Courses](https://www.analyticsvidhya.com/bbplus?ref=footer)\\n\\n## Free Courses\\n\\n[Generative AI](https://courses.analyticsvidhya.com/courses/genai-a-way-of-life?ref=footer)| [Large Language Models](https://courses.analyticsvidhya.com/courses/getting-started-with-llms?ref=footer)| [Building LLM Applications using Prompt Engineering](https://courses.analyticsvidhya.com/courses/building-llm-applications-using-prompt-engineering-free?ref=footer)| [Building Your first RAG System using LlamaIndex](https://courses.analyticsvidhya.com/courses/building-first-rag-systems-using-llamaindex?ref=footer)| [Stability.AI](https://courses.analyticsvidhya.com/courses/exploring-stability-ai?ref=footer)| [MidJourney](https://courses.analyticsvidhya.com/courses/midjourney_from_inspiration_to_implementation?ref=footer)| [Building Production Ready RAG systems using LlamaIndex](https://courses.analyticsvidhya.com/courses/rag-systems-using-llamaindex?ref=footer)| [Building LLMs for Code](https://courses.analyticsvidhya.com/courses/building-large-language-models-for-code?ref=footer)| [Deep Learning](https://courses.analyticsvidhya.com/courses/getting-started-with-deep-learning?ref=footer)| [Python](https://courses.analyticsvidhya.com/courses/introduction-to-data-science?ref=footer)| [Microsoft Excel](https://courses.analyticsvidhya.com/courses/microsoft-excel-formulas-functions?ref=footer)| [Machine Learning](https://courses.analyticsvidhya.com/courses/Machine-Learning-Certification-Course-for-Beginners?ref=footer)| [Decision Trees](https://courses.analyticsvidhya.com/courses/getting-started-with-decision-trees?ref=footer)| [Pandas for Data Analysis](https://courses.analyticsvidhya.com/courses/pandas-for-data-analysis-in-python?ref=footer)| [Ensemble Learning](https://courses.analyticsvidhya.com/courses/ensemble-learning-and-ensemble-learning-techniques?ref=footer)| [NLP](https://courses.analyticsvidhya.com/courses/Intro-to-NLP?ref=footer)| [NLP using Deep Learning](https://courses.analyticsvidhya.com/courses/exploring-natural-language-processing-nlp-using-deep-learning?ref=footer)| [Neural Networks](https://courses.analyticsvidhya.com/courses/getting-started-with-neural-networks?ref=footer)| [Loan Prediction Practice Problem](https://courses.analyticsvidhya.com/courses/loan-prediction-practice-problem-using-python?ref=footer)| [Time Series Forecasting](https://courses.analyticsvidhya.com/courses/creating-time-series-forecast-using-python?ref=footer)| [Tableau](https://courses.analyticsvidhya.com/courses/tableau-for-beginners?ref=footer)| [Business Analytics](https://courses.analyticsvidhya.com/courses/introduction-to-analytics?ref=footer)\\n\\n## Popular Categories\\n\\n[Generative AI](https://www.analyticsvidhya.com/blog/category/generative-ai/?ref=footer)| [Prompt Engineering](https://www.analyticsvidhya.com/blog/category/prompt-engineering/?ref=footer)| [Generative AI Application](https://www.analyticsvidhya.com/blog/category/generative-ai-application/?ref=footer)| [News](https://news.google.com/publications/CAAqBwgKMJiWzAswyLHjAw?hl=en-IN&gl=IN&ceid=IN%3Aen)| [Technical Guides](https://www.analyticsvidhya.com/blog/category/guide/?ref=footer)| [AI Tools](https://www.analyticsvidhya.com/blog/category/ai-tools/?ref=footer)| [Interview Preparation](https://www.analyticsvidhya.com/blog/category/interview-questions/?ref=footer)| [Research Papers](https://www.analyticsvidhya.com/blog/category/research-paper/?ref=footer)| [Success Stories](https://www.analyticsvidhya.com/blog/category/success-story/?ref=footer)| [Quiz](https://www.analyticsvidhya.com/blog/category/quiz/?ref=footer)| [Use Cases](https://www.analyticsvidhya.com/blog/category/use-cases/?ref=footer)| [Listicles](https://www.analyticsvidhya.com/blog/category/listicle/?ref=footer)\\n\\n## Generative AI Tools and Techniques\\n\\n[GANs](https://www.analyticsvidhya.com/blog/2021/10/an-end-to-end-introduction-to-generative-adversarial-networksgans/?ref=footer)| [VAEs](https://www.analyticsvidhya.com/blog/2023/07/an-overview-of-variational-autoencoders/?ref=footer)| [Transformers](https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models?ref=footer)| [StyleGAN](https://www.analyticsvidhya.com/blog/2021/05/stylegan-explained-in-less-than-five-minutes/?ref=footer)| [Pix2Pix](https://www.analyticsvidhya.com/blog/2023/10/pix2pix-unleashed-transforming-images-with-creative-superpower?ref=footer)| [Autoencoders](https://www.analyticsvidhya.com/blog/2021/06/autoencoders-a-gentle-introduction?ref=footer)| [GPT](https://www.analyticsvidhya.com/blog/2022/10/generative-pre-training-gpt-for-natural-language-understanding/?ref=footer)| [BERT](https://www.analyticsvidhya.com/blog/2022/11/comprehensive-guide-to-bert/?ref=footer)| [Word2Vec](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/?ref=footer)| [LSTM](https://www.analyticsvidhya.com/blog/2021/03/introduction-to-long-short-term-memory-lstm?ref=footer)| [Attention Mechanisms](https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/?ref=footer)| [Diffusion Models](https://www.analyticsvidhya.com/blog/2024/09/what-are-diffusion-models/?ref=footer)| [LLMs](https://www.analyticsvidhya.com/blog/2023/03/an-introduction-to-large-language-models-llms/?ref=footer)| [SLMs](https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models?ref=footer)| [StyleGAN](https://www.analyticsvidhya.com/blog/2024/05/what-are-small-language-models-slms/?ref=footer)| [Encoder Decoder Models](https://www.analyticsvidhya.com/blog/2023/10/advanced-encoders-and-decoders-in-generative-ai/?ref=footer)| [Prompt Engineering](https://www.analyticsvidhya.com/blog/2023/06/what-is-prompt-engineering/?ref=footer)| [LangChain](https://www.analyticsvidhya.com/blog/2024/06/langchain-guide/?ref=footer)| [LlamaIndex](https://www.analyticsvidhya.com/blog/2023/10/rag-pipeline-with-the-llama-index/?ref=footer)| [RAG](https://www.analyticsvidhya.com/blog/2023/09/retrieval-augmented-generation-rag-in-ai/?ref=footer)| [Fine-tuning](https://www.analyticsvidhya.com/blog/2023/08/fine-tuning-large-language-models/?ref=footer)| [LangChain AI Agent](https://www.analyticsvidhya.com/blog/2024/07/langchains-agent-framework/?ref=footer)| [Multimodal Models](https://www.analyticsvidhya.com/blog/2023/12/what-are-multimodal-models/?ref=footer)| [RNNs](https://www.analyticsvidhya.com/blog/2022/03/a-brief-overview-of-recurrent-neural-networks-rnn/?ref=footer)| [DCGAN](https://www.analyticsvidhya.com/blog/2021/07/deep-convolutional-generative-adversarial-network-dcgan-for-beginners/?ref=footer)| [ProGAN](https://www.analyticsvidhya.com/blog/2021/05/progressive-growing-gan-progan/?ref=footer)| [Text-to-Image Models](https://www.analyticsvidhya.com/blog/2024/02/llm-driven-text-to-image-with-diffusiongpt/?ref=footer)| [DDPM](https://www.analyticsvidhya.com/blog/2024/08/different-components-of-diffusion-models/?ref=footer)| [Document Question Answering](https://www.analyticsvidhya.com/blog/2024/04/a-hands-on-guide-to-creating-a-pdf-based-qa-assistant-with-llama-and-llamaindex/?ref=footer)| [Imagen](https://www.analyticsvidhya.com/blog/2024/09/google-imagen-3/?ref=footer)| [T5 (Text-to-Text Transfer Transformer)](https://www.analyticsvidhya.com/blog/2024/05/text-summarization-using-googles-t5-base/?ref=footer)| [Seq2seq Models](https://www.analyticsvidhya.com/blog/2020/08/a-simple-introduction-to-sequence-to-sequence-models/?ref=footer)| [WaveNet](https://www.analyticsvidhya.com/blog/2020/01/how-to-perform-automatic-music-generation/?ref=footer)| [Attention Is All You Need (Transformer Architecture)](https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/?ref=footer)\\n\\n## Popular GenAI Models\\n\\n[Llama 3.1](https://www.analyticsvidhya.com/blog/2024/07/meta-llama-3-1/?ref=footer)| [Llama 3](https://www.analyticsvidhya.com/blog/2024/04/meta-llama-3-redefining-large-language-model-standards/?ref=footer)| [Llama 2](https://www.analyticsvidhya.com/blog/2023/08/getting-started-with-llama-2/?ref=footer)| [GPT 4o Mini](https://www.analyticsvidhya.com/blog/2024/07/gpt-4o-mini/?ref=footer)| [GPT 4o](https://www.analyticsvidhya.com/blog/2024/05/openai-flagship-model-gpt-omni/?ref=footer)| [GPT 3](https://www.analyticsvidhya.com/blog/2021/01/gpt-3-the-next-big-thing-foundation-of-future/?ref=footer)| [Claude 3 Haiku](https://www.analyticsvidhya.com/blog/2024/03/the-fastest-ai-model-by-anthropic-claude-haiku/?ref=footer)| [Claude 3.5 Sonnet](https://www.analyticsvidhya.com/blog/2024/06/claude-3-5-sonnet/?ref=footer)| [Phi 3.5](https://www.analyticsvidhya.com/blog/2024/09/phi-3-5-slms/?ref=footer)| [Phi 3](https://www.analyticsvidhya.com/blog/2024/05/microsoft-phi3/?ref=footer)| [Mistral Large 2](https://www.analyticsvidhya.com/blog/2024/07/mistral-large-2-powerful-enough-to-challenge-llama-3-1-405b/?ref=footer)| [Mistral NeMo](https://www.analyticsvidhya.com/blog/2024/08/mistral-nemo/?ref=footer)| [Mistral-7b](https://www.analyticsvidhya.com/blog/2024/01/making-the-most-of-mistral-7b-with-finetuning/?ref=footer)| [Gemini 1.5 Pro](https://www.analyticsvidhya.com/blog/2024/04/gemini-pro-goes-global-with-powerful-new-features/?ref=footer)| [Gemini Flash 1.5](https://www.analyticsvidhya.com/blog/2024/08/building-a-food-vision-webapp-with-the-gemini-flash-1-5-model/?ref=footer)| [Bedrock](https://www.analyticsvidhya.com/blog/2024/02/building-end-to-end-generative-ai-models-with-aws-bedrock/?ref=footer)| [Vertex AI](https://www.analyticsvidhya.com/blog/2024/02/build-deploy-and-manage-ml-models-with-google-vertex-ai/?ref=footer)| [DALL.E](https://www.analyticsvidhya.com/blog/2021/01/openais-future-of-vision-with-dall-e-creating-images-from-text/?ref=footer)| [Midjourney](https://courses.analyticsvidhya.com/courses/midjourney_from_inspiration_to_implementation/?ref=footer)| [Stable Diffusion](https://www.analyticsvidhya.com/blog/2023/12/what-is-stable-diffusion/?ref=footer)\\n\\n## Data Science Tools and Techniques\\n\\n[Python](https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/?ref=footer)| [R](https://www.analyticsvidhya.com/blog/2016/02/complete-tutorial-learn-data-science-scratch/?ref=footer)| [SQL](https://www.analyticsvidhya.com/blog/2022/01/learning-sql-from-basics-to-advance/?ref=footer)| [Jupyter Notebooks](https://www.analyticsvidhya.com/blog/2018/05/starters-guide-jupyter-notebook/?ref=footer)| [TensorFlow](https://www.analyticsvidhya.com/blog/2021/11/tensorflow-for-beginners-with-examples-and-python-implementation/?ref=footer)| [Scikit-learn](https://www.analyticsvidhya.com/blog/2021/08/complete-guide-on-how-to-learn-scikit-learn-for-data-science/?ref=footer)| [PyTorch](https://www.analyticsvidhya.com/blog/2018/02/pytorch-tutorial/?ref=footer)| [Tableau](https://www.analyticsvidhya.com/blog/2021/09/a-complete-guide-to-tableau-for-beginners-in-data-visualization/?ref=footer)| [Apache Spark](https://www.analyticsvidhya.com/blog/2022/08/introduction-to-on-apache-spark-and-its-datasets/?ref=footer)| [Matplotlib](https://www.analyticsvidhya.com/blog/2021/10/introduction-to-matplotlib-using-python-for-beginners/?ref=footer)| [Seaborn](https://www.analyticsvidhya.com/blog/2021/02/a-beginners-guide-to-seaborn-the-simplest-way-to-learn/?ref=footer)| [Pandas](https://www.analyticsvidhya.com/blog/2021/03/pandas-functions-for-data-analysis-and-manipulation/?ref=footer)| [Hadoop](https://www.analyticsvidhya.com/blog/2022/05/an-introduction-to-hadoop-ecosystem-for-big-data/?ref=footer)| [Docker](https://www.analyticsvidhya.com/blog/2021/10/end-to-end-guide-to-docker-for-aspiring-data-engineers/?ref=footer)| [Git](https://www.analyticsvidhya.com/blog/2021/09/git-and-github-tutorial-for-beginners/?ref=footer)| [Keras](https://www.analyticsvidhya.com/blog/2016/10/tutorial-optimizing-neural-networks-using-keras-with-image-recognition-case-study/?ref=footer)| [Apache Kafka](https://www.analyticsvidhya.com/blog/2022/12/introduction-to-apache-kafka-fundamentals-and-working/?ref=footer)| [AWS](https://www.analyticsvidhya.com/blog/2020/09/what-is-aws-amazon-web-services-data-science/?ref=footer)| [NLP](https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/?ref=footer)| [Random Forest](https://www.analyticsvidhya.com/blog/2021/06/understanding-random-forest/?ref=footer)| [Computer Vision](https://www.analyticsvidhya.com/blog/2020/01/computer-vision-learning-path/?ref=footer)| [Data Visualization](https://www.analyticsvidhya.com/blog/2021/04/a-complete-beginners-guide-to-data-visualization/?ref=footer)| [Data Exploration](https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/?ref=footer)| [Big Data](https://www.analyticsvidhya.com/blog/2021/05/what-is-big-data-introduction-uses-and-applications/?ref=footer)| [Common Machine Learning Algorithms](https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/?ref=footer)| [Machine Learning](https://www.analyticsvidhya.com/blog/category/Machine-Learning/?ref=footer)\\n\\n## Company\\n\\n* [About Us](https://www.analyticsvidhya.com/about/?ref=global_footer)\\n* [Contact Us](https://www.analyticsvidhya.com/contact/?ref=global_footer)\\n* [Careers](https://www.analyticsvidhya.com/careers/?ref=global_footer)\\n\\n## Discover\\n\\n* [Blogs](https://www.analyticsvidhya.com/blog/?ref=global_footer)\\n* [Expert Sessions](https://www.analyticsvidhya.com/events/datahour/?ref=global_footer)\\n* [Learning Paths](https://www.analyticsvidhya.com/blog/category/learning-path/?ref=global_footer)\\n* [Comprehensive Guides](https://www.analyticsvidhya.com/category/guide/?ref=global_footer)\\n\\n## Learn\\n\\n* [Free Courses](https://www.analyticsvidhya.com/all-free-courses?ref=global_footer)\\n* [AI&ML Program](https://www.analyticsvidhya.com/bbplus?ref=global_footer)\\n* [GenAI Program](https://www.analyticsvidhya.com/genaipinnacle/?ref=global_footer)\\n* [Agentic AI Program](https://www.analyticsvidhya.com/agenticaipioneer/?ref=footer)\\n\\n## Engage\\n\\n* [Community](https://community.analyticsvidhya.com/?ref=global_footer)\\n* [Hackathons](https://www.analyticsvidhya.com/datahack/?ref=global_footer)\\n* [Events](https://www.analyticsvidhya.com/events/?ref=global_footer)\\n* [Podcasts](https://www.analyticsvidhya.com/events/leading-with-data/?ref=global_footer)\\n\\n## Contribute\\n\\n* [Become an Author](https://www.analyticsvidhya.com/datahack/blogathon/?ref=global_footer)\\n* [Become a Speaker](https://docs.google.com/forms/d/e/1FAIpQLSdTDIsIUzmliuTkXIlTX6qI65RCiksQ3nCbTJ7twNx2rgEsXw/viewform?ref=global_footer)\\n* [Become a Mentor](https://docs.google.com/forms/d/e/1FAIpQLSdTDIsIUzmliuTkXIlTX6qI65RCiksQ3nCbTJ7twNx2rgEsXw/viewform?ref=global_footer)\\n* [Become an Instructor](https://docs.google.com/forms/d/e/1FAIpQLSdTDIsIUzmliuTkXIlTX6qI65RCiksQ3nCbTJ7twNx2rgEsXw/viewform?ref=global_footer)\\n\\n## Enterprise\\n\\n* [Our Offerings](https://enterprise.analyticsvidhya.com/?ref=global_footer)\\n* [Trainings](https://www.analyticsvidhya.com/enterprise/training?ref=global_footer)\\n* [Data Culture](https://www.analyticsvidhya.com/enterprise/data-culture?ref=global_footer)\\n* [AI Newsletter](https://newsletter.ai/?ref=global_footer)\\n\\n[Terms & conditions](https://www.analyticsvidhya.com/terms/)\\n\\n[Refund Policy](https://www.analyticsvidhya.com/refund-policy/)\\n\\n[Privacy Policy](https://www.analyticsvidhya.com/privacy-policy/)\\n\\n[Cookies Policy](https://www.analyticsvidhya.com/cookies-policy)\\n© Analytics Vidhya 2025.All rights reserved.\\n\\n## GenAI Pinnacle Program\\n\\n## Revolutionizing AI Learning & Development\\n\\n* 1:1 Mentorship with Generative AI experts\\n* Advanced Curriculum with 200+ Hours of Learning\\n* Master 26+ GenAI Tools and Libraries\\n\\n#### Enroll with us today!\\n\\nI Agree to the [Terms & Conditions](/terms)\\n\\nSend WhatsApp Updates\\n\\nEnroll Now\\n\\n![Av Logo White](data:image/png;base64...)\\n## Continue your learning for FREE\\n\\nLogin with Google\\n\\nLogin with Email\\n\\n[Forgot your password?](https://id.analyticsvidhya.com/auth/password/reset/?utm_source=newhomepage)\\n\\nI accept the [Terms and Conditions](/terms)\\n\\nReceive updates on WhatsApp\\n\\n![Av Logo White](data:image/png;base64...)\\n\\n## Enter email address to continue\\n\\nEmail address\\n\\nGet OTP\\n\\n![Av Logo White](data:image/png;base64...)\\n\\n## Enter OTP sent to\\n\\nEdit\\n\\nEnter the OTP\\n\\nResend OTP\\n\\nResend OTP in 45s\\n\\nVerify OTP\", 'Top 4 Agentic AI Architecture Design Patterns | by Mohammed Lubbad | Medium\\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2ad890a543e8&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&source=---top_nav_layout_nav----------------------------------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmlubbad.medium.com%2Ftop-4-agentic-ai-architecture-design-patterns-2ad890a543e8&source=post_page---top_nav_layout_nav-----------------------global_nav-----------)\\n\\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav-----------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmlubbad.medium.com%2Ftop-4-agentic-ai-architecture-design-patterns-2ad890a543e8&source=post_page---top_nav_layout_nav-----------------------global_nav-----------)\\n\\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)![]()\\n\\nMember-only story\\n\\n# Top 4 Agentic AI Architecture Design Patterns\\n\\n[![Mohammed Lubbad](https://miro.medium.com/v2/resize:fill:88:88/1*byKvMjWHIzL79U05KynPEA.jpeg)](/?source=post_page---byline--2ad890a543e8--------------------------------)\\n\\n[Mohammed Lubbad](/?source=post_page---byline--2ad890a543e8--------------------------------)\\n\\n·\\n\\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F424d1f13b3e7&operation=register&redirect=https%3A%2F%2Fmlubbad.medium.com%2Ftop-4-agentic-ai-architecture-design-patterns-2ad890a543e8&user=Mohammed+Lubbad&userId=424d1f13b3e7&source=post_page-424d1f13b3e7--byline--2ad890a543e8---------------------post_header-----------)\\n\\n4 min read·Oct 10, 2024\\n\\n--\\n\\nShare\\n\\nArtificial intelligence (AI) is evolving rapidly, reshaping how we approach problem-solving and system design. But what if we could empower AI systems to take even more initiative, learn from their environments, and autonomously execute complex tasks? Enter “Agentic Design Patterns” — a groundbreaking approach to designing AI systems that emphasizes the agency of intelligent agents.\\n\\nThese patterns allow AI models to move beyond passive, rule-based responses, enabling more dynamic, adaptable, and autonomous behaviors.\\n\\nThis article explores how agentic design patterns can revolutionize AI systems, offering unprecedented control and flexibility for real-world applications.\\n\\n> You can continue reading [here](https://substack.com/%40mohammedalubbad/p-151048035) even without a paid Medium account.\\n\\n![]()\\n# Why AI Agents?\\n\\n* LLMs have increased performance using Agentic workflows.\\n* GPT-3.5 powered Agentic AI Systems achieved up to 95.1% on HumanEval coding benchmark.\\n* Easy to connect LLMs, tools and prompts along with external data to build simple and complex agentic workflows.\\n\\n--\\n\\n--\\n\\n[![Mohammed Lubbad](https://miro.medium.com/v2/resize:fill:96:96/1*byKvMjWHIzL79U05KynPEA.jpeg)](/?source=post_page---post_author_info--2ad890a543e8--------------------------------)[![Mohammed Lubbad](https://miro.medium.com/v2/resize:fill:128:128/1*byKvMjWHIzL79U05KynPEA.jpeg)](/?source=post_page---post_author_info--2ad890a543e8--------------------------------)Follow[## Written by Mohammed Lubbad](/?source=post_page---post_author_info--2ad890a543e8--------------------------------)[216 Followers](/followers?source=post_page---post_author_info--2ad890a543e8--------------------------------)·[98 Following](/following?source=post_page---post_author_info--2ad890a543e8--------------------------------)\\n\\nSenior Data Scientist | IBM Certified Data Scientist | AI Researcher | Chief Technology Officer | Machine Learning Expert | Public Speaker\\n\\nFollow\\n## No responses yet\\n\\n[Help](https://help.medium.com/hc/en-us?source=post_page-----2ad890a543e8--------------------------------)[Status](https://medium.statuspage.io/?source=post_page-----2ad890a543e8--------------------------------)[About](https://medium.com/about?autoplay=1&source=post_page-----2ad890a543e8--------------------------------)[Careers](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----2ad890a543e8--------------------------------)[Press](pressinquiries%40medium.com?source=post_page-----2ad890a543e8--------------------------------)[Blog](https://blog.medium.com/?source=post_page-----2ad890a543e8--------------------------------)[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2ad890a543e8--------------------------------)[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2ad890a543e8--------------------------------)[Text to speech](https://speechify.com/medium?source=post_page-----2ad890a543e8--------------------------------)[Teams](https://medium.com/business?source=post_page-----2ad890a543e8--------------------------------)', 'Agentic AI Design Patterns Examples - Analytics Yogi\\n[Analytics Yogi](https://vitalflux.com/ \"Analytics Yogi\")\\nReimagining Data-driven Society with Agentic AI\\n\\n* [Home](https://vitalflux.com/)\\n* [Prompt Library](https://vitalflux.com/prompt-library/)\\n* [DS/AI Trends](https://vitalflux.com/data-ai-machine-learning-data-science-trends/ \"Data Science / AI Trends\")\\n* [Stats Tools](https://vitalflux.com/statistical-tools/)\\n* [AI Research](https://vitalflux.com/category/data-science/)\\n  + [Admissions](https://vitalflux.com/category/career/admissions/)\\n  + [Courses](https://vitalflux.com/category/career/online-courses/)\\n  + [Interview Questions](https://vitalflux.com/category/career/interview-questions/ \"Blog posts on interview questions, tips\")\\n  + [Generative AI](https://vitalflux.com/category/generative-ai/)\\n  + [Statistics](https://vitalflux.com/category/statistics/)\\n  + [Machine Learning](https://vitalflux.com/category/machine-learning/)\\n  + [NLP](https://vitalflux.com/category/nlp/)\\n  + [Deep Learning](https://vitalflux.com/category/deep-learning/)\\nSelect a pageHome\\nPrompt Library\\nDS/AI Trends\\nStats Tools\\nAI Research\\n\\xa0\\xa0\\xa0\\xa0Admissions\\n\\xa0\\xa0\\xa0\\xa0Courses\\n\\xa0\\xa0\\xa0\\xa0Interview Questions\\n\\xa0\\xa0\\xa0\\xa0Generative AI\\n\\xa0\\xa0\\xa0\\xa0Statistics\\n\\xa0\\xa0\\xa0\\xa0Machine Learning\\n\\xa0\\xa0\\xa0\\xa0NLP\\n\\xa0\\xa0\\xa0\\xa0Deep Learning\\n\\n# Agentic AI Design Patterns Examples\\n\\n[January 6, 2025](https://vitalflux.com/agentic-ai-design-patterns-examples/ \"4:26 pm\") by [Ajitesh Kumar](https://vitalflux.com/author/vitalflux/ \"View all posts by Ajitesh Kumar\")  · [Leave a comment](https://vitalflux.com/agentic-ai-design-patterns-examples/#respond)\\n![agentic ai design patterns examples](https://vitalflux.com/wp-content/uploads/2025/01/agentic-ai-design-patterns-examples-298x300.png)\\n\\nIn the ever-evolving landscape of [agentic AI](https://vitalflux.com/category/agentic-ai) workflows and applications, understanding and leveraging design patterns is crucial for building effective and innovative solutions. Agentic AI design patterns provide structured approaches to solving complex problems. They enhance the capabilities of AI agents by enabling reasoning, planning, collaboration, and tool integration.\\n\\nFor instance, you can think of these patterns as a blueprint for constructing a well-oiled team of specialists in a workplace—each with unique roles and tools, working in harmony to tackle a project efficiently and innovatively. Imagine a team of engineers collaborating on designing a new car, where one member focuses on aerodynamics, another on engine performance, and a third on safety features, all using specialized tools to achieve a unified goal.\\n\\nThis blogpost explores some of the key design patterns suggested by Andrew NG, shedding light on their practical applications and benefits ([Watch Here](https://www.youtube.com/watch?v=KrRD7r7y7NY)). Whether you are an AI researcher refining models with iterative feedback, a product manager streamlining operations and enhancing user experiences, or a developer building robust applications, these patterns offer valuable insights and strategies to elevate your projects.\\n\\nTable of Contents\\n\\nToggle\\n\\n* [Key Design Patterns and Real-World Examples](#Key_Design_Patterns_and_Real-World_Examples \"Key Design Patterns and Real-World Examples\")\\n  + [1. Reflection with LLMs](#1_Reflection_with_LLMs \"1. Reflection with LLMs\")\\n  + [2. Tools Use](#2_Tools_Use \"2. Tools Use\")\\n  + [3. Reasoning / Planning](#3_Reasoning_Planning \"3. Reasoning / Planning\")\\n  + [4. Multi-agent Collaboration](#4_Multi-agent_Collaboration \"4. Multi-agent Collaboration\")\\n  + [5. Agents Having Single Responsibility](#5_Agents_Having_Single_Responsibility \"5. Agents Having Single Responsibility\")\\n* [Why These Design Patterns Matter](#Why_These_Design_Patterns_Matter \"Why These Design Patterns Matter\")\\n  + [Impactful Industries:](#Impactful_Industries \"Impactful Industries:\")\\n* [Additional Resources](#Additional_Resources \"Additional Resources\")\\n## Key Design Patterns and Real-World Examples\\n\\n### 1. Reflection with LLMs\\n\\nThis pattern involves collaborating with [LLMs](https://vitalflux.com/large-language-models-concepts-examples/) to refine their output instead of relying on zero-shot responses. Zero-shot responses occur when AI generates an answer without prior examples or context-specific training, often resulting in less precise outputs. By iterating with the AI, users can guide the system to produce results that better align with their needs.\\n\\n**Example:** In content creation, an LLM generates a draft blog post. The user iteratively provides feedback and prompts for improvements, refining the tone, accuracy, and content until the desired output is achieved, akin to a human-editor loop.\\n\\n### 2. Tools Use\\n\\nThis pattern enables [LLM](https://vitalflux.com/category/large-language-models/) agents to use external tools to perform actions. Examples include:\\n\\n* **Customer Service:** An AI agent integrates with a company’s CRM to retrieve or update customer data.\\n* **Healthcare:** An AI-driven assistant accesses an external medical database to assist doctors with diagnoses and treatment options.\\n* **Task Automation:** An AI agent uses a calendar application to schedule meetings.\\n\\nThese patterns involve APIs for integration, function calls for executing tasks programmatically, and writing to databases for consistent and accessible data storage. Together, they enhance the versatility and functionality of AI systems across domains like healthcare, education, and logistics.\\n\\n### 3. Reasoning / Planning\\n\\nThis pattern leverages LLMs to determine a sequence of steps or actions to achieve a goal.\\n\\n**Example:** An AI travel assistant plans a vacation itinerary, identifying the optimal sequence of flights, accommodations, and activities based on user preferences, constraints, and real-time data.\\n\\n### 4. Multi-agent Collaboration\\n\\nMultiple agents work together to achieve a design goal.\\n\\n**Example:** In supply chain management, one AI agent predicts demand, another optimizes inventory, and a third coordinates logistics. This collaboration ensures efficiency and cost-effectiveness across the supply chain.\\n\\n### 5. Agents Having Single Responsibility\\n\\nExperts recommend creating AI agents with a single focus to ensure high accuracy and reliability in their tasks.\\n\\n**Examples:**\\n\\n* **Sentiment Analysis:** An AI agent classifies customer support tickets as positive, negative, or neutral.\\n* **Incident Prioritization:** An agent assigns priorities such as “critical,” “high,” or “low” to customer issues.\\n* **Email Automation:** An agent sends notifications, such as confirming ticket updates or delivering resolution summaries.\\n\\nSuch narrowly focused agents are easier to manage and integrate into larger workflows for customer service management.\\n\\n## Why These Design Patterns Matter\\n\\nBy incorporating these design patterns, product managers, architects, and developers can create sophisticated and capable agentic AI systems to address complex challenges.\\n\\n### Impactful Industries:\\n\\n* **Healthcare:** AI streamlines diagnosis and treatment planning.\\n* **Logistics:** Multi-agent systems optimize supply chains.\\n* **Customer Service:** Sentiment analysis and prioritization enhance user experiences.\\n\\nBy targeting specific challenges like operational efficiency and data-driven decision-making, these design patterns pave the way for transformative advancements across diverse fields.\\n\\n## Additional Resources\\n\\nFor readers interested in diving deeper, we have curated a comprehensive list of agentic AI resources, including research papers, videos, and courses. Explore it here: [Agentic AI Resources](https://vitalflux.com/list-of-agentic-ai-resources-papers/).\\n\\nThese patterns streamline workflows, enhance decision-making, and enable seamless integration of AI with external tools and multi-agent systems, driving innovation and efficiency across industries.\\n\\n* [Author](#abh_about)\\n* [Recent Posts](#abh_posts)\\n\\n [![Ajitesh Kumar](https://vitalflux.com/wp-content/uploads/gravatar/ajitesh-shukla-pic.jpg)](https://vitalflux.com/author/vitalflux \"Ajitesh Kumar\")  [Ajitesh Kumar](https://vitalflux.com/author/vitalflux)I have been recently working in the area of Data analytics including Data Science and Machine Learning / Deep Learning. I am also passionate about different technologies including programming languages such as Java/JEE, Javascript, Python, R, Julia, etc, and technologies such as Blockchain, mobile computing, cloud-native technologies, application security, cloud computing platforms, big data, etc. I would love to connect with you on [Linkedin](https://www.linkedin.com/in/ajitesh/).\\nCheck out my latest book titled as [First Principles Thinking: Building winning products using first principles thinking](https://www.amazon.com/dp/B09P8SZ5RK/). [![Ajitesh Kumar](https://vitalflux.com/wp-content/uploads/gravatar/ajitesh-shukla-pic.jpg)](https://vitalflux.com/author/vitalflux \"Ajitesh Kumar\") Latest posts by Ajitesh Kumar ([see all](https://vitalflux.com/author/vitalflux/))\\n\\n* [What are AI Agents? How do they work?](https://vitalflux.com/what-ai-agents-how-do-they-work/) - January 7, 2025\\n* [Agentic AI Design Patterns Examples](https://vitalflux.com/agentic-ai-design-patterns-examples/) - January 6, 2025\\n* [List of Agentic AI Resources, Papers, Courses](https://vitalflux.com/list-of-agentic-ai-resources-papers/) - January 5, 2025\\n\\n## [Ajitesh Kumar](https://vitalflux.com/author/vitalflux/)\\n\\nI have been recently working in the area of Data analytics including Data Science and Machine Learning / Deep Learning. I am also passionate about different technologies including programming languages such as Java/JEE, Javascript, Python, R, Julia, etc, and technologies such as Blockchain, mobile computing, cloud-native technologies, application security, cloud computing platforms, big data, etc. I would love to connect with you on [Linkedin](https://www.linkedin.com/in/ajitesh/).\\nCheck out my latest book titled as [First Principles Thinking: Building winning products using first principles thinking](https://www.amazon.com/dp/B09P8SZ5RK/).\\n\\nPosted in [agentic ai](https://vitalflux.com/category/agentic-ai/), [Large Language Models](https://vitalflux.com/category/large-language-models/). Tagged with [agentic ai](https://vitalflux.com/tag/agentic-ai/), [LLMs](https://vitalflux.com/tag/llms/).\\n\\n[← List of Agentic AI Resources, Papers, Courses](https://vitalflux.com/list-of-agentic-ai-resources-papers/)\\n[What are AI Agents? How do they work? →](https://vitalflux.com/what-ai-agents-how-do-they-work/)\\n\\n* Search for:\\n* ### ChatGPT Prompts (250+)\\n\\n  + [Generate Design Ideas for App](https://vitalflux.com/prompts/20240127-prompt-generate-design-ideas-for-app/)\\n  + [Expand Feature Set of App](https://vitalflux.com/prompts/20240127-prompt-expand-feature-set-of-app/)\\n  + [Create a User Journey Map for App](https://vitalflux.com/prompts/20240127-prompt-create-a-user-journey-map-for-app/)\\n  + [Generate Visual Design Ideas for App](https://vitalflux.com/prompts/20240127-prompt-generate-visual-design-ideas-for-app/)\\n  + [Generate a List of Competitors for App](https://vitalflux.com/prompts/20240127-prompt-generate-a-list-of-competitors-for-app/)[**More...**](https://vitalflux.com/prompt-library/)\\n* ### Recent Posts\\n\\n  + [What are AI Agents? How do they work?](https://vitalflux.com/what-ai-agents-how-do-they-work/)\\n  + [Agentic AI Design Patterns Examples](https://vitalflux.com/agentic-ai-design-patterns-examples/)\\n  + [List of Agentic AI Resources, Papers, Courses](https://vitalflux.com/list-of-agentic-ai-resources-papers/)\\n  + [Understanding FAR, FRR, and EER in Auth Systems](https://vitalflux.com/understanding-far-frr-eer-authentication-systems/)\\n  + [Top 10 Gartner Technology Trends for 2025](https://vitalflux.com/top-10-gartner-technology-trends-for-2025/)\\n* ### Data Science / AI Trends\\n\\n  + • [Sentiment Analysis Real World Examples](https://www.repustate.com/blog/sentiment-analysis-real-world-examples/)\\n  + • [Prepend any arxiv.org link with talk2 to load the paper into a responsive chat application](https://github.com/evanhu1/talk2arxiv)\\n  + • [Custom LLM and AI Agents (RAG) On Structured + Unstructured Data - AI Brain For Your Organization](https://twitter.com/bindureddy/status/1748156782932029608/)\\n  + • [Guides, papers, lecture, notebooks and resources for prompt engineering](https://github.com/dair-ai/Prompt-Engineering-Guide)\\n  + • [Common tricks to make LLMs efficient and stable](https://twitter.com/cwolferesearch/status/1743045152938057996)[**More...**](https://vitalflux.com/data-ai-machine-learning-data-science-trends/)\\n* ### Free Online Tools\\n* + [Create Scatter Plots Online for your Excel Data](https://vitalflux.com/online-scatter-plot-maker-works-with-your-excel-data/)\\n  + [Histogram / Frequency Distribution Creation Tool](https://vitalflux.com/statistics-tool-for-creating-histogram-frequency-distribution/)\\n  + [Online Pie Chart Maker Tool](https://vitalflux.com/tool-online-pie-chart-maker/)\\n  + [Z-test vs T-test Decision Tool](https://vitalflux.com/z-test-vs-t-test-decision-tool/)\\n  + [Independent samples t-test calculator](https://vitalflux.com/independent-samples-t-test-calculator/)\\n* [More...](https://vitalflux.com/statistical-tools/)\\n* ### Newsletter\\n\\n  Name\\n\\n  Email\\n* ### Tag Cloud\\n\\n  [ai (98)](https://vitalflux.com/tag/ai/)\\n  [Angular (50)](https://vitalflux.com/tag/angular/)\\n  [angularjs (104)](https://vitalflux.com/tag/angularjs/)\\n  [api (16)](https://vitalflux.com/tag/api/)\\n  [Application Security (22)](https://vitalflux.com/tag/application-security/)\\n  [artificial intelligence (20)](https://vitalflux.com/tag/artificial-intelligence-2/)\\n  [AWS (23)](https://vitalflux.com/tag/aws/)\\n  [big data (41)](https://vitalflux.com/tag/big-data-2/)\\n  [blockchain (63)](https://vitalflux.com/tag/blockchain/)\\n  [career planning (21)](https://vitalflux.com/tag/career-planning/)\\n  [chatgpt (16)](https://vitalflux.com/tag/chatgpt/)\\n  [data (21)](https://vitalflux.com/tag/data/)\\n  [data analytics (36)](https://vitalflux.com/tag/data-analytics/)\\n  [datascience (33)](https://vitalflux.com/tag/datascience/)\\n  [Data Science (504)](https://vitalflux.com/tag/data-science/)\\n  [Deep Learning (60)](https://vitalflux.com/tag/deep-learning/)\\n  [docker (26)](https://vitalflux.com/tag/docker/)\\n  [freshers (14)](https://vitalflux.com/tag/freshers/)\\n  [generative ai (45)](https://vitalflux.com/tag/generative-ai/)\\n  [google (14)](https://vitalflux.com/tag/google/)\\n  [hyperledger (18)](https://vitalflux.com/tag/hyperledger/)\\n  [Interview questions (79)](https://vitalflux.com/tag/interview-questions/)\\n  [Java (92)](https://vitalflux.com/tag/java/)\\n  [javascript (103)](https://vitalflux.com/tag/javascript/)\\n  [Kubernetes (19)](https://vitalflux.com/tag/kubernetes/)\\n  [machine learning (500)](https://vitalflux.com/tag/machine-learning/)\\n  [mongodb (16)](https://vitalflux.com/tag/mongodb/)\\n  [news (16)](https://vitalflux.com/tag/news-2/)\\n  [nlp (46)](https://vitalflux.com/tag/nlp/)\\n  [nosql (17)](https://vitalflux.com/tag/nosql-2/)\\n  [online courses (13)](https://vitalflux.com/tag/online-courses/)\\n  [python (166)](https://vitalflux.com/tag/python/)\\n  [QA (12)](https://vitalflux.com/tag/qa/)\\n  [quantum computing (13)](https://vitalflux.com/tag/quantum-computing/)\\n  [reactjs (15)](https://vitalflux.com/tag/reactjs/)\\n  [r programming (13)](https://vitalflux.com/tag/r-programming/)\\n  [sklearn (30)](https://vitalflux.com/tag/sklearn/)\\n  [spring framework (16)](https://vitalflux.com/tag/spring-framework/)\\n  [statistics (85)](https://vitalflux.com/tag/statistics/)\\n  [testing (16)](https://vitalflux.com/tag/testing/)\\n  [tools (12)](https://vitalflux.com/tag/tools-2/)\\n  [tutorials (14)](https://vitalflux.com/tag/tutorials/)\\n  [UI (13)](https://vitalflux.com/tag/ui/)\\n  [Unit Testing (18)](https://vitalflux.com/tag/unit-testing/)\\n  [web (16)](https://vitalflux.com/tag/web/)\\n* ### Recent Comments\\n* 1. Justice on [Occam’s Razor in Machine Learning: Examples](https://vitalflux.com/occams-razor-in-machine-learning-examples/#comment-46344)March 21, 2024\\n\\n     I found it very helpful. However the differences are not too understandable for me\\n  2. AYUSH on [Why & When to use Eigenvalues & Eigenvectors?](https://vitalflux.com/why-when-use-eigenvalue-eigenvector/#comment-39281)February 20, 2024\\n\\n     Very Nice Explaination. Thankyiu very much,\\n  3. Muhammed Tmeizeh on [Hyperledger Fabric – Are Channels Private Blockchain? (Deep Dive)](https://vitalflux.com/hyperledger-fabric-channels-private-blockchain-deep-dive/#comment-38715)February 16, 2024\\n\\n     in your case E respresent Member or Oraganization which include on e or more peers?\\n  4. Ajay Salve on [ESG Concepts: Reports, Metrics & KPIs](https://vitalflux.com/esg-metrics-and-kpis-what-esg-team-needs-to-know/#comment-38263)February 10, 2024\\n\\n     Such a informative post. Keep it up\\n  5. Ashok Reddyboina on [Sklearn LabelEncoder Example – Single & Multiple Columns](https://vitalflux.com/labelencoder-example-single-multiple-columns/#comment-37514)February 8, 2024\\n\\n     Thank you....for your support. you given a good solution for me.\\n\\nWelcome to Vitalflux.com - your hub for AI, Machine Learning, Data Science and Data Analytics topics. Learn through detailed, real-life examples in AI/ML and Data Management. Gain practical insights and apply them to real-world scenarios!\\n[Data Science](https://vitalflux.com/category/data-science/)\\n\\n[Machine Learning](https://vitalflux.com/category/machine-learning)\\n\\n[Deep Learning](https://vitalflux.com/category/deep-learning)\\n\\n[Statistics](https://vitalflux.com/category/statistics)\\n\\n[Generative AI](https://vitalflux.com/category/generative-ai)\\n[Courses](https://vitalflux.com/category/career/online-courses/)\\n\\n[Admissions](https://vitalflux.com/category/career/admissions/)\\n\\n[Interview Questions](https://vitalflux.com/category/career/interview-questions/)\\n\\n[Educational Presentations](https://vitalflux.com/data-analytics-presentations-ppts-slides/)\\n[Privacy policy](https://vitalflux.com/privacy-policy)\\n\\n[Contact us](https://vitalflux.com/contact-us)\\n\\nAnalytics Yogi © 2025\\n\\nPowered by [WordPress](http://wordpress.org/).\\nDesign by [WildWebLab](http://wildweblab.com/)']\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tools"
      ],
      "metadata": {
        "id": "yJXO6NaNKVQk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f6f5ca5-7d10-4f71-83dd-d863d1f7365b"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[StructuredTool(name='multiply', description='use to multiply numbers', args_schema=<class '__main__.CalculatorInput'>, return_direct=True, func=<function multiply at 0x7b9c20f2efc0>),\n",
              " StructuredTool(name='search_web_extract_info', description='Search the web for a query and extracts useful information from the search links', args_schema=<class 'langchain_core.utils.pydantic.search_web_extract_info'>, func=<function search_web_extract_info at 0x7b9c0a3d3240>),\n",
              " StructuredTool(name='get_weather', description='Search weatherapi to get the current weather.', args_schema=<class 'langchain_core.utils.pydantic.get_weather'>, func=<function get_weather at 0x7b9c03fdc220>)]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tool calling for LLMs without native support for tool or function calling"
      ],
      "metadata": {
        "id": "_1Yc8d9MQDqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some models like ChatGPT have been fine-tuned for tool calling and provide a dedicated API for tool calling. Generally, such models are better at tool calling than non-fine-tuned models, and are recommended for use cases that require tool calling.\n",
        "\n",
        "Here we will explore an alternative method to invoke tools if you're using a model that does not natively support tool calling (even though we use ChatGPT here which supports it, we will assume it could be any LLM which doesn't support tool calling).\n",
        "\n",
        "We'll do this by simply writing a prompt that will get the model to invoke the appropriate tools."
      ],
      "metadata": {
        "id": "yIXJC1-9RXnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.tools import render_text_description\n",
        "\n",
        "rendered_tools = render_text_description(tools)\n",
        "print(rendered_tools)"
      ],
      "metadata": {
        "id": "xr2Wt-iuHEuw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f855c868-140a-4ad6-d107-92828004566b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multiply(a: float, b: float) -> float - use to multiply numbers\n",
            "search_web_extract_info(query: str) -> list - Search the web for a query and extracts useful information from the search links\n",
            "get_weather(query: str) -> list - Search weatherapi to get the current weather.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = f\"\"\"\\\n",
        "You are an assistant that has access to the following set of tools.\n",
        "Here are the names and descriptions for each tool:\n",
        "\n",
        "{rendered_tools}\n",
        "\n",
        "Given the user instructions, for each instruction do the following:\n",
        " - Return the name and input of the tool to use.\n",
        " - Return your response as a JSON blob with 'name' and 'arguments' keys.\n",
        " - The `arguments` should be a dictionary, with keys corresponding\n",
        "   to the argument names and the values corresponding to the requested values.\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"user\", \"{input}\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "sO8AT_uiK3zV"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instructions = [\n",
        "                  {\"input\" : \"What is 2.1 times 3.5\"},\n",
        "                  {\"input\" : \"What is the current weather in Greenland\"},\n",
        "                  {\"input\" : \"Tell me about the current state of Agentic AI in the industry\" }\n",
        "               ]"
      ],
      "metadata": {
        "id": "aNq3a5B9HEy-"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "chain = (prompt\n",
        "            |\n",
        "         chatgpt\n",
        "            |\n",
        "         JsonOutputParser())"
      ],
      "metadata": {
        "id": "GJWWhI0NHE3J"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "responses = chain.map().invoke(instructions)"
      ],
      "metadata": {
        "id": "a8O7pvViHE5l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a5c31a2-63d9-4637-e0d5-96c1257e1f8b"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<frozen importlib._bootstrap>:1047: ImportWarning: _PyDrive2ImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _GenerativeAIImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _OpenCVImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: APICoreClientInfoImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _BokehImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _AltairImportHook.find_spec() not found; falling back to find_module()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "responses"
      ],
      "metadata": {
        "id": "4F7Cqo6yMGzU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f1e4782-6b84-44fa-fe3f-25882d82fcd3"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'multiply', 'arguments': {'a': 2.1, 'b': 3.5}},\n",
              " {'name': 'get_weather', 'arguments': {'query': 'Greenland'}},\n",
              " {'name': 'search_web_extract_info',\n",
              "  'arguments': {'query': 'current state of Agentic AI in the industry 2023'}}]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "toolkit = {\n",
        "    \"multiply\": multiply,\n",
        "    \"search_web_extract_info\": search_web_extract_info,\n",
        "    \"get_weather\": get_weather\n",
        "}\n",
        "\n",
        "for tool_call in responses:\n",
        "    selected_tool = toolkit[tool_call[\"name\"].lower()]\n",
        "    print(f\"Calling tool: {tool_call['name']}\")\n",
        "    tool_output = selected_tool.invoke(tool_call[\"arguments\"])\n",
        "    print(tool_output)\n",
        "    print()"
      ],
      "metadata": {
        "id": "rLsWGvOkM5Qe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29df7cdd-dd9e-494a-c7a3-9b0b70bb4758"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling tool: multiply\n",
            "7.3500000000000005\n",
            "\n",
            "Calling tool: get_weather\n",
            "{'location': {'name': 'Nuuk', 'region': 'Vestgronland', 'country': 'Greenland', 'lat': 64.183, 'lon': -51.75, 'tz_id': 'America/Nuuk', 'localtime_epoch': 1737591345, 'localtime': '2025-01-22 22:15'}, 'current': {'last_updated_epoch': 1737591300, 'last_updated': '2025-01-22 22:15', 'temp_c': -6.8, 'temp_f': 19.8, 'is_day': 0, 'condition': {'text': 'Overcast', 'icon': '//cdn.weatherapi.com/weather/64x64/night/122.png', 'code': 1009}, 'wind_mph': 4.0, 'wind_kph': 6.5, 'wind_degree': 177, 'wind_dir': 'S', 'pressure_mb': 983.0, 'pressure_in': 29.03, 'precip_mm': 0.13, 'precip_in': 0.01, 'humidity': 79, 'cloud': 100, 'feelslike_c': -10.1, 'feelslike_f': 13.9, 'windchill_c': -11.5, 'windchill_f': 11.2, 'heatindex_c': -8.1, 'heatindex_f': 17.5, 'dewpoint_c': -9.0, 'dewpoint_f': 15.9, 'vis_km': 10.0, 'vis_miles': 6.0, 'uv': 0.0, 'gust_mph': 5.3, 'gust_kph': 8.5}}\n",
            "\n",
            "Calling tool: search_web_extract_info\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 1/5 [00:02<00:08,  2.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction blocked for url:  https://www.pwc.com/m1/en/publications/documents/2024/agentic-ai-the-new-frontier-in-genai-an-executive-playbook.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:04<00:00,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The state of AI in early 2024 | McKinsey\\n[Skip to main content](#skipToMain)\\n# The state of AI in early 2024: Gen AI adoption spikes and starts to generate value\\n\\nMay 30, 2024 | SurveyAs generative AI adoption accelerates, survey respondents report measurable benefits and increased mitigation of the risk of inaccuracy. A small group of high performers lead the way.\\n\\n###\\n\\n [(23 pages)](#/download/%2F~%2Fmedia%2Fmckinsey%2Fbusiness%20functions%2Fquantumblack%2Four%20insights%2Fthe%20state%20of%20ai%2F2024%2Fthe-state-of-ai-in-early-2024-final.pdf%3FshouldIndex%3Dfalse)\\n\\n**If 2023** was the year the world discovered [generative AI (gen AI)](/featured-insights/mckinsey-explainers/what-is-generative-ai), 2024 is the year organizations truly began using—and deriving business value from—this new technology. In the latest [McKinsey Global Survey](/featured-insights/mckinsey-global-surveys)\\xa0on AI, 65 percent of respondents report that their organizations are regularly using gen AI, nearly double the percentage from our previous survey just ten months ago. Respondents’ expectations for gen AI’s impact remain as high [as they were last year](/capabilities/quantumblack/our-insights/the-state-of-ai-in-2023-generative-ais-breakout-year), with three-quarters predicting that gen AI will lead to significant or disruptive change in their industries in the years ahead.\\n\\n## About the authors\\n\\nThis article is a collaborative effort by [Alex Singla](/our-people/alex-singla), [Alexander Sukharevsky](/our-people/alexander-sukharevsky), [Lareina Yee](/our-people/lareina-yee), and [Michael Chui](/our-people/michael-chui), with [Bryce Hall](/our-people/bryce-hall), representing views from QuantumBlack, AI by McKinsey, and McKinsey Digital.\\n\\nOrganizations are already seeing material benefits from gen AI use, reporting both cost decreases and revenue jumps in the business units deploying the technology. The survey also provides insights into the kinds of risks presented by gen AI—most notably, inaccuracy—as well as the emerging practices of top performers to mitigate those challenges and capture value.\\n\\n## AI adoption surges\\n\\nInterest in generative AI has also brightened the spotlight on a broader set of AI capabilities. For the past six years, AI adoption by respondents’ organizations has hovered at about 50 percent. This year, the survey finds that adoption has jumped to 72 percent (Exhibit 1). And the interest is truly global in scope. Our 2023 survey found that AI adoption did not reach 66 percent in *any* region; however, this year more than two-thirds of respondents in nearly *every* region say their organizations are using AI.1Organizations based in Central and South America are the exception, with 58 percent of respondents working for organizations based in Central and South America reporting AI adoption. Looking by industry, the biggest increase in adoption can be found in professional services.2Includes respondents working for organizations focused on human resources, legal services, management consulting, market research, R&D, tax preparation, and training.\\n\\n![Al adoption worldwide has increased dramatically in the past year, after years of little meaningful change.](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai/2024/stateofai-ex-1.svgz?cq=50&cpy=Center)\\n\\nAlso, responses suggest that companies are now using AI in more parts of the business. Half of respondents say their organizations have adopted AI in two or more business functions, up from less than a third of respondents in 2023 (Exhibit 2).\\n\\n![Survey findings suggest that organizations are using AI in more business functions now than in previous years.](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai/2024/stateofai-ex-2.svgz?cq=50&cpy=Center)\\n### Gen AI adoption is most common in the functions where it can create the most value\\n\\nMost respondents now report that their organizations—and they as individuals—are using gen AI. Sixty-five percent of respondents say their organizations are regularly using gen AI in at least one business function, up from one-third last year. The average organization using gen AI is doing so in two functions, most often in marketing and sales and in product and service development—two functions in which [previous research](/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier)\\xa0determined that gen AI adoption could generate the most value3“[The economic potential of generative AI: The next productivity frontier](/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier),” McKinsey, June 14, 2023.—as well as in IT (Exhibit 3). The biggest increase from 2023 is found in marketing and sales, where reported adoption has more than doubled. Yet across functions, only two use cases, both within marketing and sales, are reported by 15 percent or more of respondents.\\n\\n![Respondents most often report generative AI adoption in their marketing- and-sales, product- and service-development, and IT functions.](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai/2024/stateofai-ex-3.svgz?cq=50&cpy=Center)\\n\\nGen AI also is weaving its way into respondents’ personal lives. Compared with 2023, respondents are much more likely to be using gen AI at work and even more likely to be using gen AI both at work and in their personal lives (Exhibit 4). The survey finds upticks in gen AI use across all regions, with the largest increases in Asia–Pacific and Greater China. Respondents at the highest seniority levels, meanwhile, show larger jumps in the use of gen Al tools for work and outside of work compared with their midlevel-management peers. Looking at specific industries, respondents working in energy and materials and in professional services report the largest increase in gen AI use.\\n\\n### Investments in gen AI and analytical AI are beginning to create value\\n\\nThe latest survey also shows how different industries are budgeting for gen AI. Responses suggest that, in many industries, organizations are about equally as likely to be investing more than 5 percent of their digital budgets in gen AI as they are in nongenerative, analytical-AI solutions (Exhibit 5). Yet in most industries, larger shares of respondents report that their organizations spend more than 20 percent on analytical AI than on gen AI. Looking ahead, most respondents—67 percent—expect their organizations to invest more in AI over the next three years.\\n\\nWhere are those investments paying off? For the first time, our latest survey explored the value created by gen AI use by business function. The function in which the largest share of respondents report seeing cost decreases is human resources. Respondents most commonly report meaningful revenue increases (of more than 5 percent) in supply chain and inventory management (Exhibit 6). For analytical AI, respondents most often report seeing cost benefits in service operations—in line with what we found [last year](/capabilities/quantumblack/our-insights/the-state-of-ai-in-2023-generative-ais-breakout-year)—as well as meaningful revenue increases from AI use in marketing and sales.\\n\\n## Inaccuracy: The most recognized and experienced risk of gen AI use\\n\\nAs businesses begin to see the benefits of gen AI, they’re also recognizing the diverse risks associated with the technology. These can range from data management risks such as data privacy, bias, or intellectual property (IP) infringement to model management risks, which tend to focus on inaccurate output or lack of explainability. A third big risk category is security and incorrect use.\\n\\n##\\n\\nRespondents to the latest survey are more likely than they were last year to say their organizations consider inaccuracy and IP infringement to be relevant to their use of gen AI, and about half continue to view cybersecurity as a risk (Exhibit 7).\\n\\n##\\n\\nConversely, respondents are less likely than they were last year to say their organizations consider workforce and labor displacement to be relevant risks and are not increasing efforts to mitigate them.\\n\\n##\\n\\nIn fact, inaccuracy—[which can affect use cases across the gen AI value chain](/capabilities/risk-and-resilience/our-insights/implementing-generative-ai-with-speed-and-safety), ranging from customer journeys and summarization to coding and creative content—is the only risk that respondents are significantly more likely than last year to say their organizations are actively working to mitigate.\\n\\nExhibit 7 ![Inaccuracy and intellectual property infringement are increasingly considered relevant risks to organizations’ generative AI use. (2 of 4)](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai/2024/stateofai-ex-7b.svgz?cq=50&cpy=Center)![Inaccuracy and intellectual property infringement are increasingly considered relevant risks to organizations’ generative AI use. (3 of 4)](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai/2024/stateofai-ex-7c.svgz?cq=50&cpy=Center)![Inaccuracy and intellectual property infringement are increasingly considered relevant risks to organizations’ generative AI use. (4 of 4)](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai/2024/stateofai-ex-7d.svgz?cq=50&cpy=Center)\\n\\nSome organizations have already experienced negative consequences from the use of gen AI, with 44 percent of respondents saying their organizations have experienced at least one consequence (Exhibit 8). Respondents most often report inaccuracy as a risk that has affected their organizations, followed by cybersecurity and explainability.\\n\\n![Nearly one-quarter of respondents say their organizations have experienced negative consequences from generative AI’s inaccuracy.](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai/2024/stateofai-ex-8.svgz?cq=50&cpy=Center)\\n\\n[Our previous research](/capabilities/risk-and-resilience/our-insights/implementing-generative-ai-with-speed-and-safety) has found that there are several elements of governance that can help in scaling gen AI use responsibly, yet few respondents report having these risk-related practices in place.4“[Implementing generative AI with speed and safety](/capabilities/risk-and-resilience/our-insights/implementing-generative-ai-with-speed-and-safety),” *McKinsey Quarterly*, March 13, 2024. For example, just 18 percent say their organizations have an enterprise-wide council or board with the authority to make decisions involving responsible AI governance, and only one-third say gen AI risk awareness and risk mitigation controls are required skill sets for technical talent.\\n\\n## Bringing gen AI capabilities to bear\\n\\nThe latest survey also sought to understand how, and how quickly, organizations are deploying these new gen AI tools. We have found [three archetypes for implementing gen AI solutions](/capabilities/mckinsey-digital/our-insights/technologys-generational-moment-with-generative-ai-a-cio-and-cto-guide): *takers* use off-the-shelf, publicly available solutions; *shapers* customize those tools with proprietary data and systems; and *makers* develop their own foundation models from scratch.5“[Technology’s generational moment with generative AI: A CIO and CTO guide](/capabilities/mckinsey-digital/our-insights/technologys-generational-moment-with-generative-ai-a-cio-and-cto-guide),” McKinsey, July 11, 2023. Across most industries, the survey results suggest that organizations are finding off-the-shelf offerings applicable to their business needs—though many are pursuing opportunities to customize models or even develop their own (Exhibit 9). About half of reported gen AI uses within respondents’ business functions are utilizing off-the-shelf, publicly available models or tools, with little or no customization. Respondents in energy and materials, technology, and media and telecommunications are more likely to report significant customization or tuning of publicly available models or developing their own proprietary models to address specific business needs.\\n\\n![Organizations are pursuing a mix of off-the-shelf generative AI capabilities and also significantly customizing models or developing their own.](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai/2024/stateofai-ex-9.svgz?cq=50&cpy=Center)\\n\\nRespondents most often report that their organizations required one to four months from the start of a project to put gen AI into production, though the time it takes varies by business function (Exhibit 10). It also depends upon the approach for acquiring those capabilities. Not surprisingly, reported uses of highly customized or proprietary models are 1.5 times more likely than off-the-shelf, publicly available models to take five months or more to implement.\\n\\n![Business functions are most often able to put their generative AI capabilities to use within one to four months.](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai/2024/stateofai-ex-10.svgz?cq=50&cpy=Center)\\n\\n## Gen AI high performers are excelling despite facing challenges\\n\\nGen AI is a new technology, and organizations are still early in the journey of pursuing its opportunities and scaling it across functions. So it’s little surprise that only a small subset of respondents (46 out of 876) report that a meaningful share of their organizations’ EBIT can be attributed to their deployment of gen AI. Still, these gen AI leaders are worth examining closely. These, after all, are the early movers, who already attribute more than 10 percent of their organizations’ EBIT to their use of gen AI. Forty-two percent of these high performers say more than 20 percent of their EBIT is attributable to their use of nongenerative, analytical AI, and they span industries and regions—though most are at organizations with less than $1 billion in annual revenue. The AI-related practices at these organizations can offer guidance to those looking to create value from gen AI adoption at their own organizations.\\n\\nTo start, gen AI high performers are using gen AI in more business functions—an average of three functions, while others average two. They, like other organizations, are most likely to use gen AI in marketing and sales and product or service development, but they’re much more likely than others to use gen AI solutions in risk, legal, and compliance; in strategy and corporate finance; and in supply chain and inventory management. They’re more than three times as likely as others to be using gen AI in activities ranging from processing of accounting documents and risk assessment to R&D testing and pricing and promotions. While, overall, about half of reported gen AI applications within business functions are utilizing publicly available models or tools, gen AI high performers are less likely to use those off-the-shelf options than to either implement significantly customized versions of those tools or to develop their own proprietary foundation models.\\n\\nWhat else are these high performers doing differently? For one thing, they are paying more attention to gen-AI-related risks. Perhaps because they are further along on their journeys, they are more likely than others to say their organizations have experienced every negative consequence from gen AI we asked about, from cybersecurity and personal privacy to explainability and IP infringement. Given that, they are more likely than others to report that their organizations consider those risks, as well as regulatory compliance, environmental impacts, and political stability, to be relevant to their gen AI use, and they say they take steps to mitigate more risks than others do.\\n\\nGen AI high performers are also much more likely to say their organizations follow a set of risk-related best practices (Exhibit 11). For example, they are nearly twice as likely as others to involve the legal function and embed risk reviews early on in the development of gen AI solutions—that is, to “[shift left](/about-us/new-at-mckinsey-blog/an-inside-look-at-how-businesses-are-or-not-managing-ai-risk).” They’re also much more likely than others to employ a wide range of other best practices, from strategy-related practices to those related to scaling.\\n\\nIn addition to experiencing the risks of gen AI adoption, high performers have encountered other challenges that can serve as warnings to others (Exhibit 12). Seventy percent say they have experienced difficulties with data, including defining processes for data governance, developing the ability to quickly integrate data into AI models, and an insufficient amount of training data, highlighting the essential role that data play in capturing value. High performers are also more likely than others to report experiencing challenges with their operating models, such as implementing agile ways of working and effective sprint performance management.\\n\\n![Generative AI high performers report experiencing a range of challenges in capturing value from the technology.](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai/2024/stateofai-ex-12.svgz?cq=50&cpy=Center)\\n\\n## About the research\\n\\nThe online survey was in the field from February 22 to March 5, 2024, and garnered responses from 1,363 participants representing the full range of regions, industries, company sizes, functional specialties, and tenures. Of those respondents, 981 said their organizations had adopted AI in at least one business function, and 878 said their organizations were regularly using gen AI in at least one function. To adjust for differences in response rates, the data are weighted by the contribution of each respondent’s nation to global GDP.\\n\\n#####\\n\\n**[Alex Singla](/our-people/alex-singla)** and **[Alexander Sukharevsky](/our-people/alexander-sukharevsky)**\\xa0are global coleaders of QuantumBlack, AI by McKinsey, and senior partners in McKinsey’s Chicago and London offices, respectively; **[Lareina Yee](/our-people/lareina-yee)**\\xa0is a senior partner in the Bay Area office, where **Michael Chui,** a McKinsey Global Institute partner, is a partner; and **[Bryce Hall](/our-people/bryce-hall)**\\xa0is an associate partner in the Washington, DC, office.\\n\\nThey wish to thank Kaitlin Noe, Larry Kanter, Mallika Jhamb, and Shinjini Srivastava for their contributions to this work.\\n\\n---\\n\\nThis article was edited by Heather Hanselman, a senior editor in McKinsey’s Atlanta office.\\n\\n##### Explore a career with us\\n\\n[Search Openings](/careers/search-jobs)\\n##### Related Articles\\n\\n[![One large blue ball in mid air above many smaller blue, green, purple and white balls](/~/media/mckinsey/business%20functions/mckinsey%20digital/our%20insights/moving%20past%20gen%20ais%20honeymoon%20phase%20seven%20hard%20truths%20for%20cios/thumb-gettyimages-1551147077.jpg?cq=50&mw=767&car=16:9&cpy=Center)](/capabilities/mckinsey-digital/our-insights/moving-past-gen-ais-honeymoon-phase-seven-hard-truths-for-cios-to-get-from-pilot-to-scale)Article\\n###### [Moving past gen AI’s honeymoon phase: Seven hard truths for CIOs to get from pilot to scale](/capabilities/mckinsey-digital/our-insights/moving-past-gen-ais-honeymoon-phase-seven-hard-truths-for-cios-to-get-from-pilot-to-scale)\\n\\n[![A thumb and an index finger form a circular void, resembling the shape of a light bulb but without the glass component. Inside this empty space, a bright filament and the gleaming metal base of the light bulb are visible.](/~/media/mckinsey/business%20functions/mckinsey%20digital/our%20insights/a%20generative%20ai%20reset%20rewiring%20to%20turn%20potential%20into%20value%20in%202024/qweb_gen-ai-reset_1536x1536.jpg?cq=50&mw=767&car=16:9&cpy=Center)](/capabilities/mckinsey-digital/our-insights/a-generative-ai-reset-rewiring-to-turn-potential-into-value-in-2024)Article - *McKinsey Quarterly*\\n###### [A generative AI reset: Rewiring to turn potential into value in 2024](/capabilities/mckinsey-digital/our-insights/a-generative-ai-reset-rewiring-to-turn-potential-into-value-in-2024)\\n\\n[![High-tech bees buzz with purpose, meticulously arranging digital hexagonal cylinders into a precisely stacked formation.](/~/media/mckinsey/business%20functions/risk/our%20insights/implementing%20generative%20ai%20with%20speed%20and%20safety/qweb-implementing-generative-ai-with-speed-and-safety-1536x1536.jpg?cq=50&mw=767&car=16:9&cpy=Center)](/capabilities/risk-and-resilience/our-insights/implementing-generative-ai-with-speed-and-safety)Article - *McKinsey Quarterly*\\n###### [Implementing generative AI with speed and safety](/capabilities/risk-and-resilience/our-insights/implementing-generative-ai-with-speed-and-safety)', 'The state of AI in 2023: Generative AI’s breakout year | McKinsey\\n[Skip to main content](#skipToMain)\\n# The state of AI in 2023: Generative AI’s breakout year\\n\\nAugust 1, 2023 | SurveyAs organizations rapidly deploy generative AI tools, survey respondents expect significant effects on their industries and workforces.\\n#### You have reached a page with older survey data. Please see our 2024 survey results [here](/capabilities/quantumblack/our-insights/the-state-of-ai).\\n\\n###\\n\\n [(24 pages)](#/download/%2F~%2Fmedia%2Fmckinsey%2Fbusiness%20functions%2Fquantumblack%2Four%20insights%2Fthe%20state%20of%20ai%20in%202023%20generative%20ais%20breakout%20year%2Fthe-state-of-ai-in-2023-generative-ais-breakout-year-v3.pdf%3FshouldIndex%3Dfalse)\\n\\n**The latest annual [McKinsey Global Survey](/featured-insights/mckinsey-global-surveys)**\\xa0on the current state of AI confirms the explosive growth of [generative AI (gen AI) tools](/featured-insights/mckinsey-explainers/what-is-generative-ai). Less than a year after many of these tools debuted, one-third of our survey respondents say their organizations are using gen AI regularly in at least one business function. Amid recent advances, AI has risen from a topic relegated to tech employees to a focus of company leaders: nearly one-quarter of surveyed C-suite executives say they are personally using\\ngen AI tools for work, and more than one-quarter of respondents from companies using AI say gen AI is already on their boards’ agendas. What’s more, 40 percent of respondents say their organizations will increase their investment in AI overall because of advances in gen AI. The findings show that these are still early days for managing gen AI–related risks, with less than half of respondents saying their organizations are mitigating even the risk they consider most relevant: inaccuracy.\\n\\nThe organizations that have already embedded AI capabilities have been the first to explore gen AI’s potential, and those seeing the most value from more traditional AI capabilities—a group we call AI high performers—are already outpacing others in their [adoption of gen AI](/capabilities/people-and-organizational-performance/our-insights/gen-ais-next-inflection-point-from-employee-experimentation-to-organizational-transformation)\\xa0tools.1We define AI high performers as organizations that, according to respondents, attribute at least 20 percent of their EBIT to AI adoption.\\n\\nThe expected business disruption from gen AI is significant, and respondents predict meaningful changes to their workforces. They anticipate workforce cuts in certain areas and large reskilling efforts to address shifting talent needs. Yet while the use of gen AI might spur the adoption of other AI tools, we see few meaningful increases in organizations’ adoption of these technologies. The percent of organizations adopting any AI tools has held steady since 2022, and adoption remains concentrated within a small number of business functions.\\n\\n### Table of Contents\\n\\n1. [It’s early days still, but use of gen AI is already widespread](#widespread)\\n2. [Leading companies are already ahead with gen AI](#leading)\\n3. [AI-related talent needs shift, and AI’s workforce effects are expected to be substantial](#talent)\\n4. [With all eyes on gen AI, AI adoption and impact remain steady](#steady)\\n5. [About the research](#research)\\n\\n## 1. It’s early days still, but use of gen AI is already widespread\\n\\nThe findings from the survey—which was in the field in mid-April 2023—show that, despite gen AI’s nascent public availability, experimentation with [the tools](/capabilities/quantumblack/our-insights/exploring-opportunities-in-the-generative-ai-value-chain)\\xa0is already relatively common, and respondents expect the new capabilities to transform their industries. Gen AI has captured interest across the business population: individuals across regions, industries, and seniority levels are using gen AI for work and outside of work. Seventy-nine percent of all respondents say they’ve had at least some exposure to gen AI, either for work or outside of work, and 22\\xa0percent say they are regularly using it in their own work. While reported use is quite similar across seniority levels, it is highest among respondents working in the technology sector and those in North America.\\n\\nInteractive\\n\\nOrganizations, too, are now commonly using gen AI. One-third of all respondents say their organizations are already regularly using generative AI in at least one function—meaning that 60\\xa0percent of organizations with reported AI adoption are using gen AI. What’s more, 40 percent of those reporting AI adoption at their organizations say their companies expect to invest more in AI overall thanks to generative AI, and 28 percent say generative AI use is already on their board’s agenda. The most commonly reported business functions using these newer tools are the same as those in which AI use is most common overall: marketing and sales, product and service development, and service operations, such as customer care and back-office support. This suggests that organizations are pursuing these new tools where the most value is. [In our previous research](/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier), these three areas, along with software engineering, showed the potential to deliver about 75 percent of the total annual value from generative AI use cases.\\n\\n![The most commonly reported uses of generative AI tools are in marketing and sales, product and service development, and service operations.](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai%20in%202023%20generative%20ais%20breakout%20year/svgz_stateofai2023_ex2.svgz?cq=50&cpy=Center)\\n\\nIn these early days, expectations for [gen AI’s impact are high](/mgi/our-research/generative-ai-how-will-it-affect-future-jobs-and-workflows): three-quarters of all respondents expect gen AI to cause significant or disruptive change in the nature of their industry’s competition in the next three years. Survey respondents working in the technology and financial-services industries are the most likely to expect disruptive change from gen AI. [Our previous research shows](/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier)\\xa0that, while all industries are indeed likely to see some degree of disruption, the level of impact is likely to vary.2“[The economic potential of generative AI: The next productivity frontier](/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier),” McKinsey, June 14, 2023. Industries relying most heavily on knowledge work are likely to see more disruption—and potentially reap more value. While our estimates suggest that tech companies, unsurprisingly, are poised to see the highest impact from gen AI—adding value equivalent to as much as 9 percent of global industry revenue—knowledge-based industries such as banking (up to 5 percent), pharmaceuticals and medical products (also up to 5 percent), and education (up to 4 percent) could experience significant effects as well. By contrast, manufacturing-based industries, such as aerospace, automotives, and advanced electronics, could experience less disruptive effects. This stands in contrast to the impact of previous technology waves that affected manufacturing the most and is due to gen AI’s strengths in language-based activities, as opposed to those requiring physical labor.\\n\\n### Responses show many organizations not yet addressing potential risks from gen AI\\n\\nAccording to the survey, few companies seem fully prepared for the widespread use of gen AI—or the business risks these tools may bring. Just 21 percent of respondents reporting AI adoption say their organizations have established policies governing employees’ use of gen AI technologies in their work. And when we asked specifically about the risks of adopting gen AI, few respondents say their companies are mitigating the most commonly cited risk with gen AI: inaccuracy. Respondents cite inaccuracy more frequently than both cybersecurity and regulatory compliance, which were the most common risks from AI overall in previous surveys. Just 32 percent say they’re mitigating inaccuracy, a smaller percentage than the 38\\xa0percent who say they mitigate cybersecurity risks. Interestingly, this figure is significantly lower than the percentage of respondents who reported mitigating AI-related cybersecurity last year (51 percent). Overall, much as we’ve seen in previous years, most respondents say their organizations are not addressing AI-related risks.\\n\\n![Inaccuracy, cybersecurity, and intellectual-property infringement are the most-cited risks of generative AI adoption.](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai%20in%202023%20generative%20ais%20breakout%20year/svgz_stateofai2023_ex3.svgz?cq=50&cpy=Center)\\n\\n## 2. Leading companies are already ahead with gen AI\\n\\nThe survey results show that AI high performers—that is, organizations where respondents say at least 20 percent of EBIT in 2022 was attributable to AI use—are going all in on artificial intelligence, both with gen AI and more traditional AI capabilities. These organizations that achieve significant value from AI are already using gen AI in more business functions than other organizations do, especially in product and service development and risk and supply chain management. When looking at all AI capabilities—including more traditional machine learning capabilities, robotic process automation, and chatbots—AI high performers also are much more likely than others to use AI in product and service development, for uses such as product-development-cycle optimization, adding new features to existing products, and creating new AI-based products. These organizations also are using AI more often than other organizations in risk modeling and for uses within HR such as performance management and organization design and workforce deployment optimization.\\n\\n> AI high performers are much more likely than others to use AI in product and service development.\\n\\nAnother difference from their peers: high performers’ gen AI efforts are less oriented toward cost reduction, which is a top priority at other organizations. Respondents from AI high performers are twice as likely as others to say their organizations’ top objective for gen AI is to create entirely new businesses or sources of revenue—and they’re *most* likely to cite the increase in the value of existing offerings through new AI-based features.\\n\\n![Smaller shares of AI high performers see cost reductions as their top objective for generative AI efforts. ](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai%20in%202023%20generative%20ais%20breakout%20year/svgz_stateofai2023_ex4.svgz?cq=50&cpy=Center)\\n\\nAs we’ve seen [in previous years](/capabilities/quantumblack/our-insights/the-state-of-ai-in-2022-and-a-half-decade-in-review), these high-performing organizations invest much more than others in AI: respondents from AI high performers are more than five times more likely than others to say they spend more than 20 percent of their digital budgets on AI. They also use AI capabilities more broadly throughout the organization. Respondents from high performers are much more likely than others to say that their organizations have adopted AI in four or more business functions and that they have embedded a higher number of AI capabilities. For example, respondents from high performers more often report embedding knowledge graphs in at least one product or business function process, in addition to gen AI and related natural-language capabilities.\\n\\nWhile AI high performers are not immune to the challenges of capturing value from AI, the results suggest that the difficulties they face reflect their relative AI maturity, while others struggle with the more foundational, strategic elements of AI adoption. Respondents at AI high performers most often point to models and tools, such as monitoring model performance in production and retraining models as needed over time, as their top challenge. By comparison, other respondents cite strategy issues, such as setting a clearly defined AI vision that is linked with business value or finding sufficient resources.\\n\\n![Models and tools pose the biggest AI-related challenge for high performers, while strategy is a common stumbling block for others.](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai%20in%202023%20generative%20ais%20breakout%20year/svgz_stateofai2023_ex5.svgz?cq=50&cpy=Center)\\n\\nThe findings offer further evidence that even high performers haven’t mastered best practices regarding AI adoption, such as machine-learning-operations (MLOps) approaches, though they are much more likely than others to do so. For example, just 35 percent of respondents at AI high performers report that where possible, their organizations assemble existing components, rather than reinvent them, but that’s a much larger share than the 19 percent of respondents from other organizations who report that practice.\\n\\nMany [specialized MLOps technologies and practices](/capabilities/quantumblack/our-insights/exploring-opportunities-in-the-generative-ai-value-chain)\\xa0may be needed to adopt some of the more transformative uses cases that gen AI applications can deliver—and do so as safely as possible. Live-model operations is one such area, where monitoring systems and setting up instant alerts to enable rapid issue resolution can keep gen AI systems in check. High performers stand out in this respect but have room to grow: one-quarter of respondents from these organizations say their entire system is monitored and equipped with instant alerts, compared with just 12 percent of other respondents.\\n\\n## 3. AI-related talent needs shift, and AI’s workforce effects are expected to be substantial\\n\\nOur latest survey results show changes in the roles that organizations are filling to support their AI ambitions. In the past year, organizations using AI most often hired data engineers, machine learning engineers, and Al data scientists—all roles that respondents commonly reported hiring in the previous survey. But a much smaller share of respondents report hiring AI-related-software engineers—the most-hired role last year—than in the previous survey (28 percent in the latest survey, down from 39 percent). Roles in prompt engineering have recently emerged, as the need for that skill set rises alongside gen AI adoption, with 7 percent of respondents whose organizations have adopted AI reporting those hires in the past year.\\n\\n**The findings suggest that hiring for AI-related roles remains a challenge** but has become somewhat easier over the past year, which could reflect the spate of layoffs at technology companies from late 2022 through the first half of 2023. Smaller shares of respondents than in the previous survey report difficulty hiring for roles such as AI data scientists, data engineers, and data-visualization specialists, though responses suggest that hiring machine learning engineers and AI product owners remains as much of a challenge as in the previous year.\\n\\n![Hiring for AI-related roles remains a challenge, though reported difficulty has decreased since 2022 for many roles.](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai%20in%202023%20generative%20ais%20breakout%20year/svgz_stateofai2023_ex6.svgz?cq=50&cpy=Center)\\n\\n**Looking ahead to the next three years, respondents predict that the adoption of AI will reshape many roles** in the workforce. Generally, they expect more employees to be reskilled than to be separated. Nearly four in ten respondents reporting AI adoption expect more than 20 percent of their companies’ workforces will be reskilled, whereas 8 percent of respondents say the size of their workforces will decrease by more than 20 percent.\\n\\n![Survey respondents expect AI to meaningfully change their organizations’ workforces.](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai%20in%202023%20generative%20ais%20breakout%20year/svgz_stateofai2023_ex7.svgz?cq=50&cpy=Center)\\n\\n**Looking specifically at gen AI’s predicted impact,** service operations is the only function in which most respondents expect to see a decrease in workforce size at their organizations. This finding generally aligns with what [our recent research](/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier)\\xa0suggests: while the emergence of gen AI increased our estimate of the percentage of worker activities that could be automated (60 to 70\\xa0percent, up from 50 percent), this doesn’t necessarily translate into the automation of an entire role.\\n\\n![Service operations is the only function in which most respondents expect to see a decrease in workforce size because of generative AI.](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai%20in%202023%20generative%20ais%20breakout%20year/svgz_stateofai2023_ex8.svgz?cq=50&cpy=Center)\\n\\n**AI high performers are expected to conduct much higher levels of reskilling** than other companies are. Respondents at these organizations are over three times more likely than others to say their organizations will reskill more than 30 percent of their workforces over the next three years as a result of AI adoption.\\n\\n![Respondents at AI high performers expect their organizations to reskill larger portions of the workforce than other respondents do.](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai%20in%202023%20generative%20ais%20breakout%20year/svgz_stateofai2023_ex9.svgz?cq=50&cpy=Center)\\n\\n## 4. With all eyes on gen AI, AI adoption and impact remain steady\\n\\nWhile the use of gen AI tools is spreading rapidly, the survey data doesn’t show that these newer tools are propelling organizations’ overall AI adoption. The share of organizations that have adopted AI overall remains steady, at least for the moment, with 55 percent of respondents reporting that their organizations have adopted AI. Less than a third of respondents continue to say that their organizations have adopted AI in more than one business function, suggesting that AI use remains limited in scope. Product and service development and service operations continue to be the two business functions in which respondents most often report AI adoption, as was true in the previous four surveys. And overall, just 23 percent of respondents say at least 5 percent of their organizations’ EBIT last year was attributable to their use of AI—essentially flat with the previous survey—suggesting there is much more room to capture value.\\n\\n![Less than one-third of respondents say their organizations use AI in more than one function—a share largely unchanged since 2021.](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai%20in%202023%20generative%20ais%20breakout%20year/svgz_stateofai2023_ex10.svgz?cq=50&cpy=Center)\\n\\nOrganizations continue to see returns in the business areas in which they are using AI, and\\nthey plan to increase investment in the years ahead. We see a majority of respondents reporting AI-related revenue increases within each business function using AI. And looking ahead, more than two-thirds expect their organizations to increase their AI investment over the next three years.\\n\\n![Organizations continue to see benefits from AI adoption in the functions using AI capabilities.](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai%20in%202023%20generative%20ais%20breakout%20year/svgz_stateofai2023_ex11.svgz?cq=50&cpy=Center)\\n\\n## About the research\\n\\nThe online survey was in the field April 11 to 21, 2023, and garnered responses from 1,684 participants representing the full range of regions, industries, company sizes, functional specialties, and tenures. Of those respondents, 913 said their organizations had adopted AI in at least one function and were asked questions about their organizations’ AI use. To adjust for differences in response rates, the data are weighted by the contribution of each respondent’s nation to global GDP.\\n\\n#####\\n\\nThe survey content and analysis were developed by **Michael Chui**, a partner at the McKinsey Global Institute and a partner in McKinsey’s Bay Area office, where **[Lareina Yee](/our-people/lareina-yee)** is a senior partner; **[Bryce Hall](/our-people/bryce-hall)**, an associate partner in the Washington, DC, office; and senior partners **[Alex Singla](/our-people/alex-singla)** and **[Alexander Sukharevsky](/our-people/alexander-sukharevsky)**, global leaders of QuantumBlack, AI by McKinsey, based in the Chicago and London offices, respectively.\\n\\nThey wish to thank Shivani Gupta, Abhisek Jena, Begum Ortaoglu, Barr Seitz, and Li Zhang for their contributions to this work.\\n\\n---\\n\\nThis article was edited by Heather Hanselman, an editor in the Atlanta office.\\n\\n##### Explore a career with us\\n\\n[Search Openings](/careers/search-jobs)\\n##### Related Articles\\n\\n[![The economic potential of generative AI: The next productivity frontier](/~/media/mckinsey/featured%20insights/mckinsey%20live/new/7-jun-2023.png?cq=50&mw=767&car=16:9&cpy=Center)](/featured-insights/mckinsey-live/webinars/the-economic-potential-of-generative-ai-the-next-productivity-frontier)Webcast\\n###### [The economic potential of generative AI: The next productivity frontier](/featured-insights/mckinsey-live/webinars/the-economic-potential-of-generative-ai-the-next-productivity-frontier)\\n\\n[![A green apple split into 3 parts on a gray background. Half of the apple is made out of a digital blue wireframe mesh. ](/~/media/mckinsey/featured%20insights/mckinsey%20explainers/what%20is%20generative%20ai/generative-ai-1219474321-thumb-1536x1536-v3.jpg?cq=50&mw=767&car=16:9&cpy=Center)](/featured-insights/mckinsey-explainers/what-is-generative-ai)Article\\n###### [What is generative AI?](/featured-insights/mckinsey-explainers/what-is-generative-ai)\\n\\n[![Circular hub element virtual reality of big data, technology concept.](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/exploring%20opportunities%20in%20the%20generative%20ai%20value%20chain/thumb-gettyimages-1470671176.jpg?cq=50&mw=767&car=16:9&cpy=Center)](/capabilities/quantumblack/our-insights/exploring-opportunities-in-the-generative-ai-value-chain)Article\\n###### [Exploring opportunities in the generative AI value chain](/capabilities/quantumblack/our-insights/exploring-opportunities-in-the-generative-ai-value-chain)', 'Agentic AI – the new frontier in GenAI\\nSkip to content\\n[Skip to footer](#pgFooter)\\n\\n![Site Search](/etc.clientlibs/pwc/clientlibs/rebrand-clientlibs/components-colors/resources/images/slim-header-v2/search-icon.svg)\\n\\nTransforming our Region\\nIndustries\\nServices\\nInsights & Media\\nCareers\\nAbout us\\n\\nMore\\n\\nSearch\\n\\nMenu\\n\\nTransforming our Region\\n\\n[Transforming our Region](https://www.pwc.com/m1/en/publications.html)\\n\\n[Transforming our Region](https://www.pwc.com/m1/en/publications.html)\\n[Sustainability](https://www.pwc.com/m1/en/sustainability.html)\\n[Technology Consulting](https://www.pwc.com/m1/en/services/consulting/technology.html)\\n[Managed Services](https://www.pwc.com/m1/en/services/managed-services.html)\\n[Client stories](https://www.pwc.com/m1/en/about-us/client-stories.html)\\n[Business model reinvention](https://www.pwc.com/m1/en/services/transformation/business-model-reinvention.html)\\n\\nMenu\\n\\nTransforming our Region\\n\\n[Transforming our Region](https://www.pwc.com/m1/en/publications.html)\\n\\n[Middle East Economy Watch](https://www.pwc.com/m1/en/publications/middle-east-economy-watch.html)\\n[Transforming our region: webcast series](https://www.pwc.com/m1/en/transforming-our-region-middle-east-updates-webcast-series.html)\\n[Middle East - How to do Business Guides](https://www.pwc.com/m1/en/services/tax/legal-services/pathfinder-doing-business-in-the-middle-east.html)\\n\\nMenu\\n\\nTransforming our Region\\n\\n[Sustainability](https://www.pwc.com/m1/en/sustainability.html)\\n\\nMenu\\n\\nTransforming our Region\\n\\n[Technology Consulting](https://www.pwc.com/m1/en/services/consulting/technology.html)\\n\\n[GenAI](https://www.pwc.com/m1/en/services/consulting/technology/generative-ai.html)\\n[Cloud](https://www.pwc.com/m1/en/services/consulting/technology/cloud-transformation.html)\\n[Cybersecurity and Digital Trust](https://www.pwc.com/m1/en/services/consulting/technology/cyber-security.html)\\n[Data and analytics](https://www.pwc.com/m1/en/services/consulting/technology/data-and-analytics.html)\\n[Emerging Technology](https://www.pwc.com/m1/en/services/consulting/technology/emerging-technology.html)\\n[Enterprise Solutions](https://www.pwc.com/m1/en/services/consulting/technology/enterprise-solutions.html)\\n[Artificial Intelligence](https://www.pwc.com/m1/en/services/consulting/technology/artificial-intelligence-everywhere.html)\\n[Business applications](https://www.pwc.com/m1/en/services/consulting/technology/alliances.html)\\n[Metaverse](https://www.pwc.com/gx/en/issues/technology/metaverse.html)\\n[Public Safety](https://www.pwc.com/m1/en/services/consulting/technology/public-safety.html)\\n[Technology Strategy](https://www.strategyand.pwc.com/m1/en/strategic-foresight/sector-strategies/technology.html)\\n[Experience Consulting](https://www.pwc.com/m1/en/services/consulting/technology/experience-consulting.html)\\n\\nMenu\\n\\nTransforming our Region\\n\\n[Managed Services](https://www.pwc.com/m1/en/services/managed-services.html)\\n\\nMenu\\n\\nTransforming our Region\\n\\n[Client stories](https://www.pwc.com/m1/en/about-us/client-stories.html)\\n\\nMenu\\n\\nTransforming our Region\\n\\n[Business model reinvention](https://www.pwc.com/m1/en/services/transformation/business-model-reinvention.html)\\n\\nFeatured\\n\\n[![](/m1/en/the-new-equation/trust-you-have-to-earn-it.jpg.pwcimage.150.100.jpg)\\n\\nThe New Equation](https://www.pwc.com/m1/en/the-new-equation.html)\\n\\n[![](/m1/en/publications/images-new/geospatial-hero.jpg.pwcimage.150.100.jpg)\\n\\nTech powered](https://www.pwc.com/m1/en/services/consulting/technology/tech-perspectives-series.html)\\n\\nMenu\\n\\nIndustries\\n\\n[Industries](https://www.pwc.com/m1/en/industries.html)\\n\\n[Consumer Markets](https://www.pwc.com/m1/en/industries/consumer-markets.html)\\n[Energy, Utilities & Resources](https://www.pwc.com/m1/en/industries/energy-utilities-resources.html)\\n[Financial Services](https://www.pwc.com/m1/en/industries/financial-services.html)\\n[Government & Public Services](https://www.pwc.com/m1/en/industries/government-public-services.html)\\n[Industrial Manufacturing](https://www.pwc.com/m1/en/industries/manufacturing.html)\\n[Transport & Logistics](https://www.pwc.com/m1/en/industries/transportation-and-logistics.html)\\n[Health Industries](https://www.pwc.com/m1/en/industries/healthcare.html)\\n[Real Estate](https://www.pwc.com/m1/en/industries/investment-management-real-estate.html)\\n[Sovereign Investment Funds](https://www.pwc.com/m1/en/industries/sovereign-wealth-investment-funds.html)\\n[Tourism & Hospitality](https://www.pwc.com/m1/en/industries/tourism-global-center-of-excellence.html)\\n[Entrepreneurial & Private Business](https://www.pwc.com/m1/en/services/private-business.html)\\n[Education and skills](https://www.pwc.com/m1/en/industries/education.html)\\n\\nMenu\\n\\nIndustries\\n\\n[Consumer Markets](https://www.pwc.com/m1/en/industries/consumer-markets.html)\\n\\nMenu\\n\\nIndustries\\n\\n[Energy, Utilities & Resources](https://www.pwc.com/m1/en/industries/energy-utilities-resources.html)\\n\\nMenu\\n\\nIndustries\\n\\n[Financial Services](https://www.pwc.com/m1/en/industries/financial-services.html)\\n\\nMenu\\n\\nIndustries\\n\\n[Government & Public Services](https://www.pwc.com/m1/en/industries/government-public-services.html)\\n\\nMenu\\n\\nIndustries\\n\\n[Industrial Manufacturing](https://www.pwc.com/m1/en/industries/manufacturing.html)\\n\\nMenu\\n\\nIndustries\\n\\n[Transport & Logistics](https://www.pwc.com/m1/en/industries/transportation-and-logistics.html)\\n\\nMenu\\n\\nIndustries\\n\\n[Health Industries](https://www.pwc.com/m1/en/industries/healthcare.html)\\n\\nMenu\\n\\nIndustries\\n\\n[Real Estate](https://www.pwc.com/m1/en/industries/investment-management-real-estate.html)\\n\\nMenu\\n\\nIndustries\\n\\n[Sovereign Investment Funds](https://www.pwc.com/m1/en/industries/sovereign-wealth-investment-funds.html)\\n\\nMenu\\n\\nIndustries\\n\\n[Tourism & Hospitality](https://www.pwc.com/m1/en/industries/tourism-global-center-of-excellence.html)\\n\\nMenu\\n\\nIndustries\\n\\n[Entrepreneurial & Private Business](https://www.pwc.com/m1/en/services/private-business.html)\\n\\nMenu\\n\\nIndustries\\n\\n[Education and skills](https://www.pwc.com/m1/en/industries/education.html)\\n\\nFeatured\\n\\n[![](/gx/en/brand-simplified/hero-images-1600/transportation/damil-gettyimages-929485798-1600.jpg.pwcimage.150.100.jpg)\\n\\nSupply Chain & Spend Efficiency](https://www.pwc.com/m1/en/industries/supply-chain-spend-efficiency.html)\\n\\nMenu\\n\\nServices\\n\\n[Services](https://www.pwc.com/m1/en/services.html)\\n\\n[Audit and Assurance](https://www.pwc.com/m1/en/services/assurance.html)\\n[Consulting](https://www.pwc.com/m1/en/services/consulting.html)\\n[Deals](https://www.pwc.com/m1/en/services/deals.html)\\n[Tax & Legal](https://www.pwc.com/m1/en/services/tax.html)\\n[Strategy&](https://www.strategyand.pwc.com/m1/en.html)\\n[Startups & Scaleups](https://www.pwc.com/m1/en/services/startups-scaleups.html)\\n[Transformation](https://www.pwc.com/m1/en/services/transformation.html)\\n[People & Organisation](https://www.pwc.com/m1/en/services/people-and-organisation.html)\\n[PwC Academy](https://www.pwcacademy-me.com/)\\n\\nMenu\\n\\nServices\\n\\n[Audit and Assurance](https://www.pwc.com/m1/en/services/assurance.html)\\n\\nMenu\\n\\nServices\\n\\n[Consulting](https://www.pwc.com/m1/en/services/consulting.html)\\n\\nMenu\\n\\nServices\\n\\n[Deals](https://www.pwc.com/m1/en/services/deals.html)\\n\\nMenu\\n\\nServices\\n\\n[Tax & Legal](https://www.pwc.com/m1/en/services/tax.html)\\n\\nMenu\\n\\nServices\\n\\n[Strategy&](https://www.strategyand.pwc.com/m1/en.html)\\n\\nMenu\\n\\nServices\\n\\n[Startups & Scaleups](https://www.pwc.com/m1/en/services/startups-scaleups.html)\\n\\nMenu\\n\\nServices\\n\\n[Transformation](https://www.pwc.com/m1/en/services/transformation.html)\\n\\nMenu\\n\\nServices\\n\\n[People & Organisation](https://www.pwc.com/m1/en/services/people-and-organisation.html)\\n\\nMenu\\n\\nServices\\n\\n[PwC Academy](https://www.pwcacademy-me.com/)\\n\\nFeatured\\n\\n[![](/m1/en/ceosurvey/2025/images/ceo-survey-banner.png.pwcimage.150.100.jpg)\\n\\n28th CEO Survey](https://www.pwc.com/m1/en/ceo-survey/28th-ceo-survey-middle-east-findings-2025.html)\\n\\n[![](/m1/en/services/assurance/perspectives-in-risk-series/images/perspectives-in-risk-series-thumb.png.pwcimage.150.100.jpg)\\n\\nPerspectives in Risk](https://www.pwc.com/m1/en/services/assurance/perspectives-in-risk-series.html)\\n\\nMenu\\n\\nInsights & Media\\n\\n[Insights & Media](https://www.pwc.com/m1/en/media-centre.html)\\n\\n[Publications](https://www.pwc.com/m1/en/publications.html)\\n[Blogs & Articles](https://www.pwc.com/m1/en/media-centre/articles.html)\\n[Press releases](https://www.pwc.com/m1/en/media-centre/press-releases.html)\\n[Radio & TV interviews](https://www.pwc.com/m1/en/media-centre/tv-radio-interviews.html)\\n[Videos](https://www.pwc.com/m1/en/media-centre/videos.html)\\n[Events](https://www.pwc.com/m1/en/events.html)\\n\\nMenu\\n\\nInsights & Media\\n\\n[Publications](https://www.pwc.com/m1/en/publications.html)\\n\\nMenu\\n\\nInsights & Media\\n\\n[Blogs & Articles](https://www.pwc.com/m1/en/media-centre/articles.html)\\n\\nMenu\\n\\nInsights & Media\\n\\n[Press releases](https://www.pwc.com/m1/en/media-centre/press-releases.html)\\n\\nMenu\\n\\nInsights & Media\\n\\n[Radio & TV interviews](https://www.pwc.com/m1/en/media-centre/tv-radio-interviews.html)\\n\\nMenu\\n\\nInsights & Media\\n\\n[Videos](https://www.pwc.com/m1/en/media-centre/videos.html)\\n\\nMenu\\n\\nInsights & Media\\n\\n[Events](https://www.pwc.com/m1/en/events.html)\\n\\nFeatured\\n\\n[![](/m1/en/ceosurvey/2025/images/ceo-survey-banner.png.pwcimage.150.100.jpg)\\n\\n28th CEO Survey](https://www.pwc.com/m1/en/ceo-survey/28th-ceo-survey-middle-east-findings-2025.html)\\n\\n[![](/m1/en/the-new-equation/trust-you-have-to-earn-it.jpg.pwcimage.150.100.jpg)\\n\\nThe New Equation](https://www.pwc.com/m1/en/the-new-equation.html)\\n\\nMenu\\n\\nCareers\\n\\n[Careers](https://www.pwc.com/m1/en/careers.html)\\n\\n[Life at PwC](https://www.pwc.com/m1/en/about-us/life-at-pwc.html)\\n[Experienced](https://www.pwc.com/m1/en/careers/experienced-jobs.html)\\n[Graduate & Undergraduate](https://www.pwc.com/m1/en/careers/graduates-and-undergraduates-careers.html)\\n[UAE - Watani Programme](https://www.pwc.com/m1/en/careers/uae-nationals-careers.html)\\n[Alumni](https://www.pwc.com/m1/en/about-us/alumni-new.html)\\n\\nMenu\\n\\nCareers\\n\\n[Life at PwC](https://www.pwc.com/m1/en/about-us/life-at-pwc.html)\\n\\nMenu\\n\\nCareers\\n\\n[Experienced](https://www.pwc.com/m1/en/careers/experienced-jobs.html)\\n\\nMenu\\n\\nCareers\\n\\n[Graduate & Undergraduate](https://www.pwc.com/m1/en/careers/graduates-and-undergraduates-careers.html)\\n\\nMenu\\n\\nCareers\\n\\n[UAE - Watani Programme](https://www.pwc.com/m1/en/careers/uae-nationals-careers.html)\\n\\nMenu\\n\\nCareers\\n\\n[Alumni](https://www.pwc.com/m1/en/about-us/alumni-new.html)\\n\\nMenu\\n\\nAbout us\\n\\n[About us](https://www.pwc.com/m1/en/about-us.html)\\n\\n[The New Equation](https://www.pwc.com/m1/en/the-new-equation.html)\\n[We as strategic buyer and investor](https://www.pwc.com/m1/en/about-us/we-as-strategic-buyer-and-investor.html)\\n[Our leadership team](https://www.pwc.com/m1/en/about-us/our-leadership-team.html)\\n[Ethics and compliance](https://www.pwc.com/m1/en/about-us/ethics-business-conduct.html)\\n[Corporate Sustainability](https://www.pwc.com/m1/en/about-us/corporate-responsibility-pwc-me.html)\\n[Net Zero with 2030 goals](https://www.pwc.com/m1/en/about-us/net-zero.html)\\n[Transparency Reports](https://www.pwc.com/m1/en/about-us/transparency-reports.html)\\n[Analyst relations](https://www.pwc.com/gx/en/about/analyst-relations.html)\\n[PwC office locations in the Middle East](https://www.pwc.com/m1/en/about-us/office-locations-middle-east.html)\\n[Alumni](https://www.pwc.com/m1/en/about-us/alumni-new.html)\\n\\nMenu\\n\\nAbout us\\n\\n[The New Equation](https://www.pwc.com/m1/en/the-new-equation.html)\\n\\nMenu\\n\\nAbout us\\n\\n[We as strategic buyer and investor](https://www.pwc.com/m1/en/about-us/we-as-strategic-buyer-and-investor.html)\\n\\nMenu\\n\\nAbout us\\n\\n[Our leadership team](https://www.pwc.com/m1/en/about-us/our-leadership-team.html)\\n\\nMenu\\n\\nAbout us\\n\\n[Ethics and compliance](https://www.pwc.com/m1/en/about-us/ethics-business-conduct.html)\\n\\nMenu\\n\\nAbout us\\n\\n[Corporate Sustainability](https://www.pwc.com/m1/en/about-us/corporate-responsibility-pwc-me.html)\\n\\nMenu\\n\\nAbout us\\n\\n[Net Zero with 2030 goals](https://www.pwc.com/m1/en/about-us/net-zero.html)\\n\\nMenu\\n\\nAbout us\\n\\n[Transparency Reports](https://www.pwc.com/m1/en/about-us/transparency-reports.html)\\n\\nMenu\\n\\nAbout us\\n\\n[Analyst relations](https://www.pwc.com/gx/en/about/analyst-relations.html)\\n\\nMenu\\n\\nAbout us\\n\\n[PwC office locations in the Middle East](https://www.pwc.com/m1/en/about-us/office-locations-middle-east.html)\\n\\nMenu\\n\\nAbout us\\n\\n[Alumni](https://www.pwc.com/m1/en/about-us/alumni-new.html)\\n\\nLoading Results\\n\\nNo Match Found\\n\\nView All Results\\n\\n# Agentic AI – the new frontier in GenAI\\n\\nCopy link\\n\\nLink copied to clipboard\\n\\n**GenAI is poised to make a significant economic impact, with estimates suggesting it could contribute between US$2.6 trillion and US$4.4 trillion annually to global GDP by 2030 across various sectors. The future of GenAI is agentic, where AI agents collaborate in real-time to automate complex tasks and enhance decision-making. This executive playbook explores how organisations can harness agentic AI to boost efficiency, improve customer experiences, and drive revenue growth.**\\n\\n## Key aspects of agentic AI\\n\\n![Space strategy](/gx/en/brand-simplified/picto-images-132/black/picto-automation-black.svg)\\n\\nAutonomy\\n\\nAutonomy Agentic AI systems can operate independently, making decisions based on their programming, learning, and environmental inputs.\\n\\n![Market research and analytics](/gx/en/brand-simplified/picto-images-132/black/picto-sales-black.svg)\\n\\nGoal-oriented behaviour\\n\\nThese AI agents are designed to pursue specific objectives, optimising their actions to achieve the desired outcomes.\\n\\n![Socio-economic impact assessment](/gx/en/brand-simplified/picto-images-132/black/picto-environment-black.svg)\\n\\nEnvironment interaction\\n\\nAn agentic AI interacts with its surroundings, perceiving changes and adapting its strategies accordingly.\\n\\n![Financial advisory](/gx/en/brand-simplified/picto-images-132/black/picto-deep-learning-black.svg)\\n\\nLearning capability\\n\\nMany agentic AI systems employ machine learning or reinforcement learning techniques to improve their performance over time.\\n\\n![Technology and innovation](/gx/en/brand-simplified/picto-images-132/black/picto-net-working-capital-black.svg)\\n\\nWorkflow optimisation\\n\\nAgentic AI agents enhance workflows and business processes by integrating language understanding with reasoning, planning, and decision making. This involves optimising resource allocation, improving communication and collaboration, and identifying automation opportunities.\\n\\n![Governance and organisational structure](/gx/en/brand-simplified/picto-images-132/black/picto-multi-step-logic-black.svg)\\n\\nMulti-agent and system conversation\\n\\nAnalysing governance and organisational structures and performing programmes and organisation performance reviews.\\n\\n## Download playbook here\\n\\n[Download](https://www.pwc.com/m1/en/publications/documents/2024/agentic-ai-the-new-frontier-in-genai-an-executive-playbook.pdf \"Download\")\\n(PDF of 1.35mb)\\n\\n## Contact us\\n\\nAkif Kamal\\n\\nPartner, Technology Consulting, PwC Middle East\\n\\n+971 50 8734894\\n\\n[Email](/m1/en/content/pwc/global/forms/contactUsNew.html?parentPagePath=/content/pwc/m1/en/publications/agentic-ai-the-new-frontier-in-genai&style=pwc&territory=m1&contactLink=L2NvbnRlbnQvZGFtL3B3Yy9tMS9lbi9jb250ZW50LWZyYWdtZW50cy9jb250YWN0cy9hL2FraWYta2FtYWwtbg==&cf=true)\\n\\nDr. Mohammad Tanvir Ansari\\n\\nDirector, Technology Consulting, PwC Middle East\\n\\n+971 50 716 4679\\n\\n[Email](/m1/en/content/pwc/global/forms/contactUsNew.html?parentPagePath=/content/pwc/m1/en/publications/agentic-ai-the-new-frontier-in-genai&style=pwc&territory=m1&contactLink=L2NvbnRlbnQvZGFtL3B3Yy9tMS9lbi9jb250ZW50LWZyYWdtZW50cy9jb250YWN0cy9tL21vaGFtbWFkLXRhbnZpci1hbnNhcmk=&cf=true)\\n\\n## Contact us\\n\\nRajat Chowdhary\\n\\nPartner, Technology Consulting, PwC Middle East\\n\\nTel: +971 50 429 3733\\n\\n[![Linkedin Follow](data:image/gif;base64...)](https://www.linkedin.com/in/rajatchowdhary/)\\n Email\\n\\nSharang Gupta\\n\\nDirector, Technology Consulting, PwC Middle East\\n\\nTel: +971 50 432 6559\\n\\n[![Linkedin Follow](data:image/gif;base64...)](https://www.linkedin.com/in/sharang-gupta-6aa93621/)\\n Email\\n\\nVishesh Kalia\\n\\nDirector, Technology Consulting, PwC Middle East\\n\\nTel: + 971 56 520 3814\\n\\n[![Linkedin Follow](data:image/gif;base64...)](https://www.linkedin.com/in/vishesh-kalia-35577438/?originalSubdomain=ae)\\n Email\\n\\n## Contact us\\n\\n![Fadi Komati](/m1/en/people/photos/fadi-komati.jpg.pwcimage.105.105.jpg)\\n\\n![Joseph  Abboud](/m1/en/people/photos/joseph-abboud-new.jpg.pwcimage.105.105.jpg)\\n\\nJoseph Abboud\\n\\nTechnology Consulting, Partner, PwC Middle East\\n\\n[![Linkedin Follow](data:image/gif;base64...)](https://www.linkedin.com/in/joseph-abboud-63b248155/)\\n Email\\n\\n![Wassim Mukaddam](/m1/en/people/photos/wassim-mukaddam.JPG.pwcimage.105.105.jpg)\\n\\nWassim Mukaddam\\n\\nTechnology Consulting, Director, PwC Middle East\\n\\n Email\\n\\n[Issues](/m1/en/issues.html)\\n[Navigating data privacy regulations](/m1/en/services/consulting/technology/cyber-security/navigating-data-privacy-regulations.html)\\n[New world. New skills.](/m1/en/issues/upskilling.html)\\n[Responding to the potential business impacts of COVID-19](/m1/en/publications/covid-19.html)\\n[Doing Business in the Middle East](/m1/en/services/tax/doing-business-guides-middle-east.html)\\n[VAT in the Middle East](/m1/en/services/tax/vat-in-the-middle-east.html)\\n[Megatrends Now: The need to ADAPT](/m1/en/issues/adapt.html)\\n\\n[Services](/m1/en/services.html)\\n[Assurance and Audit](/m1/en/services/assurance.html)\\n[Consulting](/m1/en/services/consulting.html)\\n[Deals](/m1/en/services/deals.html)\\n[Entrepreneurial & Private Business](/m1/en/services/private-business.html)\\n[PwC Academy](https://www.pwcacademy-me.com/)\\n[Tax and Legal](/m1/en/services/tax.html)\\n[Strategy&](https://www.strategyand.pwc.com/m1/en.html)\\n\\n[Industries](/m1/en/industries.html)\\n\\n[Banking and Capital Markets](/m1/en/industries/banking-capital-markets.html)\\n[Consumer Markets](/m1/en/industries/consumer-markets.html)\\n[Energy, Utilities & Resources](https://www.pwc.com/m1/en/industries/energy-utilities-resources.html)\\n[Financial Services](/m1/en/industries/financial-services.html)\\n[Government/Public Services](/m1/en/industries/government-public-services.html)\\n\\n[Health Industries](/m1/en/industries/healthcare.html)\\n[Industrial Manufacturing](/m1/en/industries/manufacturing.html)\\n[Real Estate](/m1/en/industries/investment-management-real-estate.html)\\n[Transportation and Logistics](/m1/en/industries/transportation-and-logistics.html)\\n\\n[About us](/m1/en/about-us.html)\\n[PwC office locations in the Middle East](/m1/en/about-us/office-locations-middle-east.html)\\n[Our purpose and values](/m1/en/about-us/purpose-and-values.html)\\n[Corporate Sustainability](/m1/en/about-us/corporate-responsibility-pwc-me.html)\\n[Ethics and compliance](/m1/en/about-us/ethics-business-conduct.html)\\n[Transparency reports](/m1/en/about-us/transparency-reports.html)\\n[Health, Safety and Environment Policy](/m1/en/about-us/health-safety-environment-policy.html)\\n[Third party code of conduct](/m1/en/about-us/third-party-code-of-conduct.html)\\n[PwC’s global human rights statement](https://www.pwc.com/m1/en/about-us/2024/document/pwc-human-rights-policy.pdf)\\n[Alumni](/m1/en/about-us/alumni-new.html)\\n\\n[Publications](/m1/en/publications.html)\\n\\n[Careers](/m1/en/careers.html)\\n[Graduate & undergraduate careers](/m1/en/careers/graduates.html)\\n[Experienced careers](/m1/en/careers/experienced-jobs.html)\\n[Opportunities for Omani nationals](/m1/en/careers/oman-nationals-careers.html)\\n[Opportunities for Saudi nationals](/m1/en/careers/saudi-national-careers.html)\\n[PwC\\'s Watani Graduate Programme for UAE Nationals](/m1/en/careers/uae-nationals-careers.html)\\n\\n[Media centre](/m1/en/media-centre.html)\\n[Press releases](/m1/en/media-centre/press-releases.html)\\n[Articles and blog posts](/m1/en/media-centre/articles.html)\\n[TV and radio interviews](/m1/en/media-centre/tv-radio-interviews.html)\\n\\n©\\xa02017\\n- 2025 PwC. All rights reserved. PwC refers to the PwC network and/or one or more of its member firms, each of which is a separate legal entity. Please see [www.pwc.com/structure](https://www.pwc.com/structure) for further details.\\n\\n* [Privacy](https://www.pwc.com/gx/en/legal-notices/pwc-privacy-statement.html)\\n* [Cookie policy](/m1/en/cookie-information.html)\\n* [Legal](/gx/en/site-information/legal-disclaimer.html)\\n* [About site provider](/m1/en/home.html)\\n* [Site map](/m1/en/sitemap.html)', 'AI Index: State of AI in 13 Charts\\n[Skip to main content](#main-content)\\n[Skip to secondary navigation](#secondary-navigation)\\n\\n[Stanford University(link is external)](https://stanford.edu)\\n\\n[![Stanford HAI](/themes/hai/stanford_basic_hai/lockup.svg)](https://hai.stanford.edu/)\\n\\nSearch this site\\n\\nSubmit Search\\n\\n* [About](/about)\\n  + [People](/about/people)\\n  + [Values](/about/values)\\n  + [Faculty Publications](/about/faculty-publications)\\n  + [Corporate Programs](/about/corporate-programs)\\n  + [Get Involved](/about/get-involved)\\n* [Centers](/centers)\\n  + [Center for Research on Foundation Models](https://crfm.stanford.edu/)\\n  + [Center for the Study of Language and Information](https://www-csli.stanford.edu/)\\n  + [Digital Economy Lab](https://digitaleconomy.stanford.edu/)\\n  + [RAISE Health](https://med.stanford.edu/raisehealth)\\n  + [RegLab](https://reglab.stanford.edu/)\\n  + [The Stanford Robotics Center](https://src.stanford.edu/)\\n  + [Initiatives](/centers/initiatives)\\n  + [Partners](/centers/stanford-hai-partners)\\n* [Research](/research)\\n  + [Fellowship Programs](/research/fellowship-programs)\\n  + [Grant Programs](/research/grant-programs)\\n  + [AI Index Report](/research/ai-index-report)\\n  + [Student Affinity Groups](/research/student-affinity-groups)\\n* [Education](/education)\\n  + [Professional Education](/education/professional-education)\\n  + [Stanford Student Courses](/education/courses)\\n* [Policy](/policy)\\n  + [Policy Publications](/policy/policy-publications)\\n  + [National AI Research Resource](/policy/national-ai-research-resource)\\n  + [Congressional Boot Camp on AI](/congressional-boot-camp-ai)\\n  + [Tech Ethics & Policy Summer Fellowships](/policy/tech-ethics-policy-summer-fellowships)\\n  + [Tracking the U.S. AI Executive Order](/policy/tracking-us-ai-executive-order)\\n* [News](/news)\\n  + [Blog](/news/blog)\\n  + [Announcements](/news/announcements)\\n  + [Media Mentions](/news/media-mentions)\\n  + [Subscribe to HAI Newsletter](/news/subscribe-hai-mailing-list)\\n* [Events](/events)\\n  + [Upcoming Events](/events/upcoming-events)\\n  + [Past Events](/events/past-events)\\n  + [HAI Seminars](/hai-weekly-seminars)\\n\\nYour browser does not support JavaScript! Please enable JavaScript in order to experience this site.\\nPage Content\\n[Economy and Markets](/taxonomy/term/58), [Language Processing](/taxonomy/term/68), [Law, Regulation, and Policy](/taxonomy/term/60)\\n\\n# AI Index: State of AI in 13 Charts\\n\\nIn the new report, foundation models dominate, benchmarks fall, prices skyrocket, and on the global stage, the U.S. overshadows.\\n\\n Apr 15, 2024\\n\\n| Shana Lynch\\nhttps://twitter.com/StanfordHAI?ref\\\\_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor\\nhttps://www.facebook.com/StanfordHAI/\\n<https://www.youtube.com/channel/UChugFTK0KyrES9terTid8vA>\\nhttps://www.linkedin.com/company/stanfordhai\\n<https://www.instagram.com/stanfordhai/?hl=en>\\n\\nImage\\n ![Illustration of bright lines intersecting on a dark background](/sites/default/files/styles/media/public/2024-04/AIIndex_2024_StateofAI_BLOG_01.jpg?itok=dJQB96bo)\\n\\nThis year’s [AI Index](https://aiindex.stanford.edu/report/) — a 500-page report tracking 2023’s worldwide trends in AI — is out.\\n\\nThe index is an independent initiative at the [Stanford Institute for Human-Centered Artificial Intelligence](https://hai.stanford.edu/) (HAI), led by the AI Index Steering Committee, an interdisciplinary group of experts from across academia and industry. This year’s report covers the rise of multimodal foundation models, major cash investments into generative AI, new performance benchmarks, shifting global opinions, and new major regulations.\\n\\nDon’t have an afternoon to pore through the findings? Check out the high level here.\\n\\n![Pie chart showing 98 models were open-sourced in 2023](/sites/default/files/inline-images/AIIndex_2024_StateofAI_BLOG_02.jpg)\\n## A Move Toward Open-Sourced\\n\\nThis past year, organizations released 149 foundation models, more than double the number released in 2022. Of these newly released models, 65.7% were open-source (meaning they can be freely used and modified by anyone), compared with only 44.4% in 2022 and 33.3% in 2021.\\n\\n![bar chart showing that closed models outperformed open models across tasks](/sites/default/files/inline-images/AIIndex_2024_StateofAI_BLOG_07.jpg)\\n## But At a Cost of Performance?\\n\\nClosed-source models still outperform their open-sourced counterparts. On 10 selected benchmarks, closed models achieved a median performance advantage of 24.2%, with differences ranging from as little as 4.0% on mathematical tasks like GSM8K to as much as 317.7% on agentic tasks like AgentBench.\\n\\n![Bar chart showing Google has more foundation models than any other company](/sites/default/files/inline-images/AIIndex_2024_StateofAI_BLOG_03.jpg)\\n## Biggest Players\\n\\nIndustry dominates AI, especially in building and releasing foundation models. This past year Google edged out other industry players in releasing the most models, including Gemini and RT-2. In fact, since 2019, Google has led in releasing the most foundation models, with a total of 40, followed by OpenAI with 20. Academia trails industry: This past year, UC Berkeley released three models and Stanford two.\\n\\n![Line chart showing industry far outpaces academia and government in creating foundation models over the decade](/sites/default/files/inline-images/AIIndex_2024_StateofAI_BLOG_14.jpg)\\n## Industry Dwarfs All\\n\\nIf you needed more striking evidence that corporate AI is the only player in the room right now, this should do it. In 2023, industry accounted for 72% of all new foundation models.\\n\\n![Chart showing the growing costs of training AI models](/sites/default/files/inline-images/AIIndex_2024_StateofAI_BLOG_04.jpg)\\n## Prices Skyrocket\\n\\nOne of the reasons academia and government have been edged out of the AI race: the exponential increase in cost of training these giant models. Google’s Gemini Ultra cost an estimated $191 million worth of compute to train, while OpenAI’s GPT-4 cost an estimated $78 million. In comparison, in 2017, the original Transformer model, which introduced the architecture that underpins virtually every modern LLM, cost around $900.\\n\\n![Bar chart showing the united states produces by far the largest number of foundation models](/sites/default/files/inline-images/AIIndex_2024_StateofAI_BLOG_05.jpg)\\n## What AI Race?\\n\\nAt least in terms of notable machine learning models, the United States vastly outpaced other countries in 2023, developing a total of 61 models in 2023. Since 2019, the U.S. has consistently led in originating the majority of notable models, followed by China and the UK.\\n\\n![Line chart showing that across many intellectual task categories, AI has exceeded human performance](/sites/default/files/inline-images/AIIndex_2024_StateofAI_BLOG_06.jpg)\\n## Move Over, Human\\n\\nAs of 2023, AI has hit human-level performance on many significant AI benchmarks, from those testing reading comprehension to visual reasoning. Still, it falls just short on some benchmarks like competition-level math. Because AI has been blasting past so many standard benchmarks, AI scholars have had to create new and more difficult challenges. This year’s index also tracked several of these new benchmarks, including those for tasks in coding, advanced reasoning, and agentic behavior.\\n\\n![Bar chart showing a dip in overall private investment in AI, but a surge in generative AI investment](/sites/default/files/inline-images/AIIndex_2024_StateofAI_BLOG_08.jpg)\\n## Private Investment Drops (But We See You, GenAI)\\n\\nWhile AI private investment has steadily dropped since 2021, generative AI is gaining steam. In 2023, the sector attracted $25.2 billion, nearly ninefold the investment of 2022 and about 30 times the amount from 2019 (call it the ChatGPT effect). Generative AI accounted for over a quarter of all AI-related private investments in 2023.\\n\\n![Bar chart showing the united states overwhelming dwarfs other countries in private investment in AI](/sites/default/files/inline-images/AIIndex_2024_StateofAI_BLOG_09.jpg)\\n## U.S. Wins $$ Race\\n\\nAnd again, in 2023 the United States dominates in AI private investment. In 2023, the $67.2 billion invested in the U.S. was roughly 8.7 times greater than the amount invested in the next highest country, China, and 17.8 times the amount invested in the United Kingdom. That lineup looks the same when zooming out: Cumulatively since 2013, the United States leads investments at $335.2 billion, followed by China with $103.7 billion, and the United Kingdom at $22.3 billion.\\n\\n![Infographic showing 26% of businesses use AI for contact-center automation, and 23% use it for personalization](/sites/default/files/inline-images/AIIndex_2024_StateofAI_BLOG_10.jpg)\\n## Where is Corporate Adoption?\\n\\nMore companies are implementing AI in some part of their business: In surveys, 55% of organizations said they were using AI in 2023, up from 50% in 2022 and 20% in 2017. Businesses report using AI to automate contact centers, personalize content, and acquire new customers.\\n\\n![Bar chart showing 57% of people believe AI will change how they do their job in 5 years, and 36% believe AI will replace their jobs.](/sites/default/files/inline-images/AIIndex_2024_StateofAI_BLOG_12.jpg)\\n## Younger and Wealthier People Worry About Jobs\\n\\nGlobally, most people expect AI to change their jobs, and more than a third expect AI to replace them. Younger generations — Gen Z and millennials — anticipate more substantial effects from AI compared with older generations like Gen X and baby boomers. Specifically, 66% of Gen Z compared with 46% of boomer respondents believe AI will significantly affect their current jobs. Meanwhile, individuals with higher incomes, more education, and decision-making roles foresee AI having a great impact on their employment.\\n\\n![Bar chart depicting the countries most nervous about AI; Australia at 69%, Great Britain at 65%, and Canada at 63% top the list](/sites/default/files/inline-images/AIIndex_2024_StateofAI_BLOG_13.jpg)\\n## While the Commonwealth Worries About AI Products\\n\\nWhen asked in a survey about whether AI products and services make you nervous, 69% of Aussies and 65% of Brits said yes. Japan is the least worried about their AI products at 23%.\\n\\n![Line graph showing uptick in AI regulation in the united states since 2016; 25 policies passed in 2023](/sites/default/files/inline-images/AIIndex_2024_StateofAI_BLOG_11.jpg)\\n## Regulation Rallies\\n\\nMore American regulatory agencies are passing regulations to protect citizens and govern the use of AI tools and data. For example, the Copyright Office and the Library of Congress passed copyright registration guidance concerning works that contained material generated by AI, while the Securities and Exchange Commission developed a cybersecurity risk management strategy, governance, and incident disclosure plan. The agencies to pass the most regulation were the Executive Office of the President and the Commerce Department.\\n\\n*The* [*AI Index*](https://aiindex.stanford.edu/report/) *was first created to track AI development. The index collaborates with such organizations as LinkedIn, Quid, McKinsey, Studyportals, the Schwartz Reisman Institute, and the International Federation of Robotics to gather the most current research and feature important insights on the AI ecosystem.*\\n\\n## More News Topics\\n\\n[Economy and Markets](/taxonomy/term/58), [Language Processing](/taxonomy/term/68), [Law, Regulation, and Policy](/taxonomy/term/60), [Machine Learning](/taxonomy/term/69)\\n\\n[![Stanford HAI](/themes/hai/stanford_basic_hai/src/assets/img/hai-logo-vertical-sm.png)](/)\\n\\n## Navigate\\n\\n* [Welcome](/navigate/welcome)\\n* [Values](/about/values)\\n* [News](/news)\\n* [Events](/events)\\n* [Careers](/about/careers)\\n\\n## Participate\\n\\n* [Get Involved](/about/get-involved)\\n* [Grant Programs](/research/grant-programs)\\n* [Corporate Programs](/about/corporate-programs)\\n* [Support HAI](/participate/support-hai)\\n* [Contact Us](/contact-us)\\n\\n## Follow us\\n\\n<https://twitter.com/StanfordHAI>\\n<https://www.facebook.com/StanfordHAI>\\n<https://www.youtube.com/channel/UChugFTK0KyrES9terTid8vA>\\n<https://www.linkedin.com/company/stanfordhai>\\n<https://www.instagram.com/stanfordhai>\\n\\n### Newsletter Sign Up\\n\\nDon’t miss out. Get Stanford HAI updates delivered directly to your inbox.\\n\\n[Subscribe](/join-hai-community)\\n\\n[Stanford\\nUniversity](https://www.stanford.edu)\\n\\n* [Stanford Home(link is external)](https://www.stanford.edu)\\n* [Maps & Directions(link is external)](https://visit.stanford.edu/plan/)\\n* [Search Stanford(link is external)](https://www.stanford.edu/search/)\\n* [Emergency Info(link is external)](https://emergency.stanford.edu)\\n\\n* [Terms of Use(link is external)](https://www.stanford.edu/site/terms/ \"Terms of use for sites\")\\n* [Privacy(link is external)](https://www.stanford.edu/site/privacy/ \"Privacy and cookie policy\")\\n* [Copyright(link is external)](https://uit.stanford.edu/security/copyright-infringement \"Report alleged copyright infringement\")\\n* [Trademarks(link is external)](https://adminguide.stanford.edu/chapter-1/subchapter-5/policy-1-5-4 \"Ownership and use of Stanford trademarks and images\")\\n* [Non-Discrimination(link is external)](https://studentservices.stanford.edu/more-resources/student-policies/non-academic/non-discrimination \"Non-discrimination policy\")\\n* [Accessibility(link is external)](https://www.stanford.edu/site/accessibility \"Report web accessibility issues\")\\n\\n© Stanford University.\\n\\xa0 Stanford, California 94305.']\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in tool_output:\n",
        "    print(doc)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOS3qwf6tKNF",
        "outputId": "b7caee23-452d-4de5-b0cd-909a5fc7ff62"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The state of AI in early 2024 | McKinsey\n",
            "[Skip to main content](#skipToMain)\n",
            "# The state of AI in early 2024: Gen AI adoption spikes and starts to generate value\n",
            "\n",
            "May 30, 2024 | SurveyAs generative AI adoption accelerates, survey respondents report measurable benefits and increased mitigation of the risk of inaccuracy. A small group of high performers lead the way.\n",
            "\n",
            "###\n",
            "\n",
            " [(23 pages)](#/download/%2F~%2Fmedia%2Fmckinsey%2Fbusiness%20functions%2Fquantumblack%2Four%20insights%2Fthe%20state%20of%20ai%2F2024%2Fthe-state-of-ai-in-early-2024-final.pdf%3FshouldIndex%3Dfalse)\n",
            "\n",
            "**If 2023** was the year the world discovered [generative AI (gen AI)](/featured-insights/mckinsey-explainers/what-is-generative-ai), 2024 is the year organizations truly began using—and deriving business value from—this new technology. In the latest [McKinsey Global Survey](/featured-insights/mckinsey-global-surveys) on AI, 65 percent of respondents report that their organizations are regularly using gen AI, nearly double the percentage from our previous survey just ten months ago. Respondents’ expectations for gen AI’s impact remain as high [as they were last year](/capabilities/quantumblack/our-insights/the-state-of-ai-in-2023-generative-ais-breakout-year), with three-quarters predicting that gen AI will lead to significant or disruptive change in their industries in the years ahead.\n",
            "\n",
            "## About the authors\n",
            "\n",
            "This article is a collaborative effort by [Alex Singla](/our-people/alex-singla), [Alexander Sukharevsky](/our-people/alexander-sukharevsky), [Lareina Yee](/our-people/lareina-yee), and [Michael Chui](/our-people/michael-chui), with [Bryce Hall](/our-people/bryce-hall), representing views from QuantumBlack, AI by McKinsey, and McKinsey Digital.\n",
            "\n",
            "Organizations are already seeing material benefits from gen AI use, reporting both cost decreases and revenue jumps in the business units deploying the technology. The survey also provides insights into the kinds of risks presented by gen AI—most notably, inaccuracy—as well as the emerging practices of top performers to mitigate those challenges and capture value.\n",
            "\n",
            "## AI adoption surges\n",
            "\n",
            "Interest in generative AI has also brightened the spotlight on a broader set of AI capabilities. For the past six years, AI adoption by respondents’ organizations has hovered at about 50 percent. This year, the survey finds that adoption has jumped to 72 percent (Exhibit 1). And the interest is truly global in scope. Our 2023 survey found that AI adoption did not reach 66 percent in *any* region; however, this year more than two-thirds of respondents in nearly *every* region say their organizations are using AI.1Organizations based in Central and South America are the exception, with 58 percent of respondents working for organizations based in Central and South America reporting AI adoption. Looking by industry, the biggest increase in adoption can be found in professional services.2Includes respondents working for organizations focused on human resources, legal services, management consulting, market research, R&D, tax preparation, and training.\n",
            "\n",
            "![Al adoption worldwide has increased dramatically in the past year, after years of little meaningful change.](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai/2024/stateofai-ex-1.svgz?cq=50&cpy=Center)\n",
            "\n",
            "Also, responses suggest that companies are now using AI in more parts of the business. Half of respondents say their organizations have adopted AI in two or more business functions, up from less than a third of respondents in 2023 (Exhibit 2).\n",
            "\n",
            "![Survey findings suggest that organizations are using AI in more business functions now than in previous years.](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai/2024/stateofai-ex-2.svgz?cq=50&cpy=Center)\n",
            "### Gen AI adoption is most common in the functions where it can create the most value\n",
            "\n",
            "Most respondents now report that their organizations—and they as individuals—are using gen AI. Sixty-five percent of respondents say their organizations are regularly using gen AI in at least one business function, up from one-third last year. The average organization using gen AI is doing so in two functions, most often in marketing and sales and in product and service development—two functions in which [previous research](/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier) determined that gen AI adoption could generate the most value3“[The economic potential of generative AI: The next productivity frontier](/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier),” McKinsey, June 14, 2023.—as well as in IT (Exhibit 3). The biggest increase from 2023 is found in marketing and sales, where reported adoption has more than doubled. Yet across functions, only two use cases, both within marketing and sales, are reported by 15 percent or more of respondents.\n",
            "\n",
            "![Respondents most often report generative AI adoption in their marketing- and-sales, product- and service-development, and IT functions.](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai/2024/stateofai-ex-3.svgz?cq=50&cpy=Center)\n",
            "\n",
            "Gen AI also is weaving its way into respondents’ personal lives. Compared with 2023, respondents are much more likely to be using gen AI at work and even more likely to be using gen AI both at work and in their personal lives (Exhibit 4). The survey finds upticks in gen AI use across all regions, with the largest increases in Asia–Pacific and Greater China. Respondents at the highest seniority levels, meanwhile, show larger jumps in the use of gen Al tools for work and outside of work compared with their midlevel-management peers. Looking at specific industries, respondents working in energy and materials and in professional services report the largest increase in gen AI use.\n",
            "\n",
            "### Investments in gen AI and analytical AI are beginning to create value\n",
            "\n",
            "The latest survey also shows how different industries are budgeting for gen AI. Responses suggest that, in many industries, organizations are about equally as likely to be investing more than 5 percent of their digital budgets in gen AI as they are in nongenerative, analytical-AI solutions (Exhibit 5). Yet in most industries, larger shares of respondents report that their organizations spend more than 20 percent on analytical AI than on gen AI. Looking ahead, most respondents—67 percent—expect their organizations to invest more in AI over the next three years.\n",
            "\n",
            "Where are those investments paying off? For the first time, our latest survey explored the value created by gen AI use by business function. The function in which the largest share of respondents report seeing cost decreases is human resources. Respondents most commonly report meaningful revenue increases (of more than 5 percent) in supply chain and inventory management (Exhibit 6). For analytical AI, respondents most often report seeing cost benefits in service operations—in line with what we found [last year](/capabilities/quantumblack/our-insights/the-state-of-ai-in-2023-generative-ais-breakout-year)—as well as meaningful revenue increases from AI use in marketing and sales.\n",
            "\n",
            "## Inaccuracy: The most recognized and experienced risk of gen AI use\n",
            "\n",
            "As businesses begin to see the benefits of gen AI, they’re also recognizing the diverse risks associated with the technology. These can range from data management risks such as data privacy, bias, or intellectual property (IP) infringement to model management risks, which tend to focus on inaccurate output or lack of explainability. A third big risk category is security and incorrect use.\n",
            "\n",
            "##\n",
            "\n",
            "Respondents to the latest survey are more likely than they were last year to say their organizations consider inaccuracy and IP infringement to be relevant to their use of gen AI, and about half continue to view cybersecurity as a risk (Exhibit 7).\n",
            "\n",
            "##\n",
            "\n",
            "Conversely, respondents are less likely than they were last year to say their organizations consider workforce and labor displacement to be relevant risks and are not increasing efforts to mitigate them.\n",
            "\n",
            "##\n",
            "\n",
            "In fact, inaccuracy—[which can affect use cases across the gen AI value chain](/capabilities/risk-and-resilience/our-insights/implementing-generative-ai-with-speed-and-safety), ranging from customer journeys and summarization to coding and creative content—is the only risk that respondents are significantly more likely than last year to say their organizations are actively working to mitigate.\n",
            "\n",
            "Exhibit 7 ![Inaccuracy and intellectual property infringement are increasingly considered relevant risks to organizations’ generative AI use. (2 of 4)](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai/2024/stateofai-ex-7b.svgz?cq=50&cpy=Center)![Inaccuracy and intellectual property infringement are increasingly considered relevant risks to organizations’ generative AI use. (3 of 4)](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai/2024/stateofai-ex-7c.svgz?cq=50&cpy=Center)![Inaccuracy and intellectual property infringement are increasingly considered relevant risks to organizations’ generative AI use. (4 of 4)](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai/2024/stateofai-ex-7d.svgz?cq=50&cpy=Center)\n",
            "\n",
            "Some organizations have already experienced negative consequences from the use of gen AI, with 44 percent of respondents saying their organizations have experienced at least one consequence (Exhibit 8). Respondents most often report inaccuracy as a risk that has affected their organizations, followed by cybersecurity and explainability.\n",
            "\n",
            "![Nearly one-quarter of respondents say their organizations have experienced negative consequences from generative AI’s inaccuracy.](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai/2024/stateofai-ex-8.svgz?cq=50&cpy=Center)\n",
            "\n",
            "[Our previous research](/capabilities/risk-and-resilience/our-insights/implementing-generative-ai-with-speed-and-safety) has found that there are several elements of governance that can help in scaling gen AI use responsibly, yet few respondents report having these risk-related practices in place.4“[Implementing generative AI with speed and safety](/capabilities/risk-and-resilience/our-insights/implementing-generative-ai-with-speed-and-safety),” *McKinsey Quarterly*, March 13, 2024. For example, just 18 percent say their organizations have an enterprise-wide council or board with the authority to make decisions involving responsible AI governance, and only one-third say gen AI risk awareness and risk mitigation controls are required skill sets for technical talent.\n",
            "\n",
            "## Bringing gen AI capabilities to bear\n",
            "\n",
            "The latest survey also sought to understand how, and how quickly, organizations are deploying these new gen AI tools. We have found [three archetypes for implementing gen AI solutions](/capabilities/mckinsey-digital/our-insights/technologys-generational-moment-with-generative-ai-a-cio-and-cto-guide): *takers* use off-the-shelf, publicly available solutions; *shapers* customize those tools with proprietary data and systems; and *makers* develop their own foundation models from scratch.5“[Technology’s generational moment with generative AI: A CIO and CTO guide](/capabilities/mckinsey-digital/our-insights/technologys-generational-moment-with-generative-ai-a-cio-and-cto-guide),” McKinsey, July 11, 2023. Across most industries, the survey results suggest that organizations are finding off-the-shelf offerings applicable to their business needs—though many are pursuing opportunities to customize models or even develop their own (Exhibit 9). About half of reported gen AI uses within respondents’ business functions are utilizing off-the-shelf, publicly available models or tools, with little or no customization. Respondents in energy and materials, technology, and media and telecommunications are more likely to report significant customization or tuning of publicly available models or developing their own proprietary models to address specific business needs.\n",
            "\n",
            "![Organizations are pursuing a mix of off-the-shelf generative AI capabilities and also significantly customizing models or developing their own.](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai/2024/stateofai-ex-9.svgz?cq=50&cpy=Center)\n",
            "\n",
            "Respondents most often report that their organizations required one to four months from the start of a project to put gen AI into production, though the time it takes varies by business function (Exhibit 10). It also depends upon the approach for acquiring those capabilities. Not surprisingly, reported uses of highly customized or proprietary models are 1.5 times more likely than off-the-shelf, publicly available models to take five months or more to implement.\n",
            "\n",
            "![Business functions are most often able to put their generative AI capabilities to use within one to four months.](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai/2024/stateofai-ex-10.svgz?cq=50&cpy=Center)\n",
            "\n",
            "## Gen AI high performers are excelling despite facing challenges\n",
            "\n",
            "Gen AI is a new technology, and organizations are still early in the journey of pursuing its opportunities and scaling it across functions. So it’s little surprise that only a small subset of respondents (46 out of 876) report that a meaningful share of their organizations’ EBIT can be attributed to their deployment of gen AI. Still, these gen AI leaders are worth examining closely. These, after all, are the early movers, who already attribute more than 10 percent of their organizations’ EBIT to their use of gen AI. Forty-two percent of these high performers say more than 20 percent of their EBIT is attributable to their use of nongenerative, analytical AI, and they span industries and regions—though most are at organizations with less than $1 billion in annual revenue. The AI-related practices at these organizations can offer guidance to those looking to create value from gen AI adoption at their own organizations.\n",
            "\n",
            "To start, gen AI high performers are using gen AI in more business functions—an average of three functions, while others average two. They, like other organizations, are most likely to use gen AI in marketing and sales and product or service development, but they’re much more likely than others to use gen AI solutions in risk, legal, and compliance; in strategy and corporate finance; and in supply chain and inventory management. They’re more than three times as likely as others to be using gen AI in activities ranging from processing of accounting documents and risk assessment to R&D testing and pricing and promotions. While, overall, about half of reported gen AI applications within business functions are utilizing publicly available models or tools, gen AI high performers are less likely to use those off-the-shelf options than to either implement significantly customized versions of those tools or to develop their own proprietary foundation models.\n",
            "\n",
            "What else are these high performers doing differently? For one thing, they are paying more attention to gen-AI-related risks. Perhaps because they are further along on their journeys, they are more likely than others to say their organizations have experienced every negative consequence from gen AI we asked about, from cybersecurity and personal privacy to explainability and IP infringement. Given that, they are more likely than others to report that their organizations consider those risks, as well as regulatory compliance, environmental impacts, and political stability, to be relevant to their gen AI use, and they say they take steps to mitigate more risks than others do.\n",
            "\n",
            "Gen AI high performers are also much more likely to say their organizations follow a set of risk-related best practices (Exhibit 11). For example, they are nearly twice as likely as others to involve the legal function and embed risk reviews early on in the development of gen AI solutions—that is, to “[shift left](/about-us/new-at-mckinsey-blog/an-inside-look-at-how-businesses-are-or-not-managing-ai-risk).” They’re also much more likely than others to employ a wide range of other best practices, from strategy-related practices to those related to scaling.\n",
            "\n",
            "In addition to experiencing the risks of gen AI adoption, high performers have encountered other challenges that can serve as warnings to others (Exhibit 12). Seventy percent say they have experienced difficulties with data, including defining processes for data governance, developing the ability to quickly integrate data into AI models, and an insufficient amount of training data, highlighting the essential role that data play in capturing value. High performers are also more likely than others to report experiencing challenges with their operating models, such as implementing agile ways of working and effective sprint performance management.\n",
            "\n",
            "![Generative AI high performers report experiencing a range of challenges in capturing value from the technology.](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai/2024/stateofai-ex-12.svgz?cq=50&cpy=Center)\n",
            "\n",
            "## About the research\n",
            "\n",
            "The online survey was in the field from February 22 to March 5, 2024, and garnered responses from 1,363 participants representing the full range of regions, industries, company sizes, functional specialties, and tenures. Of those respondents, 981 said their organizations had adopted AI in at least one business function, and 878 said their organizations were regularly using gen AI in at least one function. To adjust for differences in response rates, the data are weighted by the contribution of each respondent’s nation to global GDP.\n",
            "\n",
            "#####\n",
            "\n",
            "**[Alex Singla](/our-people/alex-singla)** and **[Alexander Sukharevsky](/our-people/alexander-sukharevsky)** are global coleaders of QuantumBlack, AI by McKinsey, and senior partners in McKinsey’s Chicago and London offices, respectively; **[Lareina Yee](/our-people/lareina-yee)** is a senior partner in the Bay Area office, where **Michael Chui,** a McKinsey Global Institute partner, is a partner; and **[Bryce Hall](/our-people/bryce-hall)** is an associate partner in the Washington, DC, office.\n",
            "\n",
            "They wish to thank Kaitlin Noe, Larry Kanter, Mallika Jhamb, and Shinjini Srivastava for their contributions to this work.\n",
            "\n",
            "---\n",
            "\n",
            "This article was edited by Heather Hanselman, a senior editor in McKinsey’s Atlanta office.\n",
            "\n",
            "##### Explore a career with us\n",
            "\n",
            "[Search Openings](/careers/search-jobs)\n",
            "##### Related Articles\n",
            "\n",
            "[![One large blue ball in mid air above many smaller blue, green, purple and white balls](/~/media/mckinsey/business%20functions/mckinsey%20digital/our%20insights/moving%20past%20gen%20ais%20honeymoon%20phase%20seven%20hard%20truths%20for%20cios/thumb-gettyimages-1551147077.jpg?cq=50&mw=767&car=16:9&cpy=Center)](/capabilities/mckinsey-digital/our-insights/moving-past-gen-ais-honeymoon-phase-seven-hard-truths-for-cios-to-get-from-pilot-to-scale)Article\n",
            "###### [Moving past gen AI’s honeymoon phase: Seven hard truths for CIOs to get from pilot to scale](/capabilities/mckinsey-digital/our-insights/moving-past-gen-ais-honeymoon-phase-seven-hard-truths-for-cios-to-get-from-pilot-to-scale)\n",
            "\n",
            "[![A thumb and an index finger form a circular void, resembling the shape of a light bulb but without the glass component. Inside this empty space, a bright filament and the gleaming metal base of the light bulb are visible.](/~/media/mckinsey/business%20functions/mckinsey%20digital/our%20insights/a%20generative%20ai%20reset%20rewiring%20to%20turn%20potential%20into%20value%20in%202024/qweb_gen-ai-reset_1536x1536.jpg?cq=50&mw=767&car=16:9&cpy=Center)](/capabilities/mckinsey-digital/our-insights/a-generative-ai-reset-rewiring-to-turn-potential-into-value-in-2024)Article - *McKinsey Quarterly*\n",
            "###### [A generative AI reset: Rewiring to turn potential into value in 2024](/capabilities/mckinsey-digital/our-insights/a-generative-ai-reset-rewiring-to-turn-potential-into-value-in-2024)\n",
            "\n",
            "[![High-tech bees buzz with purpose, meticulously arranging digital hexagonal cylinders into a precisely stacked formation.](/~/media/mckinsey/business%20functions/risk/our%20insights/implementing%20generative%20ai%20with%20speed%20and%20safety/qweb-implementing-generative-ai-with-speed-and-safety-1536x1536.jpg?cq=50&mw=767&car=16:9&cpy=Center)](/capabilities/risk-and-resilience/our-insights/implementing-generative-ai-with-speed-and-safety)Article - *McKinsey Quarterly*\n",
            "###### [Implementing generative AI with speed and safety](/capabilities/risk-and-resilience/our-insights/implementing-generative-ai-with-speed-and-safety)\n",
            "\n",
            "The state of AI in 2023: Generative AI’s breakout year | McKinsey\n",
            "[Skip to main content](#skipToMain)\n",
            "# The state of AI in 2023: Generative AI’s breakout year\n",
            "\n",
            "August 1, 2023 | SurveyAs organizations rapidly deploy generative AI tools, survey respondents expect significant effects on their industries and workforces.\n",
            "#### You have reached a page with older survey data. Please see our 2024 survey results [here](/capabilities/quantumblack/our-insights/the-state-of-ai).\n",
            "\n",
            "###\n",
            "\n",
            " [(24 pages)](#/download/%2F~%2Fmedia%2Fmckinsey%2Fbusiness%20functions%2Fquantumblack%2Four%20insights%2Fthe%20state%20of%20ai%20in%202023%20generative%20ais%20breakout%20year%2Fthe-state-of-ai-in-2023-generative-ais-breakout-year-v3.pdf%3FshouldIndex%3Dfalse)\n",
            "\n",
            "**The latest annual [McKinsey Global Survey](/featured-insights/mckinsey-global-surveys)** on the current state of AI confirms the explosive growth of [generative AI (gen AI) tools](/featured-insights/mckinsey-explainers/what-is-generative-ai). Less than a year after many of these tools debuted, one-third of our survey respondents say their organizations are using gen AI regularly in at least one business function. Amid recent advances, AI has risen from a topic relegated to tech employees to a focus of company leaders: nearly one-quarter of surveyed C-suite executives say they are personally using\n",
            "gen AI tools for work, and more than one-quarter of respondents from companies using AI say gen AI is already on their boards’ agendas. What’s more, 40 percent of respondents say their organizations will increase their investment in AI overall because of advances in gen AI. The findings show that these are still early days for managing gen AI–related risks, with less than half of respondents saying their organizations are mitigating even the risk they consider most relevant: inaccuracy.\n",
            "\n",
            "The organizations that have already embedded AI capabilities have been the first to explore gen AI’s potential, and those seeing the most value from more traditional AI capabilities—a group we call AI high performers—are already outpacing others in their [adoption of gen AI](/capabilities/people-and-organizational-performance/our-insights/gen-ais-next-inflection-point-from-employee-experimentation-to-organizational-transformation) tools.1We define AI high performers as organizations that, according to respondents, attribute at least 20 percent of their EBIT to AI adoption.\n",
            "\n",
            "The expected business disruption from gen AI is significant, and respondents predict meaningful changes to their workforces. They anticipate workforce cuts in certain areas and large reskilling efforts to address shifting talent needs. Yet while the use of gen AI might spur the adoption of other AI tools, we see few meaningful increases in organizations’ adoption of these technologies. The percent of organizations adopting any AI tools has held steady since 2022, and adoption remains concentrated within a small number of business functions.\n",
            "\n",
            "### Table of Contents\n",
            "\n",
            "1. [It’s early days still, but use of gen AI is already widespread](#widespread)\n",
            "2. [Leading companies are already ahead with gen AI](#leading)\n",
            "3. [AI-related talent needs shift, and AI’s workforce effects are expected to be substantial](#talent)\n",
            "4. [With all eyes on gen AI, AI adoption and impact remain steady](#steady)\n",
            "5. [About the research](#research)\n",
            "\n",
            "## 1. It’s early days still, but use of gen AI is already widespread\n",
            "\n",
            "The findings from the survey—which was in the field in mid-April 2023—show that, despite gen AI’s nascent public availability, experimentation with [the tools](/capabilities/quantumblack/our-insights/exploring-opportunities-in-the-generative-ai-value-chain) is already relatively common, and respondents expect the new capabilities to transform their industries. Gen AI has captured interest across the business population: individuals across regions, industries, and seniority levels are using gen AI for work and outside of work. Seventy-nine percent of all respondents say they’ve had at least some exposure to gen AI, either for work or outside of work, and 22 percent say they are regularly using it in their own work. While reported use is quite similar across seniority levels, it is highest among respondents working in the technology sector and those in North America.\n",
            "\n",
            "Interactive\n",
            "\n",
            "Organizations, too, are now commonly using gen AI. One-third of all respondents say their organizations are already regularly using generative AI in at least one function—meaning that 60 percent of organizations with reported AI adoption are using gen AI. What’s more, 40 percent of those reporting AI adoption at their organizations say their companies expect to invest more in AI overall thanks to generative AI, and 28 percent say generative AI use is already on their board’s agenda. The most commonly reported business functions using these newer tools are the same as those in which AI use is most common overall: marketing and sales, product and service development, and service operations, such as customer care and back-office support. This suggests that organizations are pursuing these new tools where the most value is. [In our previous research](/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier), these three areas, along with software engineering, showed the potential to deliver about 75 percent of the total annual value from generative AI use cases.\n",
            "\n",
            "![The most commonly reported uses of generative AI tools are in marketing and sales, product and service development, and service operations.](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai%20in%202023%20generative%20ais%20breakout%20year/svgz_stateofai2023_ex2.svgz?cq=50&cpy=Center)\n",
            "\n",
            "In these early days, expectations for [gen AI’s impact are high](/mgi/our-research/generative-ai-how-will-it-affect-future-jobs-and-workflows): three-quarters of all respondents expect gen AI to cause significant or disruptive change in the nature of their industry’s competition in the next three years. Survey respondents working in the technology and financial-services industries are the most likely to expect disruptive change from gen AI. [Our previous research shows](/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier) that, while all industries are indeed likely to see some degree of disruption, the level of impact is likely to vary.2“[The economic potential of generative AI: The next productivity frontier](/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier),” McKinsey, June 14, 2023. Industries relying most heavily on knowledge work are likely to see more disruption—and potentially reap more value. While our estimates suggest that tech companies, unsurprisingly, are poised to see the highest impact from gen AI—adding value equivalent to as much as 9 percent of global industry revenue—knowledge-based industries such as banking (up to 5 percent), pharmaceuticals and medical products (also up to 5 percent), and education (up to 4 percent) could experience significant effects as well. By contrast, manufacturing-based industries, such as aerospace, automotives, and advanced electronics, could experience less disruptive effects. This stands in contrast to the impact of previous technology waves that affected manufacturing the most and is due to gen AI’s strengths in language-based activities, as opposed to those requiring physical labor.\n",
            "\n",
            "### Responses show many organizations not yet addressing potential risks from gen AI\n",
            "\n",
            "According to the survey, few companies seem fully prepared for the widespread use of gen AI—or the business risks these tools may bring. Just 21 percent of respondents reporting AI adoption say their organizations have established policies governing employees’ use of gen AI technologies in their work. And when we asked specifically about the risks of adopting gen AI, few respondents say their companies are mitigating the most commonly cited risk with gen AI: inaccuracy. Respondents cite inaccuracy more frequently than both cybersecurity and regulatory compliance, which were the most common risks from AI overall in previous surveys. Just 32 percent say they’re mitigating inaccuracy, a smaller percentage than the 38 percent who say they mitigate cybersecurity risks. Interestingly, this figure is significantly lower than the percentage of respondents who reported mitigating AI-related cybersecurity last year (51 percent). Overall, much as we’ve seen in previous years, most respondents say their organizations are not addressing AI-related risks.\n",
            "\n",
            "![Inaccuracy, cybersecurity, and intellectual-property infringement are the most-cited risks of generative AI adoption.](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai%20in%202023%20generative%20ais%20breakout%20year/svgz_stateofai2023_ex3.svgz?cq=50&cpy=Center)\n",
            "\n",
            "## 2. Leading companies are already ahead with gen AI\n",
            "\n",
            "The survey results show that AI high performers—that is, organizations where respondents say at least 20 percent of EBIT in 2022 was attributable to AI use—are going all in on artificial intelligence, both with gen AI and more traditional AI capabilities. These organizations that achieve significant value from AI are already using gen AI in more business functions than other organizations do, especially in product and service development and risk and supply chain management. When looking at all AI capabilities—including more traditional machine learning capabilities, robotic process automation, and chatbots—AI high performers also are much more likely than others to use AI in product and service development, for uses such as product-development-cycle optimization, adding new features to existing products, and creating new AI-based products. These organizations also are using AI more often than other organizations in risk modeling and for uses within HR such as performance management and organization design and workforce deployment optimization.\n",
            "\n",
            "> AI high performers are much more likely than others to use AI in product and service development.\n",
            "\n",
            "Another difference from their peers: high performers’ gen AI efforts are less oriented toward cost reduction, which is a top priority at other organizations. Respondents from AI high performers are twice as likely as others to say their organizations’ top objective for gen AI is to create entirely new businesses or sources of revenue—and they’re *most* likely to cite the increase in the value of existing offerings through new AI-based features.\n",
            "\n",
            "![Smaller shares of AI high performers see cost reductions as their top objective for generative AI efforts. ](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai%20in%202023%20generative%20ais%20breakout%20year/svgz_stateofai2023_ex4.svgz?cq=50&cpy=Center)\n",
            "\n",
            "As we’ve seen [in previous years](/capabilities/quantumblack/our-insights/the-state-of-ai-in-2022-and-a-half-decade-in-review), these high-performing organizations invest much more than others in AI: respondents from AI high performers are more than five times more likely than others to say they spend more than 20 percent of their digital budgets on AI. They also use AI capabilities more broadly throughout the organization. Respondents from high performers are much more likely than others to say that their organizations have adopted AI in four or more business functions and that they have embedded a higher number of AI capabilities. For example, respondents from high performers more often report embedding knowledge graphs in at least one product or business function process, in addition to gen AI and related natural-language capabilities.\n",
            "\n",
            "While AI high performers are not immune to the challenges of capturing value from AI, the results suggest that the difficulties they face reflect their relative AI maturity, while others struggle with the more foundational, strategic elements of AI adoption. Respondents at AI high performers most often point to models and tools, such as monitoring model performance in production and retraining models as needed over time, as their top challenge. By comparison, other respondents cite strategy issues, such as setting a clearly defined AI vision that is linked with business value or finding sufficient resources.\n",
            "\n",
            "![Models and tools pose the biggest AI-related challenge for high performers, while strategy is a common stumbling block for others.](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai%20in%202023%20generative%20ais%20breakout%20year/svgz_stateofai2023_ex5.svgz?cq=50&cpy=Center)\n",
            "\n",
            "The findings offer further evidence that even high performers haven’t mastered best practices regarding AI adoption, such as machine-learning-operations (MLOps) approaches, though they are much more likely than others to do so. For example, just 35 percent of respondents at AI high performers report that where possible, their organizations assemble existing components, rather than reinvent them, but that’s a much larger share than the 19 percent of respondents from other organizations who report that practice.\n",
            "\n",
            "Many [specialized MLOps technologies and practices](/capabilities/quantumblack/our-insights/exploring-opportunities-in-the-generative-ai-value-chain) may be needed to adopt some of the more transformative uses cases that gen AI applications can deliver—and do so as safely as possible. Live-model operations is one such area, where monitoring systems and setting up instant alerts to enable rapid issue resolution can keep gen AI systems in check. High performers stand out in this respect but have room to grow: one-quarter of respondents from these organizations say their entire system is monitored and equipped with instant alerts, compared with just 12 percent of other respondents.\n",
            "\n",
            "## 3. AI-related talent needs shift, and AI’s workforce effects are expected to be substantial\n",
            "\n",
            "Our latest survey results show changes in the roles that organizations are filling to support their AI ambitions. In the past year, organizations using AI most often hired data engineers, machine learning engineers, and Al data scientists—all roles that respondents commonly reported hiring in the previous survey. But a much smaller share of respondents report hiring AI-related-software engineers—the most-hired role last year—than in the previous survey (28 percent in the latest survey, down from 39 percent). Roles in prompt engineering have recently emerged, as the need for that skill set rises alongside gen AI adoption, with 7 percent of respondents whose organizations have adopted AI reporting those hires in the past year.\n",
            "\n",
            "**The findings suggest that hiring for AI-related roles remains a challenge** but has become somewhat easier over the past year, which could reflect the spate of layoffs at technology companies from late 2022 through the first half of 2023. Smaller shares of respondents than in the previous survey report difficulty hiring for roles such as AI data scientists, data engineers, and data-visualization specialists, though responses suggest that hiring machine learning engineers and AI product owners remains as much of a challenge as in the previous year.\n",
            "\n",
            "![Hiring for AI-related roles remains a challenge, though reported difficulty has decreased since 2022 for many roles.](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai%20in%202023%20generative%20ais%20breakout%20year/svgz_stateofai2023_ex6.svgz?cq=50&cpy=Center)\n",
            "\n",
            "**Looking ahead to the next three years, respondents predict that the adoption of AI will reshape many roles** in the workforce. Generally, they expect more employees to be reskilled than to be separated. Nearly four in ten respondents reporting AI adoption expect more than 20 percent of their companies’ workforces will be reskilled, whereas 8 percent of respondents say the size of their workforces will decrease by more than 20 percent.\n",
            "\n",
            "![Survey respondents expect AI to meaningfully change their organizations’ workforces.](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai%20in%202023%20generative%20ais%20breakout%20year/svgz_stateofai2023_ex7.svgz?cq=50&cpy=Center)\n",
            "\n",
            "**Looking specifically at gen AI’s predicted impact,** service operations is the only function in which most respondents expect to see a decrease in workforce size at their organizations. This finding generally aligns with what [our recent research](/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier) suggests: while the emergence of gen AI increased our estimate of the percentage of worker activities that could be automated (60 to 70 percent, up from 50 percent), this doesn’t necessarily translate into the automation of an entire role.\n",
            "\n",
            "![Service operations is the only function in which most respondents expect to see a decrease in workforce size because of generative AI.](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai%20in%202023%20generative%20ais%20breakout%20year/svgz_stateofai2023_ex8.svgz?cq=50&cpy=Center)\n",
            "\n",
            "**AI high performers are expected to conduct much higher levels of reskilling** than other companies are. Respondents at these organizations are over three times more likely than others to say their organizations will reskill more than 30 percent of their workforces over the next three years as a result of AI adoption.\n",
            "\n",
            "![Respondents at AI high performers expect their organizations to reskill larger portions of the workforce than other respondents do.](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai%20in%202023%20generative%20ais%20breakout%20year/svgz_stateofai2023_ex9.svgz?cq=50&cpy=Center)\n",
            "\n",
            "## 4. With all eyes on gen AI, AI adoption and impact remain steady\n",
            "\n",
            "While the use of gen AI tools is spreading rapidly, the survey data doesn’t show that these newer tools are propelling organizations’ overall AI adoption. The share of organizations that have adopted AI overall remains steady, at least for the moment, with 55 percent of respondents reporting that their organizations have adopted AI. Less than a third of respondents continue to say that their organizations have adopted AI in more than one business function, suggesting that AI use remains limited in scope. Product and service development and service operations continue to be the two business functions in which respondents most often report AI adoption, as was true in the previous four surveys. And overall, just 23 percent of respondents say at least 5 percent of their organizations’ EBIT last year was attributable to their use of AI—essentially flat with the previous survey—suggesting there is much more room to capture value.\n",
            "\n",
            "![Less than one-third of respondents say their organizations use AI in more than one function—a share largely unchanged since 2021.](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai%20in%202023%20generative%20ais%20breakout%20year/svgz_stateofai2023_ex10.svgz?cq=50&cpy=Center)\n",
            "\n",
            "Organizations continue to see returns in the business areas in which they are using AI, and\n",
            "they plan to increase investment in the years ahead. We see a majority of respondents reporting AI-related revenue increases within each business function using AI. And looking ahead, more than two-thirds expect their organizations to increase their AI investment over the next three years.\n",
            "\n",
            "![Organizations continue to see benefits from AI adoption in the functions using AI capabilities.](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20ai%20in%202023%20generative%20ais%20breakout%20year/svgz_stateofai2023_ex11.svgz?cq=50&cpy=Center)\n",
            "\n",
            "## About the research\n",
            "\n",
            "The online survey was in the field April 11 to 21, 2023, and garnered responses from 1,684 participants representing the full range of regions, industries, company sizes, functional specialties, and tenures. Of those respondents, 913 said their organizations had adopted AI in at least one function and were asked questions about their organizations’ AI use. To adjust for differences in response rates, the data are weighted by the contribution of each respondent’s nation to global GDP.\n",
            "\n",
            "#####\n",
            "\n",
            "The survey content and analysis were developed by **Michael Chui**, a partner at the McKinsey Global Institute and a partner in McKinsey’s Bay Area office, where **[Lareina Yee](/our-people/lareina-yee)** is a senior partner; **[Bryce Hall](/our-people/bryce-hall)**, an associate partner in the Washington, DC, office; and senior partners **[Alex Singla](/our-people/alex-singla)** and **[Alexander Sukharevsky](/our-people/alexander-sukharevsky)**, global leaders of QuantumBlack, AI by McKinsey, based in the Chicago and London offices, respectively.\n",
            "\n",
            "They wish to thank Shivani Gupta, Abhisek Jena, Begum Ortaoglu, Barr Seitz, and Li Zhang for their contributions to this work.\n",
            "\n",
            "---\n",
            "\n",
            "This article was edited by Heather Hanselman, an editor in the Atlanta office.\n",
            "\n",
            "##### Explore a career with us\n",
            "\n",
            "[Search Openings](/careers/search-jobs)\n",
            "##### Related Articles\n",
            "\n",
            "[![The economic potential of generative AI: The next productivity frontier](/~/media/mckinsey/featured%20insights/mckinsey%20live/new/7-jun-2023.png?cq=50&mw=767&car=16:9&cpy=Center)](/featured-insights/mckinsey-live/webinars/the-economic-potential-of-generative-ai-the-next-productivity-frontier)Webcast\n",
            "###### [The economic potential of generative AI: The next productivity frontier](/featured-insights/mckinsey-live/webinars/the-economic-potential-of-generative-ai-the-next-productivity-frontier)\n",
            "\n",
            "[![A green apple split into 3 parts on a gray background. Half of the apple is made out of a digital blue wireframe mesh. ](/~/media/mckinsey/featured%20insights/mckinsey%20explainers/what%20is%20generative%20ai/generative-ai-1219474321-thumb-1536x1536-v3.jpg?cq=50&mw=767&car=16:9&cpy=Center)](/featured-insights/mckinsey-explainers/what-is-generative-ai)Article\n",
            "###### [What is generative AI?](/featured-insights/mckinsey-explainers/what-is-generative-ai)\n",
            "\n",
            "[![Circular hub element virtual reality of big data, technology concept.](/~/media/mckinsey/business%20functions/quantumblack/our%20insights/exploring%20opportunities%20in%20the%20generative%20ai%20value%20chain/thumb-gettyimages-1470671176.jpg?cq=50&mw=767&car=16:9&cpy=Center)](/capabilities/quantumblack/our-insights/exploring-opportunities-in-the-generative-ai-value-chain)Article\n",
            "###### [Exploring opportunities in the generative AI value chain](/capabilities/quantumblack/our-insights/exploring-opportunities-in-the-generative-ai-value-chain)\n",
            "\n",
            "Agentic AI – the new frontier in GenAI\n",
            "Skip to content\n",
            "[Skip to footer](#pgFooter)\n",
            "\n",
            "![Site Search](/etc.clientlibs/pwc/clientlibs/rebrand-clientlibs/components-colors/resources/images/slim-header-v2/search-icon.svg)\n",
            "\n",
            "Transforming our Region\n",
            "Industries\n",
            "Services\n",
            "Insights & Media\n",
            "Careers\n",
            "About us\n",
            "\n",
            "More\n",
            "\n",
            "Search\n",
            "\n",
            "Menu\n",
            "\n",
            "Transforming our Region\n",
            "\n",
            "[Transforming our Region](https://www.pwc.com/m1/en/publications.html)\n",
            "\n",
            "[Transforming our Region](https://www.pwc.com/m1/en/publications.html)\n",
            "[Sustainability](https://www.pwc.com/m1/en/sustainability.html)\n",
            "[Technology Consulting](https://www.pwc.com/m1/en/services/consulting/technology.html)\n",
            "[Managed Services](https://www.pwc.com/m1/en/services/managed-services.html)\n",
            "[Client stories](https://www.pwc.com/m1/en/about-us/client-stories.html)\n",
            "[Business model reinvention](https://www.pwc.com/m1/en/services/transformation/business-model-reinvention.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Transforming our Region\n",
            "\n",
            "[Transforming our Region](https://www.pwc.com/m1/en/publications.html)\n",
            "\n",
            "[Middle East Economy Watch](https://www.pwc.com/m1/en/publications/middle-east-economy-watch.html)\n",
            "[Transforming our region: webcast series](https://www.pwc.com/m1/en/transforming-our-region-middle-east-updates-webcast-series.html)\n",
            "[Middle East - How to do Business Guides](https://www.pwc.com/m1/en/services/tax/legal-services/pathfinder-doing-business-in-the-middle-east.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Transforming our Region\n",
            "\n",
            "[Sustainability](https://www.pwc.com/m1/en/sustainability.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Transforming our Region\n",
            "\n",
            "[Technology Consulting](https://www.pwc.com/m1/en/services/consulting/technology.html)\n",
            "\n",
            "[GenAI](https://www.pwc.com/m1/en/services/consulting/technology/generative-ai.html)\n",
            "[Cloud](https://www.pwc.com/m1/en/services/consulting/technology/cloud-transformation.html)\n",
            "[Cybersecurity and Digital Trust](https://www.pwc.com/m1/en/services/consulting/technology/cyber-security.html)\n",
            "[Data and analytics](https://www.pwc.com/m1/en/services/consulting/technology/data-and-analytics.html)\n",
            "[Emerging Technology](https://www.pwc.com/m1/en/services/consulting/technology/emerging-technology.html)\n",
            "[Enterprise Solutions](https://www.pwc.com/m1/en/services/consulting/technology/enterprise-solutions.html)\n",
            "[Artificial Intelligence](https://www.pwc.com/m1/en/services/consulting/technology/artificial-intelligence-everywhere.html)\n",
            "[Business applications](https://www.pwc.com/m1/en/services/consulting/technology/alliances.html)\n",
            "[Metaverse](https://www.pwc.com/gx/en/issues/technology/metaverse.html)\n",
            "[Public Safety](https://www.pwc.com/m1/en/services/consulting/technology/public-safety.html)\n",
            "[Technology Strategy](https://www.strategyand.pwc.com/m1/en/strategic-foresight/sector-strategies/technology.html)\n",
            "[Experience Consulting](https://www.pwc.com/m1/en/services/consulting/technology/experience-consulting.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Transforming our Region\n",
            "\n",
            "[Managed Services](https://www.pwc.com/m1/en/services/managed-services.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Transforming our Region\n",
            "\n",
            "[Client stories](https://www.pwc.com/m1/en/about-us/client-stories.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Transforming our Region\n",
            "\n",
            "[Business model reinvention](https://www.pwc.com/m1/en/services/transformation/business-model-reinvention.html)\n",
            "\n",
            "Featured\n",
            "\n",
            "[![](/m1/en/the-new-equation/trust-you-have-to-earn-it.jpg.pwcimage.150.100.jpg)\n",
            "\n",
            "The New Equation](https://www.pwc.com/m1/en/the-new-equation.html)\n",
            "\n",
            "[![](/m1/en/publications/images-new/geospatial-hero.jpg.pwcimage.150.100.jpg)\n",
            "\n",
            "Tech powered](https://www.pwc.com/m1/en/services/consulting/technology/tech-perspectives-series.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Industries\n",
            "\n",
            "[Industries](https://www.pwc.com/m1/en/industries.html)\n",
            "\n",
            "[Consumer Markets](https://www.pwc.com/m1/en/industries/consumer-markets.html)\n",
            "[Energy, Utilities & Resources](https://www.pwc.com/m1/en/industries/energy-utilities-resources.html)\n",
            "[Financial Services](https://www.pwc.com/m1/en/industries/financial-services.html)\n",
            "[Government & Public Services](https://www.pwc.com/m1/en/industries/government-public-services.html)\n",
            "[Industrial Manufacturing](https://www.pwc.com/m1/en/industries/manufacturing.html)\n",
            "[Transport & Logistics](https://www.pwc.com/m1/en/industries/transportation-and-logistics.html)\n",
            "[Health Industries](https://www.pwc.com/m1/en/industries/healthcare.html)\n",
            "[Real Estate](https://www.pwc.com/m1/en/industries/investment-management-real-estate.html)\n",
            "[Sovereign Investment Funds](https://www.pwc.com/m1/en/industries/sovereign-wealth-investment-funds.html)\n",
            "[Tourism & Hospitality](https://www.pwc.com/m1/en/industries/tourism-global-center-of-excellence.html)\n",
            "[Entrepreneurial & Private Business](https://www.pwc.com/m1/en/services/private-business.html)\n",
            "[Education and skills](https://www.pwc.com/m1/en/industries/education.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Industries\n",
            "\n",
            "[Consumer Markets](https://www.pwc.com/m1/en/industries/consumer-markets.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Industries\n",
            "\n",
            "[Energy, Utilities & Resources](https://www.pwc.com/m1/en/industries/energy-utilities-resources.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Industries\n",
            "\n",
            "[Financial Services](https://www.pwc.com/m1/en/industries/financial-services.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Industries\n",
            "\n",
            "[Government & Public Services](https://www.pwc.com/m1/en/industries/government-public-services.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Industries\n",
            "\n",
            "[Industrial Manufacturing](https://www.pwc.com/m1/en/industries/manufacturing.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Industries\n",
            "\n",
            "[Transport & Logistics](https://www.pwc.com/m1/en/industries/transportation-and-logistics.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Industries\n",
            "\n",
            "[Health Industries](https://www.pwc.com/m1/en/industries/healthcare.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Industries\n",
            "\n",
            "[Real Estate](https://www.pwc.com/m1/en/industries/investment-management-real-estate.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Industries\n",
            "\n",
            "[Sovereign Investment Funds](https://www.pwc.com/m1/en/industries/sovereign-wealth-investment-funds.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Industries\n",
            "\n",
            "[Tourism & Hospitality](https://www.pwc.com/m1/en/industries/tourism-global-center-of-excellence.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Industries\n",
            "\n",
            "[Entrepreneurial & Private Business](https://www.pwc.com/m1/en/services/private-business.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Industries\n",
            "\n",
            "[Education and skills](https://www.pwc.com/m1/en/industries/education.html)\n",
            "\n",
            "Featured\n",
            "\n",
            "[![](/gx/en/brand-simplified/hero-images-1600/transportation/damil-gettyimages-929485798-1600.jpg.pwcimage.150.100.jpg)\n",
            "\n",
            "Supply Chain & Spend Efficiency](https://www.pwc.com/m1/en/industries/supply-chain-spend-efficiency.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Services\n",
            "\n",
            "[Services](https://www.pwc.com/m1/en/services.html)\n",
            "\n",
            "[Audit and Assurance](https://www.pwc.com/m1/en/services/assurance.html)\n",
            "[Consulting](https://www.pwc.com/m1/en/services/consulting.html)\n",
            "[Deals](https://www.pwc.com/m1/en/services/deals.html)\n",
            "[Tax & Legal](https://www.pwc.com/m1/en/services/tax.html)\n",
            "[Strategy&](https://www.strategyand.pwc.com/m1/en.html)\n",
            "[Startups & Scaleups](https://www.pwc.com/m1/en/services/startups-scaleups.html)\n",
            "[Transformation](https://www.pwc.com/m1/en/services/transformation.html)\n",
            "[People & Organisation](https://www.pwc.com/m1/en/services/people-and-organisation.html)\n",
            "[PwC Academy](https://www.pwcacademy-me.com/)\n",
            "\n",
            "Menu\n",
            "\n",
            "Services\n",
            "\n",
            "[Audit and Assurance](https://www.pwc.com/m1/en/services/assurance.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Services\n",
            "\n",
            "[Consulting](https://www.pwc.com/m1/en/services/consulting.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Services\n",
            "\n",
            "[Deals](https://www.pwc.com/m1/en/services/deals.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Services\n",
            "\n",
            "[Tax & Legal](https://www.pwc.com/m1/en/services/tax.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Services\n",
            "\n",
            "[Strategy&](https://www.strategyand.pwc.com/m1/en.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Services\n",
            "\n",
            "[Startups & Scaleups](https://www.pwc.com/m1/en/services/startups-scaleups.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Services\n",
            "\n",
            "[Transformation](https://www.pwc.com/m1/en/services/transformation.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Services\n",
            "\n",
            "[People & Organisation](https://www.pwc.com/m1/en/services/people-and-organisation.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Services\n",
            "\n",
            "[PwC Academy](https://www.pwcacademy-me.com/)\n",
            "\n",
            "Featured\n",
            "\n",
            "[![](/m1/en/ceosurvey/2025/images/ceo-survey-banner.png.pwcimage.150.100.jpg)\n",
            "\n",
            "28th CEO Survey](https://www.pwc.com/m1/en/ceo-survey/28th-ceo-survey-middle-east-findings-2025.html)\n",
            "\n",
            "[![](/m1/en/services/assurance/perspectives-in-risk-series/images/perspectives-in-risk-series-thumb.png.pwcimage.150.100.jpg)\n",
            "\n",
            "Perspectives in Risk](https://www.pwc.com/m1/en/services/assurance/perspectives-in-risk-series.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Insights & Media\n",
            "\n",
            "[Insights & Media](https://www.pwc.com/m1/en/media-centre.html)\n",
            "\n",
            "[Publications](https://www.pwc.com/m1/en/publications.html)\n",
            "[Blogs & Articles](https://www.pwc.com/m1/en/media-centre/articles.html)\n",
            "[Press releases](https://www.pwc.com/m1/en/media-centre/press-releases.html)\n",
            "[Radio & TV interviews](https://www.pwc.com/m1/en/media-centre/tv-radio-interviews.html)\n",
            "[Videos](https://www.pwc.com/m1/en/media-centre/videos.html)\n",
            "[Events](https://www.pwc.com/m1/en/events.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Insights & Media\n",
            "\n",
            "[Publications](https://www.pwc.com/m1/en/publications.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Insights & Media\n",
            "\n",
            "[Blogs & Articles](https://www.pwc.com/m1/en/media-centre/articles.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Insights & Media\n",
            "\n",
            "[Press releases](https://www.pwc.com/m1/en/media-centre/press-releases.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Insights & Media\n",
            "\n",
            "[Radio & TV interviews](https://www.pwc.com/m1/en/media-centre/tv-radio-interviews.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Insights & Media\n",
            "\n",
            "[Videos](https://www.pwc.com/m1/en/media-centre/videos.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Insights & Media\n",
            "\n",
            "[Events](https://www.pwc.com/m1/en/events.html)\n",
            "\n",
            "Featured\n",
            "\n",
            "[![](/m1/en/ceosurvey/2025/images/ceo-survey-banner.png.pwcimage.150.100.jpg)\n",
            "\n",
            "28th CEO Survey](https://www.pwc.com/m1/en/ceo-survey/28th-ceo-survey-middle-east-findings-2025.html)\n",
            "\n",
            "[![](/m1/en/the-new-equation/trust-you-have-to-earn-it.jpg.pwcimage.150.100.jpg)\n",
            "\n",
            "The New Equation](https://www.pwc.com/m1/en/the-new-equation.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Careers\n",
            "\n",
            "[Careers](https://www.pwc.com/m1/en/careers.html)\n",
            "\n",
            "[Life at PwC](https://www.pwc.com/m1/en/about-us/life-at-pwc.html)\n",
            "[Experienced](https://www.pwc.com/m1/en/careers/experienced-jobs.html)\n",
            "[Graduate & Undergraduate](https://www.pwc.com/m1/en/careers/graduates-and-undergraduates-careers.html)\n",
            "[UAE - Watani Programme](https://www.pwc.com/m1/en/careers/uae-nationals-careers.html)\n",
            "[Alumni](https://www.pwc.com/m1/en/about-us/alumni-new.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Careers\n",
            "\n",
            "[Life at PwC](https://www.pwc.com/m1/en/about-us/life-at-pwc.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Careers\n",
            "\n",
            "[Experienced](https://www.pwc.com/m1/en/careers/experienced-jobs.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Careers\n",
            "\n",
            "[Graduate & Undergraduate](https://www.pwc.com/m1/en/careers/graduates-and-undergraduates-careers.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Careers\n",
            "\n",
            "[UAE - Watani Programme](https://www.pwc.com/m1/en/careers/uae-nationals-careers.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "Careers\n",
            "\n",
            "[Alumni](https://www.pwc.com/m1/en/about-us/alumni-new.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "About us\n",
            "\n",
            "[About us](https://www.pwc.com/m1/en/about-us.html)\n",
            "\n",
            "[The New Equation](https://www.pwc.com/m1/en/the-new-equation.html)\n",
            "[We as strategic buyer and investor](https://www.pwc.com/m1/en/about-us/we-as-strategic-buyer-and-investor.html)\n",
            "[Our leadership team](https://www.pwc.com/m1/en/about-us/our-leadership-team.html)\n",
            "[Ethics and compliance](https://www.pwc.com/m1/en/about-us/ethics-business-conduct.html)\n",
            "[Corporate Sustainability](https://www.pwc.com/m1/en/about-us/corporate-responsibility-pwc-me.html)\n",
            "[Net Zero with 2030 goals](https://www.pwc.com/m1/en/about-us/net-zero.html)\n",
            "[Transparency Reports](https://www.pwc.com/m1/en/about-us/transparency-reports.html)\n",
            "[Analyst relations](https://www.pwc.com/gx/en/about/analyst-relations.html)\n",
            "[PwC office locations in the Middle East](https://www.pwc.com/m1/en/about-us/office-locations-middle-east.html)\n",
            "[Alumni](https://www.pwc.com/m1/en/about-us/alumni-new.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "About us\n",
            "\n",
            "[The New Equation](https://www.pwc.com/m1/en/the-new-equation.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "About us\n",
            "\n",
            "[We as strategic buyer and investor](https://www.pwc.com/m1/en/about-us/we-as-strategic-buyer-and-investor.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "About us\n",
            "\n",
            "[Our leadership team](https://www.pwc.com/m1/en/about-us/our-leadership-team.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "About us\n",
            "\n",
            "[Ethics and compliance](https://www.pwc.com/m1/en/about-us/ethics-business-conduct.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "About us\n",
            "\n",
            "[Corporate Sustainability](https://www.pwc.com/m1/en/about-us/corporate-responsibility-pwc-me.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "About us\n",
            "\n",
            "[Net Zero with 2030 goals](https://www.pwc.com/m1/en/about-us/net-zero.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "About us\n",
            "\n",
            "[Transparency Reports](https://www.pwc.com/m1/en/about-us/transparency-reports.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "About us\n",
            "\n",
            "[Analyst relations](https://www.pwc.com/gx/en/about/analyst-relations.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "About us\n",
            "\n",
            "[PwC office locations in the Middle East](https://www.pwc.com/m1/en/about-us/office-locations-middle-east.html)\n",
            "\n",
            "Menu\n",
            "\n",
            "About us\n",
            "\n",
            "[Alumni](https://www.pwc.com/m1/en/about-us/alumni-new.html)\n",
            "\n",
            "Loading Results\n",
            "\n",
            "No Match Found\n",
            "\n",
            "View All Results\n",
            "\n",
            "# Agentic AI – the new frontier in GenAI\n",
            "\n",
            "Copy link\n",
            "\n",
            "Link copied to clipboard\n",
            "\n",
            "**GenAI is poised to make a significant economic impact, with estimates suggesting it could contribute between US$2.6 trillion and US$4.4 trillion annually to global GDP by 2030 across various sectors. The future of GenAI is agentic, where AI agents collaborate in real-time to automate complex tasks and enhance decision-making. This executive playbook explores how organisations can harness agentic AI to boost efficiency, improve customer experiences, and drive revenue growth.**\n",
            "\n",
            "## Key aspects of agentic AI\n",
            "\n",
            "![Space strategy](/gx/en/brand-simplified/picto-images-132/black/picto-automation-black.svg)\n",
            "\n",
            "Autonomy\n",
            "\n",
            "Autonomy Agentic AI systems can operate independently, making decisions based on their programming, learning, and environmental inputs.\n",
            "\n",
            "![Market research and analytics](/gx/en/brand-simplified/picto-images-132/black/picto-sales-black.svg)\n",
            "\n",
            "Goal-oriented behaviour\n",
            "\n",
            "These AI agents are designed to pursue specific objectives, optimising their actions to achieve the desired outcomes.\n",
            "\n",
            "![Socio-economic impact assessment](/gx/en/brand-simplified/picto-images-132/black/picto-environment-black.svg)\n",
            "\n",
            "Environment interaction\n",
            "\n",
            "An agentic AI interacts with its surroundings, perceiving changes and adapting its strategies accordingly.\n",
            "\n",
            "![Financial advisory](/gx/en/brand-simplified/picto-images-132/black/picto-deep-learning-black.svg)\n",
            "\n",
            "Learning capability\n",
            "\n",
            "Many agentic AI systems employ machine learning or reinforcement learning techniques to improve their performance over time.\n",
            "\n",
            "![Technology and innovation](/gx/en/brand-simplified/picto-images-132/black/picto-net-working-capital-black.svg)\n",
            "\n",
            "Workflow optimisation\n",
            "\n",
            "Agentic AI agents enhance workflows and business processes by integrating language understanding with reasoning, planning, and decision making. This involves optimising resource allocation, improving communication and collaboration, and identifying automation opportunities.\n",
            "\n",
            "![Governance and organisational structure](/gx/en/brand-simplified/picto-images-132/black/picto-multi-step-logic-black.svg)\n",
            "\n",
            "Multi-agent and system conversation\n",
            "\n",
            "Analysing governance and organisational structures and performing programmes and organisation performance reviews.\n",
            "\n",
            "## Download playbook here\n",
            "\n",
            "[Download](https://www.pwc.com/m1/en/publications/documents/2024/agentic-ai-the-new-frontier-in-genai-an-executive-playbook.pdf \"Download\")\n",
            "(PDF of 1.35mb)\n",
            "\n",
            "## Contact us\n",
            "\n",
            "Akif Kamal\n",
            "\n",
            "Partner, Technology Consulting, PwC Middle East\n",
            "\n",
            "+971 50 8734894\n",
            "\n",
            "[Email](/m1/en/content/pwc/global/forms/contactUsNew.html?parentPagePath=/content/pwc/m1/en/publications/agentic-ai-the-new-frontier-in-genai&style=pwc&territory=m1&contactLink=L2NvbnRlbnQvZGFtL3B3Yy9tMS9lbi9jb250ZW50LWZyYWdtZW50cy9jb250YWN0cy9hL2FraWYta2FtYWwtbg==&cf=true)\n",
            "\n",
            "Dr. Mohammad Tanvir Ansari\n",
            "\n",
            "Director, Technology Consulting, PwC Middle East\n",
            "\n",
            "+971 50 716 4679\n",
            "\n",
            "[Email](/m1/en/content/pwc/global/forms/contactUsNew.html?parentPagePath=/content/pwc/m1/en/publications/agentic-ai-the-new-frontier-in-genai&style=pwc&territory=m1&contactLink=L2NvbnRlbnQvZGFtL3B3Yy9tMS9lbi9jb250ZW50LWZyYWdtZW50cy9jb250YWN0cy9tL21vaGFtbWFkLXRhbnZpci1hbnNhcmk=&cf=true)\n",
            "\n",
            "## Contact us\n",
            "\n",
            "Rajat Chowdhary\n",
            "\n",
            "Partner, Technology Consulting, PwC Middle East\n",
            "\n",
            "Tel: +971 50 429 3733\n",
            "\n",
            "[![Linkedin Follow](data:image/gif;base64...)](https://www.linkedin.com/in/rajatchowdhary/)\n",
            " Email\n",
            "\n",
            "Sharang Gupta\n",
            "\n",
            "Director, Technology Consulting, PwC Middle East\n",
            "\n",
            "Tel: +971 50 432 6559\n",
            "\n",
            "[![Linkedin Follow](data:image/gif;base64...)](https://www.linkedin.com/in/sharang-gupta-6aa93621/)\n",
            " Email\n",
            "\n",
            "Vishesh Kalia\n",
            "\n",
            "Director, Technology Consulting, PwC Middle East\n",
            "\n",
            "Tel: + 971 56 520 3814\n",
            "\n",
            "[![Linkedin Follow](data:image/gif;base64...)](https://www.linkedin.com/in/vishesh-kalia-35577438/?originalSubdomain=ae)\n",
            " Email\n",
            "\n",
            "## Contact us\n",
            "\n",
            "![Fadi Komati](/m1/en/people/photos/fadi-komati.jpg.pwcimage.105.105.jpg)\n",
            "\n",
            "![Joseph  Abboud](/m1/en/people/photos/joseph-abboud-new.jpg.pwcimage.105.105.jpg)\n",
            "\n",
            "Joseph Abboud\n",
            "\n",
            "Technology Consulting, Partner, PwC Middle East\n",
            "\n",
            "[![Linkedin Follow](data:image/gif;base64...)](https://www.linkedin.com/in/joseph-abboud-63b248155/)\n",
            " Email\n",
            "\n",
            "![Wassim Mukaddam](/m1/en/people/photos/wassim-mukaddam.JPG.pwcimage.105.105.jpg)\n",
            "\n",
            "Wassim Mukaddam\n",
            "\n",
            "Technology Consulting, Director, PwC Middle East\n",
            "\n",
            " Email\n",
            "\n",
            "[Issues](/m1/en/issues.html)\n",
            "[Navigating data privacy regulations](/m1/en/services/consulting/technology/cyber-security/navigating-data-privacy-regulations.html)\n",
            "[New world. New skills.](/m1/en/issues/upskilling.html)\n",
            "[Responding to the potential business impacts of COVID-19](/m1/en/publications/covid-19.html)\n",
            "[Doing Business in the Middle East](/m1/en/services/tax/doing-business-guides-middle-east.html)\n",
            "[VAT in the Middle East](/m1/en/services/tax/vat-in-the-middle-east.html)\n",
            "[Megatrends Now: The need to ADAPT](/m1/en/issues/adapt.html)\n",
            "\n",
            "[Services](/m1/en/services.html)\n",
            "[Assurance and Audit](/m1/en/services/assurance.html)\n",
            "[Consulting](/m1/en/services/consulting.html)\n",
            "[Deals](/m1/en/services/deals.html)\n",
            "[Entrepreneurial & Private Business](/m1/en/services/private-business.html)\n",
            "[PwC Academy](https://www.pwcacademy-me.com/)\n",
            "[Tax and Legal](/m1/en/services/tax.html)\n",
            "[Strategy&](https://www.strategyand.pwc.com/m1/en.html)\n",
            "\n",
            "[Industries](/m1/en/industries.html)\n",
            "\n",
            "[Banking and Capital Markets](/m1/en/industries/banking-capital-markets.html)\n",
            "[Consumer Markets](/m1/en/industries/consumer-markets.html)\n",
            "[Energy, Utilities & Resources](https://www.pwc.com/m1/en/industries/energy-utilities-resources.html)\n",
            "[Financial Services](/m1/en/industries/financial-services.html)\n",
            "[Government/Public Services](/m1/en/industries/government-public-services.html)\n",
            "\n",
            "[Health Industries](/m1/en/industries/healthcare.html)\n",
            "[Industrial Manufacturing](/m1/en/industries/manufacturing.html)\n",
            "[Real Estate](/m1/en/industries/investment-management-real-estate.html)\n",
            "[Transportation and Logistics](/m1/en/industries/transportation-and-logistics.html)\n",
            "\n",
            "[About us](/m1/en/about-us.html)\n",
            "[PwC office locations in the Middle East](/m1/en/about-us/office-locations-middle-east.html)\n",
            "[Our purpose and values](/m1/en/about-us/purpose-and-values.html)\n",
            "[Corporate Sustainability](/m1/en/about-us/corporate-responsibility-pwc-me.html)\n",
            "[Ethics and compliance](/m1/en/about-us/ethics-business-conduct.html)\n",
            "[Transparency reports](/m1/en/about-us/transparency-reports.html)\n",
            "[Health, Safety and Environment Policy](/m1/en/about-us/health-safety-environment-policy.html)\n",
            "[Third party code of conduct](/m1/en/about-us/third-party-code-of-conduct.html)\n",
            "[PwC’s global human rights statement](https://www.pwc.com/m1/en/about-us/2024/document/pwc-human-rights-policy.pdf)\n",
            "[Alumni](/m1/en/about-us/alumni-new.html)\n",
            "\n",
            "[Publications](/m1/en/publications.html)\n",
            "\n",
            "[Careers](/m1/en/careers.html)\n",
            "[Graduate & undergraduate careers](/m1/en/careers/graduates.html)\n",
            "[Experienced careers](/m1/en/careers/experienced-jobs.html)\n",
            "[Opportunities for Omani nationals](/m1/en/careers/oman-nationals-careers.html)\n",
            "[Opportunities for Saudi nationals](/m1/en/careers/saudi-national-careers.html)\n",
            "[PwC's Watani Graduate Programme for UAE Nationals](/m1/en/careers/uae-nationals-careers.html)\n",
            "\n",
            "[Media centre](/m1/en/media-centre.html)\n",
            "[Press releases](/m1/en/media-centre/press-releases.html)\n",
            "[Articles and blog posts](/m1/en/media-centre/articles.html)\n",
            "[TV and radio interviews](/m1/en/media-centre/tv-radio-interviews.html)\n",
            "\n",
            "© 2017\n",
            "- 2025 PwC. All rights reserved. PwC refers to the PwC network and/or one or more of its member firms, each of which is a separate legal entity. Please see [www.pwc.com/structure](https://www.pwc.com/structure) for further details.\n",
            "\n",
            "* [Privacy](https://www.pwc.com/gx/en/legal-notices/pwc-privacy-statement.html)\n",
            "* [Cookie policy](/m1/en/cookie-information.html)\n",
            "* [Legal](/gx/en/site-information/legal-disclaimer.html)\n",
            "* [About site provider](/m1/en/home.html)\n",
            "* [Site map](/m1/en/sitemap.html)\n",
            "\n",
            "AI Index: State of AI in 13 Charts\n",
            "[Skip to main content](#main-content)\n",
            "[Skip to secondary navigation](#secondary-navigation)\n",
            "\n",
            "[Stanford University(link is external)](https://stanford.edu)\n",
            "\n",
            "[![Stanford HAI](/themes/hai/stanford_basic_hai/lockup.svg)](https://hai.stanford.edu/)\n",
            "\n",
            "Search this site\n",
            "\n",
            "Submit Search\n",
            "\n",
            "* [About](/about)\n",
            "  + [People](/about/people)\n",
            "  + [Values](/about/values)\n",
            "  + [Faculty Publications](/about/faculty-publications)\n",
            "  + [Corporate Programs](/about/corporate-programs)\n",
            "  + [Get Involved](/about/get-involved)\n",
            "* [Centers](/centers)\n",
            "  + [Center for Research on Foundation Models](https://crfm.stanford.edu/)\n",
            "  + [Center for the Study of Language and Information](https://www-csli.stanford.edu/)\n",
            "  + [Digital Economy Lab](https://digitaleconomy.stanford.edu/)\n",
            "  + [RAISE Health](https://med.stanford.edu/raisehealth)\n",
            "  + [RegLab](https://reglab.stanford.edu/)\n",
            "  + [The Stanford Robotics Center](https://src.stanford.edu/)\n",
            "  + [Initiatives](/centers/initiatives)\n",
            "  + [Partners](/centers/stanford-hai-partners)\n",
            "* [Research](/research)\n",
            "  + [Fellowship Programs](/research/fellowship-programs)\n",
            "  + [Grant Programs](/research/grant-programs)\n",
            "  + [AI Index Report](/research/ai-index-report)\n",
            "  + [Student Affinity Groups](/research/student-affinity-groups)\n",
            "* [Education](/education)\n",
            "  + [Professional Education](/education/professional-education)\n",
            "  + [Stanford Student Courses](/education/courses)\n",
            "* [Policy](/policy)\n",
            "  + [Policy Publications](/policy/policy-publications)\n",
            "  + [National AI Research Resource](/policy/national-ai-research-resource)\n",
            "  + [Congressional Boot Camp on AI](/congressional-boot-camp-ai)\n",
            "  + [Tech Ethics & Policy Summer Fellowships](/policy/tech-ethics-policy-summer-fellowships)\n",
            "  + [Tracking the U.S. AI Executive Order](/policy/tracking-us-ai-executive-order)\n",
            "* [News](/news)\n",
            "  + [Blog](/news/blog)\n",
            "  + [Announcements](/news/announcements)\n",
            "  + [Media Mentions](/news/media-mentions)\n",
            "  + [Subscribe to HAI Newsletter](/news/subscribe-hai-mailing-list)\n",
            "* [Events](/events)\n",
            "  + [Upcoming Events](/events/upcoming-events)\n",
            "  + [Past Events](/events/past-events)\n",
            "  + [HAI Seminars](/hai-weekly-seminars)\n",
            "\n",
            "Your browser does not support JavaScript! Please enable JavaScript in order to experience this site.\n",
            "Page Content\n",
            "[Economy and Markets](/taxonomy/term/58), [Language Processing](/taxonomy/term/68), [Law, Regulation, and Policy](/taxonomy/term/60)\n",
            "\n",
            "# AI Index: State of AI in 13 Charts\n",
            "\n",
            "In the new report, foundation models dominate, benchmarks fall, prices skyrocket, and on the global stage, the U.S. overshadows.\n",
            "\n",
            " Apr 15, 2024\n",
            "\n",
            "| Shana Lynch\n",
            "https://twitter.com/StanfordHAI?ref\\_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor\n",
            "https://www.facebook.com/StanfordHAI/\n",
            "<https://www.youtube.com/channel/UChugFTK0KyrES9terTid8vA>\n",
            "https://www.linkedin.com/company/stanfordhai\n",
            "<https://www.instagram.com/stanfordhai/?hl=en>\n",
            "\n",
            "Image\n",
            " ![Illustration of bright lines intersecting on a dark background](/sites/default/files/styles/media/public/2024-04/AIIndex_2024_StateofAI_BLOG_01.jpg?itok=dJQB96bo)\n",
            "\n",
            "This year’s [AI Index](https://aiindex.stanford.edu/report/) — a 500-page report tracking 2023’s worldwide trends in AI — is out.\n",
            "\n",
            "The index is an independent initiative at the [Stanford Institute for Human-Centered Artificial Intelligence](https://hai.stanford.edu/) (HAI), led by the AI Index Steering Committee, an interdisciplinary group of experts from across academia and industry. This year’s report covers the rise of multimodal foundation models, major cash investments into generative AI, new performance benchmarks, shifting global opinions, and new major regulations.\n",
            "\n",
            "Don’t have an afternoon to pore through the findings? Check out the high level here.\n",
            "\n",
            "![Pie chart showing 98 models were open-sourced in 2023](/sites/default/files/inline-images/AIIndex_2024_StateofAI_BLOG_02.jpg)\n",
            "## A Move Toward Open-Sourced\n",
            "\n",
            "This past year, organizations released 149 foundation models, more than double the number released in 2022. Of these newly released models, 65.7% were open-source (meaning they can be freely used and modified by anyone), compared with only 44.4% in 2022 and 33.3% in 2021.\n",
            "\n",
            "![bar chart showing that closed models outperformed open models across tasks](/sites/default/files/inline-images/AIIndex_2024_StateofAI_BLOG_07.jpg)\n",
            "## But At a Cost of Performance?\n",
            "\n",
            "Closed-source models still outperform their open-sourced counterparts. On 10 selected benchmarks, closed models achieved a median performance advantage of 24.2%, with differences ranging from as little as 4.0% on mathematical tasks like GSM8K to as much as 317.7% on agentic tasks like AgentBench.\n",
            "\n",
            "![Bar chart showing Google has more foundation models than any other company](/sites/default/files/inline-images/AIIndex_2024_StateofAI_BLOG_03.jpg)\n",
            "## Biggest Players\n",
            "\n",
            "Industry dominates AI, especially in building and releasing foundation models. This past year Google edged out other industry players in releasing the most models, including Gemini and RT-2. In fact, since 2019, Google has led in releasing the most foundation models, with a total of 40, followed by OpenAI with 20. Academia trails industry: This past year, UC Berkeley released three models and Stanford two.\n",
            "\n",
            "![Line chart showing industry far outpaces academia and government in creating foundation models over the decade](/sites/default/files/inline-images/AIIndex_2024_StateofAI_BLOG_14.jpg)\n",
            "## Industry Dwarfs All\n",
            "\n",
            "If you needed more striking evidence that corporate AI is the only player in the room right now, this should do it. In 2023, industry accounted for 72% of all new foundation models.\n",
            "\n",
            "![Chart showing the growing costs of training AI models](/sites/default/files/inline-images/AIIndex_2024_StateofAI_BLOG_04.jpg)\n",
            "## Prices Skyrocket\n",
            "\n",
            "One of the reasons academia and government have been edged out of the AI race: the exponential increase in cost of training these giant models. Google’s Gemini Ultra cost an estimated $191 million worth of compute to train, while OpenAI’s GPT-4 cost an estimated $78 million. In comparison, in 2017, the original Transformer model, which introduced the architecture that underpins virtually every modern LLM, cost around $900.\n",
            "\n",
            "![Bar chart showing the united states produces by far the largest number of foundation models](/sites/default/files/inline-images/AIIndex_2024_StateofAI_BLOG_05.jpg)\n",
            "## What AI Race?\n",
            "\n",
            "At least in terms of notable machine learning models, the United States vastly outpaced other countries in 2023, developing a total of 61 models in 2023. Since 2019, the U.S. has consistently led in originating the majority of notable models, followed by China and the UK.\n",
            "\n",
            "![Line chart showing that across many intellectual task categories, AI has exceeded human performance](/sites/default/files/inline-images/AIIndex_2024_StateofAI_BLOG_06.jpg)\n",
            "## Move Over, Human\n",
            "\n",
            "As of 2023, AI has hit human-level performance on many significant AI benchmarks, from those testing reading comprehension to visual reasoning. Still, it falls just short on some benchmarks like competition-level math. Because AI has been blasting past so many standard benchmarks, AI scholars have had to create new and more difficult challenges. This year’s index also tracked several of these new benchmarks, including those for tasks in coding, advanced reasoning, and agentic behavior.\n",
            "\n",
            "![Bar chart showing a dip in overall private investment in AI, but a surge in generative AI investment](/sites/default/files/inline-images/AIIndex_2024_StateofAI_BLOG_08.jpg)\n",
            "## Private Investment Drops (But We See You, GenAI)\n",
            "\n",
            "While AI private investment has steadily dropped since 2021, generative AI is gaining steam. In 2023, the sector attracted $25.2 billion, nearly ninefold the investment of 2022 and about 30 times the amount from 2019 (call it the ChatGPT effect). Generative AI accounted for over a quarter of all AI-related private investments in 2023.\n",
            "\n",
            "![Bar chart showing the united states overwhelming dwarfs other countries in private investment in AI](/sites/default/files/inline-images/AIIndex_2024_StateofAI_BLOG_09.jpg)\n",
            "## U.S. Wins $$ Race\n",
            "\n",
            "And again, in 2023 the United States dominates in AI private investment. In 2023, the $67.2 billion invested in the U.S. was roughly 8.7 times greater than the amount invested in the next highest country, China, and 17.8 times the amount invested in the United Kingdom. That lineup looks the same when zooming out: Cumulatively since 2013, the United States leads investments at $335.2 billion, followed by China with $103.7 billion, and the United Kingdom at $22.3 billion.\n",
            "\n",
            "![Infographic showing 26% of businesses use AI for contact-center automation, and 23% use it for personalization](/sites/default/files/inline-images/AIIndex_2024_StateofAI_BLOG_10.jpg)\n",
            "## Where is Corporate Adoption?\n",
            "\n",
            "More companies are implementing AI in some part of their business: In surveys, 55% of organizations said they were using AI in 2023, up from 50% in 2022 and 20% in 2017. Businesses report using AI to automate contact centers, personalize content, and acquire new customers.\n",
            "\n",
            "![Bar chart showing 57% of people believe AI will change how they do their job in 5 years, and 36% believe AI will replace their jobs.](/sites/default/files/inline-images/AIIndex_2024_StateofAI_BLOG_12.jpg)\n",
            "## Younger and Wealthier People Worry About Jobs\n",
            "\n",
            "Globally, most people expect AI to change their jobs, and more than a third expect AI to replace them. Younger generations — Gen Z and millennials — anticipate more substantial effects from AI compared with older generations like Gen X and baby boomers. Specifically, 66% of Gen Z compared with 46% of boomer respondents believe AI will significantly affect their current jobs. Meanwhile, individuals with higher incomes, more education, and decision-making roles foresee AI having a great impact on their employment.\n",
            "\n",
            "![Bar chart depicting the countries most nervous about AI; Australia at 69%, Great Britain at 65%, and Canada at 63% top the list](/sites/default/files/inline-images/AIIndex_2024_StateofAI_BLOG_13.jpg)\n",
            "## While the Commonwealth Worries About AI Products\n",
            "\n",
            "When asked in a survey about whether AI products and services make you nervous, 69% of Aussies and 65% of Brits said yes. Japan is the least worried about their AI products at 23%.\n",
            "\n",
            "![Line graph showing uptick in AI regulation in the united states since 2016; 25 policies passed in 2023](/sites/default/files/inline-images/AIIndex_2024_StateofAI_BLOG_11.jpg)\n",
            "## Regulation Rallies\n",
            "\n",
            "More American regulatory agencies are passing regulations to protect citizens and govern the use of AI tools and data. For example, the Copyright Office and the Library of Congress passed copyright registration guidance concerning works that contained material generated by AI, while the Securities and Exchange Commission developed a cybersecurity risk management strategy, governance, and incident disclosure plan. The agencies to pass the most regulation were the Executive Office of the President and the Commerce Department.\n",
            "\n",
            "*The* [*AI Index*](https://aiindex.stanford.edu/report/) *was first created to track AI development. The index collaborates with such organizations as LinkedIn, Quid, McKinsey, Studyportals, the Schwartz Reisman Institute, and the International Federation of Robotics to gather the most current research and feature important insights on the AI ecosystem.*\n",
            "\n",
            "## More News Topics\n",
            "\n",
            "[Economy and Markets](/taxonomy/term/58), [Language Processing](/taxonomy/term/68), [Law, Regulation, and Policy](/taxonomy/term/60), [Machine Learning](/taxonomy/term/69)\n",
            "\n",
            "[![Stanford HAI](/themes/hai/stanford_basic_hai/src/assets/img/hai-logo-vertical-sm.png)](/)\n",
            "\n",
            "## Navigate\n",
            "\n",
            "* [Welcome](/navigate/welcome)\n",
            "* [Values](/about/values)\n",
            "* [News](/news)\n",
            "* [Events](/events)\n",
            "* [Careers](/about/careers)\n",
            "\n",
            "## Participate\n",
            "\n",
            "* [Get Involved](/about/get-involved)\n",
            "* [Grant Programs](/research/grant-programs)\n",
            "* [Corporate Programs](/about/corporate-programs)\n",
            "* [Support HAI](/participate/support-hai)\n",
            "* [Contact Us](/contact-us)\n",
            "\n",
            "## Follow us\n",
            "\n",
            "<https://twitter.com/StanfordHAI>\n",
            "<https://www.facebook.com/StanfordHAI>\n",
            "<https://www.youtube.com/channel/UChugFTK0KyrES9terTid8vA>\n",
            "<https://www.linkedin.com/company/stanfordhai>\n",
            "<https://www.instagram.com/stanfordhai>\n",
            "\n",
            "### Newsletter Sign Up\n",
            "\n",
            "Don’t miss out. Get Stanford HAI updates delivered directly to your inbox.\n",
            "\n",
            "[Subscribe](/join-hai-community)\n",
            "\n",
            "[Stanford\n",
            "University](https://www.stanford.edu)\n",
            "\n",
            "* [Stanford Home(link is external)](https://www.stanford.edu)\n",
            "* [Maps & Directions(link is external)](https://visit.stanford.edu/plan/)\n",
            "* [Search Stanford(link is external)](https://www.stanford.edu/search/)\n",
            "* [Emergency Info(link is external)](https://emergency.stanford.edu)\n",
            "\n",
            "* [Terms of Use(link is external)](https://www.stanford.edu/site/terms/ \"Terms of use for sites\")\n",
            "* [Privacy(link is external)](https://www.stanford.edu/site/privacy/ \"Privacy and cookie policy\")\n",
            "* [Copyright(link is external)](https://uit.stanford.edu/security/copyright-infringement \"Report alleged copyright infringement\")\n",
            "* [Trademarks(link is external)](https://adminguide.stanford.edu/chapter-1/subchapter-5/policy-1-5-4 \"Ownership and use of Stanford trademarks and images\")\n",
            "* [Non-Discrimination(link is external)](https://studentservices.stanford.edu/more-resources/student-policies/non-academic/non-discrimination \"Non-discrimination policy\")\n",
            "* [Accessibility(link is external)](https://www.stanford.edu/site/accessibility \"Report web accessibility issues\")\n",
            "\n",
            "© Stanford University.\n",
            "  Stanford, California 94305.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}