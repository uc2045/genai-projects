{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uc2045/genai-projects/blob/master/030825_Tools_and_Tool_Calling_by_LLMs_with_LangChain_Mar2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring Tools in LangChain"
      ],
      "metadata": {
        "id": "-CVPAiNAy9MH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install OpenAI, and LangChain dependencies"
      ],
      "metadata": {
        "id": "L1KvMtf54l0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.3.19\n",
        "!pip install langchain-openai==0.3.8\n",
        "!pip install langchain-community==0.3.19"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2evPp14fy258",
        "outputId": "08693225-0cf0-4915-e247-d99cf079176b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.3.19\n",
            "  Downloading langchain-0.3.19-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.35 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.19) (0.3.43)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.19) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.19) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.19) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.19) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.19) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.19) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.19) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.19) (9.0.0)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.19) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.19) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.19) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.19) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.19) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.19) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.19) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.19) (1.18.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain==0.3.19) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain==0.3.19) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain==0.3.19) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.19) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.19) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.19) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.19) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.19) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.19) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.19) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.19) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.19) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.19) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.19) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.19) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.19) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.19) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain==0.3.19) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.19) (1.3.1)\n",
            "Downloading langchain-0.3.19-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.20\n",
            "    Uninstalling langchain-0.3.20:\n",
            "      Successfully uninstalled langchain-0.3.20\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-community 0.3.19 requires langchain<1.0.0,>=0.3.20, but you have langchain 0.3.19 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-0.3.19\n",
            "Requirement already satisfied: langchain-openai==0.3.8 in /usr/local/lib/python3.11/dist-packages (0.3.8)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.42 in /usr/local/lib/python3.11/dist-packages (from langchain-openai==0.3.8) (0.3.43)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.11/dist-packages (from langchain-openai==0.3.8) (1.61.1)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai==0.3.8) (0.9.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (0.3.11)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (4.12.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (2.10.6)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai==0.3.8) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai==0.3.8) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai==0.3.8) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai==0.3.8) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai==0.3.8) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai==0.3.8) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.3.8) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.3.8) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain-openai==0.3.8) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai==0.3.8) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai==0.3.8) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai==0.3.8) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.3.8) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.3.8) (2.3.0)\n",
            "Requirement already satisfied: langchain-community==0.3.19 in /usr/local/lib/python3.11/dist-packages (0.3.19)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.19) (0.3.43)\n",
            "Collecting langchain<1.0.0,>=0.3.20 (from langchain-community==0.3.19)\n",
            "  Using cached langchain-0.3.20-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.19) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.19) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.19) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.19) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.19) (9.0.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.19) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.19) (2.8.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.19) (0.3.11)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.19) (0.4.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.19) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.19) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.19) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.19) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.19) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.19) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.19) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.19) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.19) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.19) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.20->langchain-community==0.3.19) (0.3.6)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.20->langchain-community==0.3.19) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain-community==0.3.19) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain-community==0.3.19) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain-community==0.3.19) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community==0.3.19) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community==0.3.19) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community==0.3.19) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community==0.3.19) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.19) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.3.19) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.3.19) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.3.19) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.3.19) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.3.19) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community==0.3.19) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community==0.3.19) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community==0.3.19) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain-community==0.3.19) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.20->langchain-community==0.3.19) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.20->langchain-community==0.3.19) (2.27.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.19) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community==0.3.19) (1.3.1)\n",
            "Using cached langchain-0.3.20-py3-none-any.whl (1.0 MB)\n",
            "Installing collected packages: langchain\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.19\n",
            "    Uninstalling langchain-0.3.19:\n",
            "      Successfully uninstalled langchain-0.3.19\n",
            "Successfully installed langchain-0.3.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Data Extraction APIs"
      ],
      "metadata": {
        "id": "TlfidBdQZRGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to create custom tools\n",
        "!pip install wikipedia==1.4.0\n",
        "# to highlight json\n",
        "!pip install rich"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZKQDgQURhmF",
        "outputId": "9836785a-c9f0-4bbb-cdb3-a05506923b6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia==1.4.0\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from wikipedia==1.4.0) (4.13.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wikipedia==1.4.0) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (2025.1.31)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia==1.4.0) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia==1.4.0) (4.12.2)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11679 sha256=c81eb864c8fdf12eebcf5f90dc62547cb3f4c4fe68d09b0e3b0200b80df8af05\n",
            "  Stored in directory: /root/.cache/pip/wheels/8f/ab/cb/45ccc40522d3a1c41e1d2ad53b8f33a62f394011ec38cd71c6\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (13.9.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enter Open AI API Key"
      ],
      "metadata": {
        "id": "H9c37cLnSrbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "OPENAI_KEY = getpass('Enter Open AI API Key: ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cv3JzCEx_PAd",
        "outputId": "ce08f81b-2f9e-4151-92a8-8d61bd457a44"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Open AI API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enter Tavily Search API Key\n",
        "\n",
        "Get a free API key from [here](https://tavily.com/#api)"
      ],
      "metadata": {
        "id": "ucWRRI3QztL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TAVILY_API_KEY = getpass('Enter Tavily Search API Key: ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mK-1WLzOrJdb",
        "outputId": "30af9dd8-2869-4ce2-9e77-26261ca9b508"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Tavily Search API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enter WeatherAPI API Key\n",
        "\n",
        "Get a free API key from [here](https://www.weatherapi.com/signup.aspx)"
      ],
      "metadata": {
        "id": "Ce5arICZEEov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WEATHER_API_KEY = getpass('Enter WeatherAPI API Key: ')"
      ],
      "metadata": {
        "id": "XpAMz1XgEEov",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ccbe5e1-aeae-4aa0-ca55-5131f2fefe4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter WeatherAPI API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Environment Variables"
      ],
      "metadata": {
        "id": "1T0s0um5Svfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_KEY\n",
        "os.environ['TAVILY_API_KEY'] = TAVILY_API_KEY"
      ],
      "metadata": {
        "id": "x1YSuHNF_lbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring Built-in Tools"
      ],
      "metadata": {
        "id": "65C3PellZGYf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring the Wikipedia Tool\n",
        "\n",
        "Enables you to tap into the Wikipedia API to search wikipedia pages for information"
      ],
      "metadata": {
        "id": "howf-v0ARWbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools import WikipediaQueryRun\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "\n",
        "wiki_api_wrapper = WikipediaAPIWrapper(top_k_results=3,\n",
        "                                       doc_content_chars_max=8000)\n",
        "wiki_tool = WikipediaQueryRun(api_wrapper=wiki_api_wrapper, features=\"lxml\")"
      ],
      "metadata": {
        "id": "q2CMhK9Rjk2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_tool.description"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "t1Ce8wbYodYO",
        "outputId": "01194ae6-1818-4f56-c3bf-050e79c788df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_tool.args"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2MSVAh2osSE",
        "outputId": "ec48aaf1-4e10-4390-b61f-9a698bdd1642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': {'description': 'query to look up on wikipedia',\n",
              "  'title': 'Query',\n",
              "  'type': 'string'}}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wiki_tool.invoke({\"query\": \"Microsoft\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luhjlzeSkgUq",
        "outputId": "0fff01e9-7441-4799-8d49-eccc80c0f75e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page: Microsoft\n",
            "Summary: Microsoft Corporation is an American multinational technology conglomerate headquartered in Redmond, Washington. Founded in 1975, the company became highly influential in the rise of personal computers through software like Windows, and the company has since expanded to Internet services, cloud computing, video gaming and other fields. Microsoft is the largest software maker, one of the most valuable public U.S. companies, and one of the most valuable brands globally.\n",
            "Microsoft was founded by Bill Gates and Paul Allen to develop and sell BASIC interpreters for the Altair 8800. It rose to dominate the personal computer operating system market with MS-DOS in the mid-1980s, followed by Windows. During the 41 years from 1980 to 2021 Microsoft released 9 versions of MS-DOS with a median frequency of 2 years, and 13 versions of Windows with a median frequency of 3 years. The company's 1986 initial public offering (IPO) and subsequent rise in its share price created three billionaires and an estimated 12,000 millionaires among Microsoft employees. Since the 1990s, it has increasingly diversified from the operating system market. Steve Ballmer replaced Gates as CEO in 2000.  He oversaw the then-largest of Microsoft's corporate acquisitions in Skype Technologies in 2011, and an increased focus on hardware that led to its first in-house PC line, the Surface, in 2012, and the formation of Microsoft Mobile through Nokia. Since Satya Nadella took over as CEO in 2014, the company has changed focus towards cloud computing, as well as its large acquisition of LinkedIn for $26.2 billion in 2016. Under Nadella's direction, the company has also expanded its video gaming business to support the Xbox brand, establishing the Microsoft Gaming division in 2022 and acquiring Activision Blizzard for $68.7 billion in 2023.\n",
            "Microsoft has been market-dominant in the IBM PC–compatible operating system market and the office software suite market since the 1990s. Its best-known software products are the Windows line of operating systems and the Microsoft Office and Microsoft 365 suite of productivity applications, which most notably include the Word word processor and Excel spreadsheet editor. Its flagship hardware products are the Surface lineup of personal computers and Xbox video game consoles, the latter of which includes the Xbox network; the company also provides a range of consumer Internet services such as Bing web search, the MSN web portal, the Outlook.com email service and the Microsoft Store. In the enterprise and development fields, Microsoft most notably provides the Azure cloud computing platform, Microsoft SQL Server database software, and Visual Studio.\n",
            "Microsoft is considered one of the Big Five American information technology companies, alongside Alphabet, Amazon, Apple, and Meta. In April 2019, Microsoft reached a trillion-dollar market cap, becoming the third public U.S. company to be valued at over $1 trillion. It has been criticized for its monopolistic practices, and the company's software has been criticized for problems with ease of use, robustness, and security.\n",
            "\n",
            "\n",
            "\n",
            "Page: Microsoft Office\n",
            "Summary: Microsoft Office, MS Office, or simply Office, is an office suite and family of client software, server software, and services developed by Microsoft. The first version of the Office suite, announced by Bill Gates on August 1, 1988 at COMDEX, contained Microsoft Word, Microsoft Excel, and Microsoft PowerPoint — all three of which remain core products in Office — and over time Office applications have grown substantially closer with shared features such as a common spell checker, Object Linking and Embedding data integration and Visual Basic for Applications scripting language. Microsoft also positions Office as a development platform for line-of-business software under the Office Business Applications brand.\n",
            "The suite currently includes a word processor (Word), a spreadsheet program (Excel), a presentation program (PowerPoint), a notetaking program (OneNote), an email client (Outlook) and a file-hosting service client (OneDrive). The Windows version includes a database management system (Access). Office is produced in several versions targeted towards different end-users and computing environments. The original, and most widely used version, is the desktop version, available for PCs running the Windows and macOS operating systems, and sold at retail or under volume licensing. Microsoft also maintains mobile apps for Android and iOS, as well as Office on the web, a version of the software that runs within a web browser, which are offered freely.\n",
            "Since Office 2013, Microsoft has promoted Office 365 as the primary means of obtaining Microsoft Office: it allows the use of the software and other services on a subscription business model, and users receive feature updates to the software for the lifetime of the subscription, including new features and cloud computing integration that are not necessarily included in the \"on-premises\" releases of Office sold under conventional license terms. In 2017, revenue from Office 365 overtook conventional license sales. Microsoft also rebranded most of their standard Office 365 editions as \"Microsoft 365\" to reflect their inclusion of features and services beyond the core Microsoft Office suite. Although Microsoft announced that it was to phase out the Microsoft Office brand in favor of Microsoft 365 by 2023, with the name continuing only for legacy product offerings, later that year it reversed this decision and announced Office 2024, which they released in September 2024.\n",
            "\n",
            "Page: Microsoft Excel\n",
            "Summary: Microsoft Excel is a spreadsheet editor developed by Microsoft for Windows, macOS, Android, iOS and iPadOS. It features calculation or computation capabilities, graphing tools, pivot tables, and a macro programming language called Visual Basic for Applications (VBA). Excel forms part of the Microsoft 365 and Microsoft Office suites of software and has been developed since 1985.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " You can customize the default tool with its own name, description and so on as follows"
      ],
      "metadata": {
        "id": "LnfMeoXJVn-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import Tool\n",
        "\n",
        "wiki_tool_init = Tool(name=\"Wikipedia\",\n",
        "                      func=wiki_api_wrapper.run,\n",
        "                      description=\"useful when you need a detailed answer about general knowledge\")"
      ],
      "metadata": {
        "id": "1tO5g9Q1jk7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_tool_init.description"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZRTfVUuGo7ti",
        "outputId": "a3505751-14ac-463f-b46c-be09444974fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'useful when you need a detailed answer about general knowledge'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_tool_init.args"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TW0emMEIou-L",
        "outputId": "3d24811f-a71b-4232-8f0f-1ec573c96c0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'tool_input': {'type': 'string'}}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wiki_tool_init.invoke({\"tool_input\": \"AI\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgAwdPrBjk-x",
        "outputId": "6b2e72cd-6c9b-403c-9177-781bc7bb4815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page: Artificial intelligence\n",
            "Summary: Artificial intelligence (AI) refers to the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. Such machines may be called AIs.\n",
            "High-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., ChatGPT and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"\n",
            "Various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics. General intelligence—the ability to complete any task performed by a human on an at least equal level—is among the field's long-term goals. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.\n",
            "Artificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters. Funding and interest vastly increased after 2012 when deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture, and by the early 2020s many billions of dollars were being invested in AI and the field experienced rapid ongoing progress in what has become known as the AI boom. The emergence of advanced generative AI in the midst of the AI boom and its ability to create and modify content exposed several unintended consequences and harms in the present and raised concerns about the risks of AI and its long-term effects in the future, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.\n",
            "\n",
            "\n",
            "\n",
            "Page: .ai\n",
            "Summary: .ai is the Internet country code top-level domain (ccTLD) for Anguilla, a British Overseas Territory in the Caribbean. It is administered by the government of Anguilla.\n",
            "It is a popular domain hack with companies and projects related to the artificial intelligence industry (AI).\n",
            "Google's ad targeting treats .ai as a generic top-level domain (gTLD) because \"users and website owners frequently see [the domain] as being more generic than country-targeted.\"\n",
            "Identity Digital began managing the domain as of January 2025.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring the Tavily Search Tool\n",
        "\n",
        "Tavily Search API is a search engine optimized for LLMs and RAG, aimed at efficient, quick and persistent search results"
      ],
      "metadata": {
        "id": "cnQfnkQeV7Hp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "tavily_tool = TavilySearchResults(max_results=5,\n",
        "                                search_depth='advanced',\n",
        "                                include_raw_content=False)"
      ],
      "metadata": {
        "id": "UjWM95p5pB4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tavily_tool.args"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmawzibmjlC6",
        "outputId": "7e3102b3-931b-4cc2-b8e3-dbd39d838f8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': {'description': 'search query to look up',\n",
              "  'title': 'Query',\n",
              "  'type': 'string'}}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tavily_tool.description"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eERoFFoPjlGJ",
        "outputId": "cf5a87c5-0c26-4ec3-f2d1-f4a28e71e13a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = tavily_tool.invoke(\"Latest LLMs 2025\")\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khqWQVcvrnG9",
        "outputId": "ba5fc02d-f556-4c8e-d3ea-0a90ca718e20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'title': 'Best 39 Large Language Models (LLMs) in 2025 - Exploding Topics',\n",
              "  'url': 'https://explodingtopics.com/blog/list-of-llms',\n",
              "  'content': \"Inflection-2.5 is the latest large language model (LLM) developed by Inflection AI to power its conversational AI assistant, Pi. Significant upgrades have been made, as the model currently achieves over 94% of GPT-4’s average performance while only having 40% of the training FLOPs. In March 2024, the Microsoft-backed startup reached 1+ million daily active users on Pi. 13. Gemma is a series of lightweight open-source language models developed and released by Google DeepMind. Pythia is a series of 16 large language models developed and released by EleutherAI, a non-profit AI research lab. Alpaca is a 7 billion-parameter language model developed by a Stanford research team and fine-tuned from Meta's LLaMA 7B model.\",\n",
              "  'score': 0.8106142},\n",
              " {'title': '25 of the best large language models in 2025 - TechTarget',\n",
              "  'url': 'https://www.techtarget.com/whatis/feature/12-of-the-best-large-language-models',\n",
              "  'content': \"Large language models are the dynamite behind the\\xa0generative AI\\xa0boom. Some of the most well-known language models today are based on the transformer model, including the\\xa0generative pre-trained transformer series\\xa0of LLMs and bidirectional encoder representations from transformers (BERT). Gemma\\xa0is a family of open-source language models from Google that were trained on the same resources as Gemini. GPT-3\\xa0is OpenAI's large language model with more than 175 billion parameters, released in 2020. Large Language Model Meta AI (Llama) is Meta's LLM which was first released in 2023. The\\xa0Pathways Language Model\\xa0is a 540 billion parameter transformer-based model from Google powering its AI chatbot\\xa0Bard. StableLM is a series of open language models developed by Stability AI, the company behind image generator Stable Diffusion.\",\n",
              "  'score': 0.7892503},\n",
              " {'title': 'Top 9 Large Language Models as of March 2025 | Shakudo',\n",
              "  'url': 'https://www.shakudo.io/blog/top-9-large-language-models',\n",
              "  'content': 'The latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data. The latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data. The latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data.',\n",
              "  'score': 0.734139},\n",
              " {'title': 'The Best LLMs for Enhanced Language Processing in 2025 - ELEKS',\n",
              "  'url': 'https://eleks.com/blog/best-llms-for-language-processing/',\n",
              "  'content': 'LLM apps are applications that use large language models (LLMs) and AI models to perform various tasks, including language translation, content generation, and other language processing tasks. Article Understanding Agentic AI: Benefits, Applications, and Future Trends View article Article Essential Guide to LLMOps: Key Insights and Implementation Strategies View article Article Strategic Technology in 2025: An Expert Assessment of Market Predictions View article Case studies Enhancing Customer Support Efficiency with AI-Powered Knowledge Management View article Article Supervised vs Unsupervised Learning: Differences, Applications, and Market Trends View article Article Nuclear Power Plants for AI Data Centres: a Solution to Growing Energy Challenge View article Article Enhancing Patient Experience in Healthcare: The Role of Experience Platforms View article Article Edge Computing for Industry 5.0: Enabling Next-Generation Industrial Intelligence View article',\n",
              "  'score': 0.7001783},\n",
              " {'title': 'The best large language models (LLMs) in 2025 - Zapier',\n",
              "  'url': 'https://zapier.com/blog/best-llm/',\n",
              "  'content': 'App picks Best apps App of the day App comparisons Automation with Zapier Productivity Productivity tips App tips App tutorials How we work at Zapier App picks Best apps App of the day App comparisons Automation with Zapier Productivity Productivity tips App tips App tutorials How we work at Zapier Productivity App tips App tips13 min read mentioned apps App tipsWhat is DeepSeek and why does it matter? App tipsWhat are OpenAI o1 and o3-mini? App tipsWhat are Claude computer use and ChatGPT Operator? App comparisonsClaude vs. Product newsIntroducing Zapier Agents: AI agents that automate work across your apps Introducing Zapier Agents: AI agents that... Use Zapier to get your apps working together. See how Zapier works App Integrations',\n",
              "  'score': 0.5274388}]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build your own tools in LangChain\n",
        "\n",
        "Tools are interfaces that an agent, chain, or LLM can use to interact with the world. They combine a few things:\n",
        "\n",
        "- The name of the tool\n",
        "- A description of what the tool is\n",
        "- JSON schema of what the inputs to the tool are\n",
        "- The function to call\n",
        "- Whether the result of a tool should be returned directly to the user\n",
        "\n",
        "It is useful to have all this information because this information can be used to build action-taking systems! The name, description, and JSON schema can be used to prompt the LLM so it knows how to specify what action to take, and then the function to call is equivalent to taking that action."
      ],
      "metadata": {
        "id": "-GNP2J9Dd_nj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building a Simple Math Tool"
      ],
      "metadata": {
        "id": "ZtoArDYfheYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will start by building a simple tool which does some basic math"
      ],
      "metadata": {
        "id": "JvPAmW62eLyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def multiply(a, b):\n",
        "    \"\"\"Multiply two numbers.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "\n",
        "# Let's inspect some of the attributes associated with the tool.\n",
        "print(multiply.name)\n",
        "print(multiply.description)\n",
        "print(multiply.args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wa0mztbQ8Ae3",
        "outputId": "0bfc73ae-6ba9-429a-d1ad-110292708a64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multiply\n",
            "Multiply two numbers.\n",
            "{'a': {'title': 'A'}, 'b': {'title': 'B'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(multiply)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "tfP2TjeK8FsR",
        "outputId": "6024498f-172f-4079-905f-ec8892835a6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain_core.tools.structured.StructuredTool"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.tools.structured.StructuredTool</b><br/>def warning_emitting_wrapper(*args: Any, **kwargs: Any) -&gt; Any</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/langchain_core/tools/structured.py</a>Tool that can operate on any number of inputs.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 36);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multiply.invoke({\"a\": 2, \"b\": 3})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsOVBIvX8Agu",
        "outputId": "556208dd-5b0b-48fb-cc67-3460e75c696d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multiply.invoke({\"a\": 2.1, \"b\": 3.2})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cC7o_Mv5-Grc",
        "outputId": "62e730f8-a66e-4b7c-9da3-32edbd4ee187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.720000000000001"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multiply.invoke({\"a\": 2, \"b\": 'abc'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vN4tHdDa-Tm2",
        "outputId": "fc8e1bbc-4f03-44c2-c165-e270fca05ca4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'abcabc'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now build a tool with data type enforcing"
      ],
      "metadata": {
        "id": "XED8giGYeS_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.tools import StructuredTool\n",
        "\n",
        "class CalculatorInput(BaseModel):\n",
        "    a: float = Field(description=\"first number\")\n",
        "    b: float = Field(description=\"second number\")\n",
        "\n",
        "\n",
        "def multiply(a: float, b: float) -> float:\n",
        "    \"\"\"Multiply two numbers.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "# we could also use the @tool decorator from before\n",
        "multiply = StructuredTool.from_function(\n",
        "    func=multiply,\n",
        "    name=\"multiply\",\n",
        "    description=\"use to multiply numbers\",\n",
        "    args_schema=CalculatorInput,\n",
        "    return_direct=True\n",
        "    )\n",
        "\n",
        "# Let's inspect some of the attributes associated with the tool.\n",
        "print(multiply.name)\n",
        "print(multiply.description)\n",
        "print(multiply.args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sKF2zqQ8Aih",
        "outputId": "1c09cec1-2579-43ca-f73d-05146ce7cdb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multiply\n",
            "use to multiply numbers\n",
            "{'a': {'description': 'first number', 'title': 'A', 'type': 'number'}, 'b': {'description': 'second number', 'title': 'B', 'type': 'number'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multiply.invoke({\"a\": 2, \"b\": 3})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5taFxZg8Akc",
        "outputId": "fd2b7315-f6b6-41ae-b15e-f13e11e7303c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.0"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this code will error out as abc is not a floating point number\n",
        "multiply.invoke({\"a\": 2, \"b\": 'abc'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "3PTl9ezG8Ans",
        "outputId": "0aa696d0-f1b7-4319-819f-4e24474399fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "1 validation error for CalculatorInput\nb\n  Input should be a valid number, unable to parse string as a number [type=float_parsing, input_value='abc', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/float_parsing",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-e6eaaafa51ca>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# this code will error out as abc is not a floating point number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmultiply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'abc'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/tools/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    507\u001b[0m     ) -> Any:\n\u001b[1;32m    508\u001b[0m         \u001b[0mtool_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prep_run_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtool_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     async def ainvoke(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/tools/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merror_to_raise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_tool_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_to_raise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_to_raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_format_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martifact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtool_call_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_tool_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/tools/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_set_config_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mtool_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtool_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_args_and_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtool_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtool_call_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                 \u001b[0mtool_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtool_kwargs\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/tools/base.py\u001b[0m in \u001b[0;36m_to_args_and_kwargs\u001b[0;34m(self, tool_input, tool_call_id)\u001b[0m\n\u001b[1;32m    647\u001b[0m             \u001b[0;31m# StructuredTool with no args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0mtool_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtool_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtool_call_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m         \u001b[0;31m# For backwards compatibility, if run_input is a string,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0;31m# pass as a positional argument.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/tools/base.py\u001b[0m in \u001b[0;36m_parse_input\u001b[0;34m(self, tool_input, tool_call_id)\u001b[0m\n\u001b[1;32m    563\u001b[0m                                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m                             \u001b[0mtool_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtool_call_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtool_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m                     \u001b[0mresult_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseModelV1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36mmodel_validate\u001b[0;34m(cls, obj, strict, from_attributes, context)\u001b[0m\n\u001b[1;32m    625\u001b[0m         \u001b[0;31m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m         \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m         return cls.__pydantic_validator__.validate_python(\n\u001b[0m\u001b[1;32m    628\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_attributes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_attributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         )\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for CalculatorInput\nb\n  Input should be a valid number, unable to parse string as a number [type=float_parsing, input_value='abc', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/float_parsing"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build a Web Search & Information Extraction Tool"
      ],
      "metadata": {
        "id": "QMsvq9sVhjuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "tavily_search = TavilySearchAPIWrapper()\n",
        "\n",
        "@tool\n",
        "def search_web_extract_info(query: str) -> list:\n",
        "    \"\"\"Search the web for a query. Userful for general information or general news\"\"\"\n",
        "    results = tavily_search.raw_results(query=query,\n",
        "                                        max_results=8,\n",
        "                                        search_depth='advanced',\n",
        "                                        include_answer=False,\n",
        "                                        include_raw_content=True) # it will also scrape the web pages\n",
        "    docs = results['results']\n",
        "    docs = ['## Title'+'\\n\\n'+doc['title']+'\\n\\n'+'## Content'+'\\n\\n'+doc['raw_content'] for doc in docs]\n",
        "    return docs"
      ],
      "metadata": {
        "id": "cazBMQsghoaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = search_web_extract_info.invoke('Top LLMs in 2025')"
      ],
      "metadata": {
        "id": "hUGdma9VQewX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d5zgIkHRR8g",
        "outputId": "645e969e-9b70-4853-9672-c90bc4f1217e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['## Title\\n\\n25 of the best large language models in 2025 - TechTarget\\n\\n## Content\\n\\nPublished Time: 2025-01-31T06:00Z\\n25 of the best large language models in 2025\\nWhatIs\\nSearch the TechTarget Network \\nBrowse Definitions :\\n\\nA\\nB\\nC\\nD\\nE\\nF\\nG\\nH\\nI\\nJ\\nK\\nL\\nM\\nN\\nO\\nP\\nQ\\nR\\nS\\nT\\nU\\nV\\nW\\nX\\nY\\nZ\\n#\\n\\nLogin Register\\n\\nTechTarget Network\\nTech Accelerator\\nNews\\n2024 IT Salary Survey Results\\n\\nRSS\\n\\n\\nWhatIs\\n\\n\\nBrowse Definitions Data analytics and AI\\nTopics View All\\n\\nBusiness software\\nCloud computing\\nComputer science\\nData centers\\nIT management\\nNetworking\\nSecurity\\nSoftware development\\n\\nPlease select a category\\n\\nTopics\\n\\n\\n\\nBrowse Features Resources\\n\\nBusiness strategies\\nCareer resources\\nEmerging tech\\nTech explainers\\n\\n\\n\\nFollow:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHome\\n\\nData analytics and AI\\n\\nTech Accelerator What is Gen AI? Generative AI explained\\nPrev Next Will AI replace jobs? 17 job types that might be affected Pros and cons of AI-generated content\\nDownload this guide1\\nX\\nFree Download What is generative AI? Everything you need to know\\nThe potential of AI technology has been percolating in the background for years. But when ChatGPT, the AI chatbot, began grabbing headlines in early 2023, it put generative AI in the spotlight. This guide is your go-to manual for generative AI, covering its benefits, limits, use cases, prospects and much more.\\nFeature\\n25 of the best large language models in 2025\\nLarge language models have been affecting search for years and have been brought to the forefront by ChatGPT and other chatbots.\\n\\nShare this item with your network:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy\\n\\nSean Michael Kerner\\nBen Lutkevich, Site Editor\\n\\nPublished: 31 Jan 2025\\nLarge language models are the dynamite behind the\\xa0generative AI\\xa0boom. However, they\\'ve been around for a while.\\nLLMs\\xa0are black box AI systems that use deep learning on extremely large datasets to understand and generate new text. Modern LLMs began taking shape in 2014 when the attention mechanism -- a machine learning technique designed to mimic human cognitive attention -- was introduced in a\\xa0research paper\\xa0titled \"Neural Machine Translation by Jointly Learning to Align and Translate.\" In 2017, that attention mechanism was honed with the introduction of the transformer model in another\\xa0paper, \"Attention Is All You Need.\"\\nSome of the most well-known language models today are based on the transformer model, including the\\xa0generative pre-trained transformer series\\xa0of LLMs and bidirectional encoder representations from transformers (BERT).\\nChatGPT, which runs on a set of language models from OpenAI, attracted more than 100 million users just two months after its release in 2022. Since then, many competing models have been released. Some belong to big companies such as Google, Amazon and Microsoft; others are open source.\\nConstant developments in the field can be difficult to keep track of. Here are some of the most influential models, both past and present. Included in it are models that paved the way for today\\'s leaders as well as those that could have a significant effect in the future.\\nThis article is part of\\nWhat is Gen AI? Generative AI explained\\n\\nWhich also includes:\\n8 top generative AI tool categories for 2025\\nWill AI replace jobs? 17 job types that might be affected\\n25 of the best large language models in 2025\\n\\nTop current LLMs\\nBelow are some of the most relevant large language models today. They do natural language processing and influence the architecture of future models.\\nBERT\\nBERT\\xa0is a family of LLMs that Google introduced in 2018. BERT is a\\xa0transformer-based\\xa0model that can convert sequences of data to other sequences of data. BERT\\'s architecture is a stack of transformer encoders and features 342 million parameters. BERT was pre-trained on a large corpus of data then fine-tuned to perform specific tasks along with natural language inference and sentence text similarity. It was used to improve query understanding in the 2019 iteration of Google search.\\nClaude\\nThe\\xa0Claude LLM\\xa0focuses on constitutional AI, which shapes AI outputs guided by a set of principles that help the AI assistant it powers helpful, harmless and accurate. Claude was created by the company Anthropic.\\nThere are three primary branches of Claude -- Opus, Haiku and Sonnet. The latest iteration of the Claude LLM is the Claude 3.5 Sonnet. It understands nuance, humor and complex instructions better than earlier versions of the LLM. It also has broad programming capabilities that make it well-suited for application development. In October 2024, Claude added a computer-use AI tool, that enables the LLM to use a computer like a human does. It\\'s available via Claude.ai, the Claude iOS app and through an API.\\nCohere\\nCohere is an enterprise AI platform that provides several LLMs including Command, Rerank and Embed. These\\xa0LLMs can be custom-trained\\xa0and fine-tuned to a specific company\\'s use case. The company that created the Cohere LLM was founded by one of the authors of Attention Is All You Need.\\nDeepSeek-R1\\nDeepSeek-R1 is an open-source reasoning model for tasks with complex reasoning, mathematical problem-solving and logical inference. The model uses reinforcement learning techniques to refine its reasoning ability and solve complex problems. DeepSeek-R1 can perform critical problem-solving through self-verification, chain-of-thought reasoning and reflection.\\nErnie\\nErnie is Baidu\\'s large language model which powers the Ernie 4.0 chatbot. The bot was released in August 2023 and has garnered more than 45 million users. Ernie is rumored to have 10 trillion parameters. The bot works best in Mandarin but is capable in other languages.\\nFalcon\\nFalcon is a family of transformer-based models developed by the Technology Innovation Institute. It is open source and has multi-lingual capabilities. Falcon 2 is available in an 11 billion parameter version that provide multimodal capabilities for both text and vision.\\nThe Falcon 1 series includes a pair of larger models with Falcon 40B and Falcon 180B. Falcon models are available on GitHub as well as on cloud provider including Amazon.\\nGemini\\nGemini\\xa0is Google\\'s family of LLMs that power the company\\'s chatbot of the same name. The model replaced Palm in powering the chatbot, which was rebranded from Bard to Gemini upon the model switch. Gemini models are multimodal, meaning they can handle images, audio and video as well as text. Gemini is also integrated in many Google applications and products. It comes in three sizes -- Ultra, Pro and Nano. Ultra is the largest and most capable model, Pro is the mid-tier model and Nano is the smallest model, designed for efficiency with on-device tasks.\\nAmong the most recent models is the Gemini 1.5 Pro update that debuted in May 2024 Gemini is available as a web chatbot, the Google Vertex AI service and via API. Early previews of Gemini 2.0 Flash became available in December 2024 with updated multimodal generation capabilities.\\nGemma\\nGemma\\xa0is a family of open-source language models from Google that were trained on the same resources as Gemini. Gemma 2 was released in June 2024 in two sizes -- a 9 billion parameter model and a 27 billion parameter model. Gemma models can be\\xa0run locally\\xa0on a personal computer, and are also available in Google Vertex AI.\\nGPT-3\\nGPT-3\\xa0is OpenAI\\'s large language model with more than 175 billion parameters, released in 2020. GPT-3 uses a decoder-only transformer architecture. In September 2022, Microsoft announced it had exclusive use of GPT-3\\'s underlying model. GPT-3 is 10 times larger than its predecessor. GPT-3\\'s training data includes Common Crawl, WebText2, Books1, Books2 and Wikipedia.\\nGPT-3 is the last of the GPT series of models in which OpenAI made the parameter counts publicly available. The GPT series was first introduced in 2018 with OpenAI\\'s paper \"Improving Language Understanding by Generative Pre-Training.\"\\nGPT-3.5\\nGPT-3.5 is an upgraded version of GPT-3 with fewer parameters. GPT-3.5 was fine-tuned using\\xa0reinforcement learning from human feedback. GPT-3.5 is the version of GPT that powers ChatGPT. There are several models, with GPT-3.5 turbo being the most capable, according to OpenAI. GPT-3.5\\'s training data extends to September 2021.\\nIt was also integrated into the Bing search engine but has since been replaced with GPT-4.\\nGPT-4\\nGPT-4\\xa0, was released in 2023 and like the others in the OpenAI GPT family, it\\'s a\\xa0transformer-based model. Unlike the others, its parameter count has not been released to the public, though there are rumors that the model has more than 170 trillion. OpenAI describes GPT-4 as a multimodal model, meaning it can\\xa0process and generate both language and images\\xa0as opposed to being limited to only language. GPT-4 also introduced a system message, which lets users specify tone of voice and task.\\nGPT-4 demonstrated human-level performance in multiple academic exams. At the model\\'s release, some speculated that GPT-4 came close to\\xa0artificial general intelligence, which means it is as smart or smarter than a human. That speculation turned out to be unfounded.\\nGPT-4o\\nGPT-4 Omni (GPT-4o) is OpenAI\\'s successor to GPT-4 and offers several improvements over the previous model. GPT-4o creates a more natural human interaction for ChatGPT and is a large multimodal model, accepting various inputs including audio, image and text. The conversations let users engage as they would in a normal human conversation, and the real-time interactivity can also pick up on emotions. GPT-4o can see photos or screens and ask questions about them during interaction.\\nGPT-4o can respond in 232 milliseconds, similar to human response time and faster than GPT-4 Turbo.\\nGranite\\nThe IBM Granite family of models are fully open source models under the Apache v.2 license. The first iteration of the open source model models debuted in May 2024, followed by Granite 3.0 in October and Granite 3.1 in December 2024.\\nThere are multiple variants in the Granite model family including General-purpose models (8B and 2B variants), guardrail model and Mixture-of-Experts models. While the model can be used for general purpose deployments, IBM itself is focusing deployment and optimization for enterprise use cases like customer service, IT automation and cybersecurity.\\nLamda\\nLamda (Language Model for Dialogue Applications) is a family of LLMs developed by Google Brain announced in 2021. Lamda used a decoder-only transformer language model and was pre-trained on a large corpus of text. In 2022, LaMDA gained widespread attention when then-Google engineer Blake Lemoine went public with claims that the\\xa0program was sentient. It was built on the Seq2Seq architecture.\\nLlama\\nLarge Language Model Meta AI (Llama) is Meta\\'s LLM which was first released in 2023. The Llama 3.1 models were released in July 2024, including both a 405 billion and 70 billion parameter model.\\nThe most recent version is Llama 3.2 which was released in September 2024, initially with smaller parameter counts of 11 billion and 90 billion.\\nLlama uses a transformer architecture and was trained on a variety of public data sources, including webpages from CommonCrawl, GitHub, Wikipedia and Project Gutenberg. Llama was effectively leaked and spawned many descendants, including Vicuna and Orca. Llama is available under an open license, allowing for free use of the models. Lllama models are available in many locations including llama.com and Hugging Face.\\nMistral\\nMistral is a family of a mixture of expert models from Mistral AI. Among the newest models is Mistral Large 2 which was first released in July 2024. The model operates with 123 billion parameters and a 128k context window, supporting dozens of languages including French, German, Spanish, Italian, and many others, along with more than 80 coding languages.\\nIn November 2024, Mistral released Pixtral Large, a 124-billion-parameter multimodal model that can handle text and visual data. Mistral models are available via Mistral\\'s API on its Le Platforme-managed web service.\\no1\\nThe OpenAI o1 model family was first introduced in Sept. 2024. The o1 model\\'s focus is to provide what OpenAI refers to as - reasoning models, that can reason through a problem or query before offering a response.\\nThe o1 models excel in STEM fields, with strong results in mathematical reasoning (scoring 83% on the International Mathematics Olympiad compared to GPT-4o\\'s 13%), code generation and scientific research tasks. While they offer enhanced reasoning and improved safety features, they operate more slowly than previous models due to their thorough reasoning processes and come with certain limitations, such as restricted access features and higher API costs. The models are available to ChatGPT Plus and Team users, with varying access levels for different user categories.\\no3\\nOpenAI introduced the successor model, o3, in December 2024. According to OpenAI, o3 is designed to handle tasks with more analytical thinking, problem-solving and complex reasoning and will improve o1\\'s capabilities and performance. The o3 model is in safety testing mode and is currently not available to the public.\\nOrca\\nOrca was developed by Microsoft and has 13 billion parameters, meaning it\\'s small enough to run on a laptop. It aims to improve on advancements made by other open source models by imitating the reasoning procedures achieved by LLMs. Orca achieves the same performance as GPT-4 with significantly fewer parameters and is on par with GPT-3.5 for many tasks. Orca is built on top of the 13 billion parameter version of Llama.\\nPalm\\nThe\\xa0Pathways Language Model\\xa0is a 540 billion parameter transformer-based model from Google powering its AI chatbot\\xa0Bard. It was trained across multiple\\xa0TPU\\xa04 Pods -- Google\\'s custom hardware for machine learning. Palm specializes in reasoning tasks such as coding, math, classification and question answering. Palm also excels at decomposing complex tasks into simpler subtasks.\\nPaLM gets its name from a Google research initiative to build Pathways, ultimately creating a single model that serves as a foundation for multiple use cases. There are\\xa0several fine-tuned versions\\xa0of Palm, including Med-Palm 2 for life sciences and medical information as well as Sec-Palm for cybersecurity deployments to speed up threat analysis.\\nPhi\\nPhi is a transformer-based language model from Microsoft. The Phi 3.5 models were first released in August 2024.\\nThe series includes Phi-3.5-mini-instruct (3.82 billion parameters), Phi-3.5-MoE-instruct (41.9 billion parameters), and Phi-3.5-vision-instruct (4.15 billion parameters), each designed for specific tasks ranging from basic reasoning to vision analysis. All three models support a 128k token context length.\\nReleased under a Microsoft-branded MIT License, they are available for developers to download, use, and modify without restrictions, including for commercial purposes.\\nQwen\\nQwen is large family of open models developed by Chinese internet giant Alibaba Cloud. The newest set of models are the Qwen2.5 suite, which support 29 different languages and currently scale up to 72 billion parameters. These models are suitable for a wide range of tasks, including code generation, structured data understanding, mathematical problem-solving as well as general language understanding and generation.\\nStableLM\\nStableLM is a series of open language models developed by Stability AI, the company behind image generator Stable Diffusion.\\nStableLM 2 debuted in January 2024 initially with a 1.6 billion parameter model. In April 2024 that was expanded to also include a 12 billion parameter model. StableLM 2 supports seven languages: English, Spanish, German, Italian, French, Portuguese, and Dutch. Stability AI positions these models as offering different options for various use cases, with the 1.6B model suitable for specific, narrow tasks and faster processing while the 12B model provides more capability but requires more computational resources.\\nTülu 3\\nAllen Institute for AI\\'s Tülu 3 is an open-source 405 billion-parameter LLM. The Tülu 3 405B model has post-training methods that combine supervised fine-tuning and reinforcement learning at a larger scale. Tülu 3 uses a \"reinforcement learning from verifiable rewards\" framework for fine-tuning tasks with verifiable outcomes -- such as solving mathematical problems and following instructions.\\nVicuna 33B\\nVicuna is another influential open source LLM derived from Llama. It was developed by LMSYS and was fine-tuned using data from sharegpt.com. It is smaller and less capable that GPT-4 according to several benchmarks, but does well for a model of its size. Vicuna has only 33 billion parameters, whereas GPT-4 has trillions.\\nLLM precursors\\nAlthough LLMs are a recent phenomenon, their precursors go back decades. Learn how recent precursor Seq2Seq and distant precursor ELIZA set the stage for modern LLMs.\\nSeq2Seq\\nSeq2Seq is a deep learning approach used for machine translation, image captioning and natural language processing. It was developed by Google and underlies some of their modern LLMs, including LaMDA. Seq2Seq also underlies AlexaTM 20B, Amazon\\'s large language model. It uses a mix of encoders and decoders.\\nEliza\\nEliza was an\\xa0early natural language processing program\\xa0created in 1966. It is one of the earliest examples of a language model. Eliza simulated conversation using pattern matching and substitution. Eliza, running a certain script, could parody the interaction between a patient and therapist by applying weights to certain keywords and responding to the user accordingly. The creator of Eliza, Joshua Weizenbaum, wrote a book on the limits of computation and artificial intelligence.\\nSean Michael Kerner is an IT consultant, technology enthusiast and tinkerer. He has pulled Token Ring, configured NetWare and been known to compile his own Linux kernel. He consults with industry and media organizations on technology issues.\\nBen Lutkevich is site editor for Informa TechTarget Software Quality. Previously, he wrote definitions and features for Whatis.com.\\nNext Steps\\nGenerative AI challenges that businesses should consider\\nGenerative AI ethics: Biggest concerns\\nGenerative AI landscape: Potential future trends\\nGenerative models: VAEs, GANs, diffusion, transformers, NeRFs\\nAI content generators to explore\\nRelated Resources\\n\\nScale your fundraising: Bonterra 1H 2025 product updates –Video\\nFive data quality trends to prepare for in the year ahead –Video\\nThe Digital Transformation And Innovation Landscape –Wipro\\nImprove customer satisfaction or cut costs? Who says you have to choose? –Video\\n\\nDig Deeper on Data analytics and AI\\n\\n ##### GPT-3.5 vs. GPT-4: Biggest differences to consider  By: Leah Zitter, Ph.D.\\n ##### What is GPT-3? Everything you need to know  By: Nick Barney\\n ##### What is a small language model (SLM)?  By: Sean Kerner\\n ##### GPT-4  By: Ben Lutkevich\\n\\nSponsored News\\n\\nThree Innovative AI Use Cases for Natural Language Processing –Dell Technologies\\nAutonomous coding: The future of the revenue cycle –Solventum\\nHybrid Work Drives New Criteria for VDI and DaaS –Dell Technologies\\n\\nRelated Content\\n\\nExploring GPT-3 architecture – Search Enterprise AI\\nWhat is GPT-3? Everything you need to know – Search Enterprise AI\\nMicrosoft exclusively licenses OpenAI\\'s GPT-3 ... – Search Enterprise AI\\n\\nLatest TechTarget resources\\n\\nNetworking\\nSecurity\\nCIO\\nHR Software\\nCustomer Experience\\n\\nSearch Networking\\n\\n\\nWhat are port numbers and how do they work?A port number is a way to identify a specific process to which an internet or other network message is to be forwarded when it ...\\n\\n\\nWhat is a router?A router is a physical or virtual appliance that passes information between two or more packet-switched computer networks.\\n\\n\\nWhat is east-west traffic?East-west traffic refers to the transfer of data packets that move from server to server within a network\\'s data center.\\n\\n\\nSearch Security\\n\\n\\nWhat is cyberstalking and how to prevent it?Cyberstalking is a crime in which someone harasses or stalks a victim using electronic or digital means, such as social media, ...\\n\\n\\nWhat is a watering hole attack?A watering hole attack is a security exploit in which the attacker seeks to compromise a specific group of end users by infecting...\\n\\n\\nWhat is multifactor authentication?Multifactor authentication (MFA) is an IT security technology that requires multiple sources of unique information from ...\\n\\n\\nSearch CIO\\n\\n\\nWhat is a think tank?A think tank is an organization that gathers a group of interdisciplinary scholars to perform research around particular policies...\\n\\n\\nWhat is emotional intelligence (EI)?Emotional intelligence (EI) is the area of cognitive ability that facilitates interpersonal behavior.\\n\\n\\nWhat are agreed-upon procedures (AUPs)?Agreed-upon procedures are a standard a company or client outlines in an engagement letter or other written agreement when it ...\\n\\n\\nSearch HRSoftware\\n\\n\\nWhat is gamification? How it works and how to use itGamification is a strategy that integrates entertaining and immersive gaming elements into nongame contexts to enhance engagement...\\n\\n\\nWhat is employee self-service (ESS)?Employee self-service (ESS) is a widely used human resources technology that enables employees to perform many job-related ...\\n\\n\\nWhat is DEI? Diversity, equity and inclusion explainedDiversity, equity and inclusion is a term used to describe policies and programs that promote the representation and ...\\n\\n\\nSearch Customer Experience\\n\\n\\nWhat is voice of the customer? A guide to VOC StrategyVoice of the customer (VOC) is the component of customer experience (CX) that focuses on customer needs, wants, expectations and ...\\n\\n\\nWhat is high-touch customer service?High-touch customer service is a category of contact center interaction that requires human interaction.\\n\\n\\nWhat is CRM (customer relationship management)?CRM (customer relationship management) is the combination of practices, strategies and technologies that companies use to manage ...\\n\\n\\nBrowse by Topic\\n\\n\\nBrowse Resources\\n\\n\\nAbout Us\\n\\nMeet The Editors\\nEditorial Ethics Policy\\nContact Us\\nAdvertisers\\nBusiness Partners\\nEvents\\nMedia Kit\\nCorporate Site\\nReprints\\n\\nAll Rights Reserved, Copyright 1999 - 2025, TechTarget  \\nPrivacy Policy\\nCookie Preferences\\nCookie Preferences\\nDo Not Sell or Share My Personal Information\\nClose',\n",
              " '## Title\\n\\nBest 39 Large Language Models (LLMs) in 2025 - Exploding Topics\\n\\n## Content\\n\\nBest 22 Large Language Models (LLMs) (February 2025)\\n\\n\\n\\nAbout\\nNewsletter\\nBlog\\n\\n\\nBest 22 Large Language Models (LLMs) (February 2025)\\n\\nby Anthony Cardillo\\nFebruary 7, 2025\\nLarge language models are pre-trained on large datasets and use natural language processing to perform linguistic tasks such as text generation, code completion, paraphrasing, and more.\\nThe initial release of ChatGPT sparked the rapid adoption of generative AI, which has led to large language model innovations and industry growth.\\nIn fact, 92% of Fortune 500 firms have started using generative AI in their workflows.\\nAs adoption continues to grow, so does the LLM industry. The global large language model market is projected to grow from $6.5 billion in 2024 to $140.8 billion by 2033.\\nWith that, here is a list of the top 21 LLMs available in September 2024.\\nLLM NameDeveloperRelease DateAccessParametersDeepSeek R1DeepSeekJanuary 20, 2025Open-Source671 billionGPT-4oOpenAIMay 13, 2024APIUnknownClaude 3.5AnthropicJune 20, 2024APIUnknownGrok-1xAINovember 4, 2023Open-Source314 billionMistral 7BMistral AISeptember 27, 2023Open-Source7.3 billionPaLM 2GoogleMay 10, 2023Open-Source340 billionFalcon 180BTechnology Innovation InstituteSeptember 6, 2023Open-Source180 billionStable LM 2Stability AIJanuary 19, 2024Open-Source1.6 billion, 12 billionGemini 1.5Google DeepMindFebruary 2nd, 2024APIUnknownLlama 3.1Meta AIJune 23, 2024Open-Source405 billionMixtral 8x22BMistral AIApril 10, 2024Open-Source141 billionInflection-2.5Inflection AIMarch 10, 2024ProprietaryUnknownJambaAI21 LabsMarch 29, 2024Open-Source52 billionCommand RCohereMarch 11, 2024Both35 billionGemmaGoogle DeepMindFebruary 21, 2024Open-Source2 billion, 7 billionPhi-3MicrosoftApril 23, 2024Both3.8 billionXGen-7BSalesforceJuly 3, 2023Open-Source7 billionDBRXDatabricks\\' Mosaic MLMarch 27, 2024Open-Source132 billionPythiaEleutherAIFebruary 13, 2023Open-Source70 million to 12 billionSoraOpenAIFebruary 15, 2024 (announced)APIUnknownAlpaca 7BStanford CRFMMarch 13, 2023Open-Source7 billionNemotron-4NvidiaJune 14, 2024Open-Source340 billion\\n1. DeepSeek R1\\n\\nDeveloper: DeepSeek\\nRelease date: January 2025\\nNumber of Parameters: 671B total, 37B active\\nWhat is it? DeepSeek R1 is a reasoning model that excels in math and coding. It beats or matches OpenAI o1 in several benchmarks, including MATH-500 and AIME 2024.\\nOn its release, DeepSeek immediately hit headlines due to the low cost of training compared to most major LLMs.\\nDeepSeek R1 is free to use and open-source. It\\'s accessible via the API, the DeepSeek website, and mobile apps.\\n2. GPT-4o\\n\\nDeveloper: OpenAI\\nRelease date: May 13, 2024\\nNumber of Parameters: Unknown\\nWhat is it? GPT-4o is the latest and most advanced OpenAI language model, succeeding GPT-4, GPT-3.5, and GPT-3. OpenAI claims that GPT-4o is 50% cheaper than GPT-4 despite being 2x faster at generating tokens. This multimodal model includes text, image, video, and voice capabilities packaged into one.\\nGPT-4o\\'s biggest upgrade is the Voice-to-Voice function, which will improve input response times to an average of 320 milliseconds (compared to a few seconds with GPT-4). This feature is expected to be released in the coming weeks.\\n3. Claude 3.5\\n\\nDeveloper: Anthropic\\nRelease date: March 14, 2024\\nNumber of Parameters: Unknown\\nWhat is it?\\xa0As a new upgrade from the highly rated\\xa0Claude 3, Claude 3.5 Sonnet is the first release of the new Claude 3.5 model family. Similar to Claude 3, it\\'ll also include the Haiku and Opus models. As debatably the biggest competitor to GPT-4 and ChatGPT, Claude made even bigger improvements to this model by maintaining the 200,000 token context window at a lower cost. This is much larger than GPT-4\\'s 32,000 token capabilities.\\nAccording to Anthropic\\'s report, Claude 3.5 Sonnet outperformed GPT-4o in major benchmarks like coding and text reasoning. Plus, this is Claude\\'s most advanced vision model, with the ability to transcribe text from images or generate insights from charts.\\nAmazon has invested over $4 billion in Anthropic, bringing the startup\\'s valuation to $15 billion. The Claude mobile app was also released in May 2024.\\n4. Grok-1\\n\\nDeveloper: xAI\\nRelease date: November 4, 2023\\nNumber of Parameters: 314 billion\\nWhat is it? Created by Elon Musk\\'s artificial intelligence startup xAI, Grok-1 is currently the largest open-source LLM released to date at 314 billion parameters. Grok directly integrates with X (Twitter), and users must pay for an X Premium+ subscription to gain access.\\nBecause of the model’s size, Grok has a mixture-of-experts (MoE) architecture that only uses 25% of its weights for any given input token to maximize calculation efficiency.\\nIn August 2024, both Grok-2 and Grok-2 mini were released to X users in beta. According to xAI\\'s reports, Grok-2 outperforms GPT-4o in numerous categories, such as GPQA, MMLU-Pro, and DocVQA.\\n5. Mistral 7B\\n\\nDeveloper: Mistral AI\\nRelease date: September 27, 2023\\nNumber of Parameters: 7.3 billion\\nWhat is it? Mistral 7B is an open-source language model with 32 layers, 32 attention heads, and eight key-value heads. Despite running with fewer parameters, they outperformed the Llama 2 family of models in nearly all metrics, including MMLU, reading comprehension, math, coding, etc.\\nMistral 7B is released under an Apache 2.0 license. Customers are free to download it locally, deploy it on the cloud, or run it on HuggingFace. The Paris-based startup is close to securing a new $600 million funding round that would value the company at $6 billion.\\n6. PaLM 2\\n\\nDeveloper: Google\\nRelease date: May 10, 2023\\nNumber of Parameters: 340 billion\\nWhat is it? PaLM 2 is an advanced large language model developed by Google. As the successor to the original Pathways Language Model (PaLM), it’s trained on 3.6 trillion tokens (compared to 780 billion) and 340 billion parameters (compared to 540 billion). PaLM 2 was originally used to power Google\\'s first generative AI chatbot, Bard (rebranded to Gemini in February 2024).\\n7. Falcon 180B\\n\\nDeveloper: Technology Innovation Institute (TII)\\nRelease date: September 6, 2023\\nNumber of Parameters: 180 billion\\nWhat is it? Developed and funded by the Technology Innovation Institute, Falcon 180B is an upgraded version of the earlier Falcon 40B LLM. It has 180 billion parameters, which is 4.5 times larger than the 40 billion parameters of Falcon 40B.\\nIn addition to Falcon 40B, it also outperforms other large language models like GPT-3.5 and LLaMA 2 on tasks such as reasoning, question answering, and coding. In February 2024, the UAE-based Technology Innovation Institute (TII) committed $300 million in funding to the Falcon Foundation.\\n8. Stable LM 2\\n\\nDeveloper: Stability AI\\nRelease date: January 19, 2024\\nNumber of Parameters: 1.6 billion and 12 billion\\nWhat is it? Stability AI, the creators of the Stable Diffusion text-to-image model, are the developers behind Stable LM 2. This series of large language models includes Stable LM 2 12B (12 billion parameters) and Stable LM 2 1.6B (1.6 billion parameters). Released in April 2024, the larger 12B model outperforms models like LLaMA 2 70B on key benchmarks despite being much smaller.\\n9. Gemini 1.5\\n\\nDeveloper: Google DeepMind\\nRelease date: February 2nd, 2024\\nNumber of Parameters: Unknown\\nWhat is it? Gemini 1.5 is Google\\'s next-generation large language model, offering a significant upgrade over its predecessor, Gemini 1.0. While it’s only available for early testing, Gemini 1.5 Pro provides a one million-token context window (1 hour of video, 700,000 words, or 30,000 lines of code), the largest to date compared to alternative LLMs and chatbots. This upgrade is 35 times larger than Gemini 1.0 Pro and surpasses the previous largest record of 200,000 tokens held by Anthropic’s Claude 2.1.\\n10. Llama 3.1\\n\\nDeveloper: Meta AI\\nRelease date: June 23, 2024\\nNumber of Parameters: 405 billion\\nWhat is it? Llama 3, the predecessor to Llama 3.1, was available in both 70B and 8B versions that outperformed other open-source models like Mistral 7B and Google\\'s Gemma 7B on MMLU, reasoning, coding, and math benchmarks. Now, users will notice major upgrades to the latest version, including 405 billion parameters and an expended context length of 128,000.\\nUsers will also notice more accuracy because of the impressive knowledge base, which has been trained on over 15 trillion tokens. Plus, Meta added eight additional languages for this model. The increased size of this model makes it the largest open-source model released to date.\\nCustomers can still access its predecessor, Llama 2, which is available in three versions: 7 billion, 13 billion, and 70 billion parameters.\\n11. Mixtral 8x22B\\n\\nDeveloper: Mistral AI\\nRelease date: April 10, 2024\\nNumber of Parameters: 141 billion\\nWhat is it? Mixtral 8x22B is Mistral AI\\'s latest and most advanced large language model. This sparse Mixture-of-Experts (SMoE) model has 141 billion total parameters but only uses 39B active parameters to focus on improving the model’s performance-to-cost ratio.\\nThe startup also recently released Mistral Large, a ChatGPT alternative that ranks second behind GPT-4 among API-based LLMs.\\n12. Inflection-2.5\\n\\nDeveloper: Inflection AI\\nRelease date: March 10, 2024\\nNumber of Parameters: Unknown\\nWhat is it? Inflection-2.5 is the latest large language model (LLM) developed by Inflection AI to power its conversational AI assistant, Pi. Significant upgrades have been made, as the model currently achieves over 94% of GPT-4’s average performance while only having 40% of the training FLOPs. In March 2024, the Microsoft-backed startup reached 1+ million daily active users on Pi.\\n13. Jamba\\n\\nDeveloper: AI21 Labs\\nRelease date: March 29, 2024\\nNumber of Parameters: 52 billion\\nWhat is it? AI21 Labs created Jamba, the world\\'s first production-grade Mamba-style large language model. It integrates SSM technology with elements of a traditional transformer model to create a hybrid architecture. The model is efficient and highly scalable, with a context window of 256K and deployment support of 140K context on a single GPU.\\n14. Command R\\n\\nDeveloper: Cohere\\nRelease date: March 11, 2024\\nNumber of Parameters: 35 billion\\nWhat is it? Command R is a series of scalable LLMs from Cohere that support ten languages and 128,000-token context length (around 100 pages of text). This model primarily excels at retrieval-augmented generation, code-related tasks like explanations or rewrites, and reasoning. In April 2024, Command R+ was released to support larger workloads and provide real-world enterprise support.\\n15. Gemma\\n\\nDeveloper: Google DeepMind\\nRelease date: February 21, 2024\\nNumber of Parameters: 2 billion and 7 billion\\nWhat is it? Gemma is a series of lightweight open-source language models developed and released by Google DeepMind. The Gemma models are built with similar tech to the Gemini models, but Gemma is limited to text inputs and outputs only. The models have a context window of 8,000 tokens and are available in 2 billion and 7 billion parameter sizes.\\n16. Phi-3\\n\\nDeveloper: Microsoft\\nRelease date: April 23, 2024\\nNumber of Parameters: 3.8 billion\\nWhat is it? Classified as a small language model (SLM), Phi-3 is Microsoft\\'s latest release with 3.8 billion parameters. Despite the smaller size, it\\'s been trained on 3.3 trillion tokens of data to compete with Mistral 8x7B and GPT-3.5 performance on MT-bench and MMLU benchmarks.\\nTo date, Phi-3-mini is the only model available. However, Microsoft plans to release the Phi-3-small and Phi-3-medium models later this year.\\n17. XGen-7B\\n\\nDeveloper: Salesforce\\nRelease date: July 3, 2023\\nNumber of Parameters: 7 billion\\nWhat is it? XGen-7B is a large language model from Salesforce with 7 billion parameters and an 8k context window. The model was trained on 1.37 trillion tokens from various sources, such as RedPajama, Wikipedia, and Salesforce\\'s own Starcoder dataset.\\nSalesforce has released two open-source versions, a 4,000 and 8,000 token context window base, hosted under an Apache 2.0 license.\\n18. DBRX\\n\\nDeveloper: Databricks\\' Mosaic ML\\nRelease date: March 27, 2024\\nNumber of Parameters: 132 billion\\nWhat is it? DBRX is an open-source LLM built by Databricks and the Mosaic ML research team. The mixture-of-experts architecture has 36 billion (of 132 billion total) active parameters on an input. DBRX has 16 experts and chooses 4 of them during inference, providing 65 times more expert combinations compared to similar models like Mixtral and Grok-1\\n19. Pythia\\n\\nDeveloper: EleutherAI\\nRelease date: February 13, 2023\\nNumber of Parameters: 70 million to 12 billion\\nWhat is it? Pythia is a series of 16 large language models developed and released by EleutherAI, a non-profit AI research lab. There are eight different model sizes: 70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, and 12B. Because of Pythia\\'s open-source license, these LLMs serve as a base model for fine-tuned, instruction-following LLMs like Dolly 2.0 by Databricks.\\n20. Sora\\n\\nDeveloper: OpenAI\\nRelease date: February 15, 2024 (announced)\\nNumber of Parameters: Unknown\\nWhat is it? OpenAI\\'s latest development is Sora, a text-to-video model that combines LLMs and generative AI to turn text prompts into realistic videos up to 60 seconds long. The model uses a transformer architecture that operates on \"spacetime patches\" of video and image data rather than text tokens like other LLMs. No official release date for Sora has been announced, but OpenAI expects it to open to the public in late 2024.\\n21. Alpaca 7B\\n\\nDeveloper: Stanford CRFM\\nRelease date: March 27, 2024\\nNumber of Parameters: 7 billion\\nWhat is it? Alpaca is a 7 billion-parameter language model developed by a Stanford research team and fine-tuned from Meta\\'s LLaMA 7B model. Users will notice that although being much smaller, Alpaca performs similarly to text-DaVinci-003 (ChatGPT 3.5). However, Alpaca 7B is available for research purposes, and no commercial licenses are available.\\n22. Nemotron-4 340B\\n\\nDeveloper: NVIDIA\\nRelease date: June 14, 2024\\nNumber of Parameters: 340 billion\\nWhat is it? Nemotron-4 340B\\xa0is a family of large language models for synthetic data generation and AI model training. These models help businesses create new LLMs without larger and more expensive datasets. Instead, Nemotron-4 can create high-quality synthetic data to train other AI models, which reduces the need for extensive human-annotated data.\\nThe model family includes Nemotron-4-340B-Base (foundation model), Nemotron-4-340B-Instruct (fine-tuned chatbot), and Nemotron-4-340B-Reward (quality assessment and preference ranking). Due to the 9 trillion tokens used in training, which includes English, multilingual, and coding language data, Nemotron-4 matches GPT-4\\'s high-quality synthetic data generation capabilities.\\nConclusion\\nThe landscape of large language models is rapidly evolving, with new breakthroughs and innovations emerging at an unprecedented pace.\\nFrom compact models like Phi-2 and Alpaca 7B to cutting-edge architectures like Jamba and DBRX, the field of LLMs is pushing the boundaries of what\\'s possible in natural language processing (NLP).\\nWe will keep this list regularly updated with new models. If you liked learning about these LLMs, check out our lists of generative AI startups and AI startups.\\nFind Thousands of Trending Topics With Our Platform\\nTry Exploding Topics Pro\\n\\nExploding Topics\\n\\nJoin Pro\\nNewsletter\\nTrending Topics\\nAdd a Topic\\nCustomer Login\\n\\nCompany\\n\\nAbout Us\\nContact\\nMethodology\\nCookie Settings\\n\\nFree Tools\\n\\nKeyword Research\\nBacklink Checker\\nSERP Checker\\nKeyword Rank Checker\\nFree SEO Tools\\n\\nConnect\\n\\nYouTube\\nInstagram\\nX (Twitter)\\n\\nResources\\n\\nBlog\\nMarketing Academy\\nFree Webinars\\n\\n\\n\\n© 2025 \\xa0Exploding Topics is a Trademark of Semrush Inc\\n\\nPrivacy Policy\\nTerms of Service\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "display(Markdown(docs[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jHJb63LQm2If",
        "outputId": "1c085d9c-0625-4384-f8c6-2bf65bf419e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Title\n\n25 of the best large language models in 2025 - TechTarget\n\n## Content\n\nPublished Time: 2025-01-31T06:00Z\n25 of the best large language models in 2025\nWhatIs\nSearch the TechTarget Network \nBrowse Definitions :\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nM\nN\nO\nP\nQ\nR\nS\nT\nU\nV\nW\nX\nY\nZ\n#\n\nLogin Register\n\nTechTarget Network\nTech Accelerator\nNews\n2024 IT Salary Survey Results\n\nRSS\n\n\nWhatIs\n\n\nBrowse Definitions Data analytics and AI\nTopics View All\n\nBusiness software\nCloud computing\nComputer science\nData centers\nIT management\nNetworking\nSecurity\nSoftware development\n\nPlease select a category\n\nTopics\n\n\n\nBrowse Features Resources\n\nBusiness strategies\nCareer resources\nEmerging tech\nTech explainers\n\n\n\nFollow:\n\n\n\n\n\n\n\n\n\nHome\n\nData analytics and AI\n\nTech Accelerator What is Gen AI? Generative AI explained\nPrev Next Will AI replace jobs? 17 job types that might be affected Pros and cons of AI-generated content\nDownload this guide1\nX\nFree Download What is generative AI? Everything you need to know\nThe potential of AI technology has been percolating in the background for years. But when ChatGPT, the AI chatbot, began grabbing headlines in early 2023, it put generative AI in the spotlight. This guide is your go-to manual for generative AI, covering its benefits, limits, use cases, prospects and much more.\nFeature\n25 of the best large language models in 2025\nLarge language models have been affecting search for years and have been brought to the forefront by ChatGPT and other chatbots.\n\nShare this item with your network:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBy\n\nSean Michael Kerner\nBen Lutkevich, Site Editor\n\nPublished: 31 Jan 2025\nLarge language models are the dynamite behind the generative AI boom. However, they've been around for a while.\nLLMs are black box AI systems that use deep learning on extremely large datasets to understand and generate new text. Modern LLMs began taking shape in 2014 when the attention mechanism -- a machine learning technique designed to mimic human cognitive attention -- was introduced in a research paper titled \"Neural Machine Translation by Jointly Learning to Align and Translate.\" In 2017, that attention mechanism was honed with the introduction of the transformer model in another paper, \"Attention Is All You Need.\"\nSome of the most well-known language models today are based on the transformer model, including the generative pre-trained transformer series of LLMs and bidirectional encoder representations from transformers (BERT).\nChatGPT, which runs on a set of language models from OpenAI, attracted more than 100 million users just two months after its release in 2022. Since then, many competing models have been released. Some belong to big companies such as Google, Amazon and Microsoft; others are open source.\nConstant developments in the field can be difficult to keep track of. Here are some of the most influential models, both past and present. Included in it are models that paved the way for today's leaders as well as those that could have a significant effect in the future.\nThis article is part of\nWhat is Gen AI? Generative AI explained\n\nWhich also includes:\n8 top generative AI tool categories for 2025\nWill AI replace jobs? 17 job types that might be affected\n25 of the best large language models in 2025\n\nTop current LLMs\nBelow are some of the most relevant large language models today. They do natural language processing and influence the architecture of future models.\nBERT\nBERT is a family of LLMs that Google introduced in 2018. BERT is a transformer-based model that can convert sequences of data to other sequences of data. BERT's architecture is a stack of transformer encoders and features 342 million parameters. BERT was pre-trained on a large corpus of data then fine-tuned to perform specific tasks along with natural language inference and sentence text similarity. It was used to improve query understanding in the 2019 iteration of Google search.\nClaude\nThe Claude LLM focuses on constitutional AI, which shapes AI outputs guided by a set of principles that help the AI assistant it powers helpful, harmless and accurate. Claude was created by the company Anthropic.\nThere are three primary branches of Claude -- Opus, Haiku and Sonnet. The latest iteration of the Claude LLM is the Claude 3.5 Sonnet. It understands nuance, humor and complex instructions better than earlier versions of the LLM. It also has broad programming capabilities that make it well-suited for application development. In October 2024, Claude added a computer-use AI tool, that enables the LLM to use a computer like a human does. It's available via Claude.ai, the Claude iOS app and through an API.\nCohere\nCohere is an enterprise AI platform that provides several LLMs including Command, Rerank and Embed. These LLMs can be custom-trained and fine-tuned to a specific company's use case. The company that created the Cohere LLM was founded by one of the authors of Attention Is All You Need.\nDeepSeek-R1\nDeepSeek-R1 is an open-source reasoning model for tasks with complex reasoning, mathematical problem-solving and logical inference. The model uses reinforcement learning techniques to refine its reasoning ability and solve complex problems. DeepSeek-R1 can perform critical problem-solving through self-verification, chain-of-thought reasoning and reflection.\nErnie\nErnie is Baidu's large language model which powers the Ernie 4.0 chatbot. The bot was released in August 2023 and has garnered more than 45 million users. Ernie is rumored to have 10 trillion parameters. The bot works best in Mandarin but is capable in other languages.\nFalcon\nFalcon is a family of transformer-based models developed by the Technology Innovation Institute. It is open source and has multi-lingual capabilities. Falcon 2 is available in an 11 billion parameter version that provide multimodal capabilities for both text and vision.\nThe Falcon 1 series includes a pair of larger models with Falcon 40B and Falcon 180B. Falcon models are available on GitHub as well as on cloud provider including Amazon.\nGemini\nGemini is Google's family of LLMs that power the company's chatbot of the same name. The model replaced Palm in powering the chatbot, which was rebranded from Bard to Gemini upon the model switch. Gemini models are multimodal, meaning they can handle images, audio and video as well as text. Gemini is also integrated in many Google applications and products. It comes in three sizes -- Ultra, Pro and Nano. Ultra is the largest and most capable model, Pro is the mid-tier model and Nano is the smallest model, designed for efficiency with on-device tasks.\nAmong the most recent models is the Gemini 1.5 Pro update that debuted in May 2024 Gemini is available as a web chatbot, the Google Vertex AI service and via API. Early previews of Gemini 2.0 Flash became available in December 2024 with updated multimodal generation capabilities.\nGemma\nGemma is a family of open-source language models from Google that were trained on the same resources as Gemini. Gemma 2 was released in June 2024 in two sizes -- a 9 billion parameter model and a 27 billion parameter model. Gemma models can be run locally on a personal computer, and are also available in Google Vertex AI.\nGPT-3\nGPT-3 is OpenAI's large language model with more than 175 billion parameters, released in 2020. GPT-3 uses a decoder-only transformer architecture. In September 2022, Microsoft announced it had exclusive use of GPT-3's underlying model. GPT-3 is 10 times larger than its predecessor. GPT-3's training data includes Common Crawl, WebText2, Books1, Books2 and Wikipedia.\nGPT-3 is the last of the GPT series of models in which OpenAI made the parameter counts publicly available. The GPT series was first introduced in 2018 with OpenAI's paper \"Improving Language Understanding by Generative Pre-Training.\"\nGPT-3.5\nGPT-3.5 is an upgraded version of GPT-3 with fewer parameters. GPT-3.5 was fine-tuned using reinforcement learning from human feedback. GPT-3.5 is the version of GPT that powers ChatGPT. There are several models, with GPT-3.5 turbo being the most capable, according to OpenAI. GPT-3.5's training data extends to September 2021.\nIt was also integrated into the Bing search engine but has since been replaced with GPT-4.\nGPT-4\nGPT-4 , was released in 2023 and like the others in the OpenAI GPT family, it's a transformer-based model. Unlike the others, its parameter count has not been released to the public, though there are rumors that the model has more than 170 trillion. OpenAI describes GPT-4 as a multimodal model, meaning it can process and generate both language and images as opposed to being limited to only language. GPT-4 also introduced a system message, which lets users specify tone of voice and task.\nGPT-4 demonstrated human-level performance in multiple academic exams. At the model's release, some speculated that GPT-4 came close to artificial general intelligence, which means it is as smart or smarter than a human. That speculation turned out to be unfounded.\nGPT-4o\nGPT-4 Omni (GPT-4o) is OpenAI's successor to GPT-4 and offers several improvements over the previous model. GPT-4o creates a more natural human interaction for ChatGPT and is a large multimodal model, accepting various inputs including audio, image and text. The conversations let users engage as they would in a normal human conversation, and the real-time interactivity can also pick up on emotions. GPT-4o can see photos or screens and ask questions about them during interaction.\nGPT-4o can respond in 232 milliseconds, similar to human response time and faster than GPT-4 Turbo.\nGranite\nThe IBM Granite family of models are fully open source models under the Apache v.2 license. The first iteration of the open source model models debuted in May 2024, followed by Granite 3.0 in October and Granite 3.1 in December 2024.\nThere are multiple variants in the Granite model family including General-purpose models (8B and 2B variants), guardrail model and Mixture-of-Experts models. While the model can be used for general purpose deployments, IBM itself is focusing deployment and optimization for enterprise use cases like customer service, IT automation and cybersecurity.\nLamda\nLamda (Language Model for Dialogue Applications) is a family of LLMs developed by Google Brain announced in 2021. Lamda used a decoder-only transformer language model and was pre-trained on a large corpus of text. In 2022, LaMDA gained widespread attention when then-Google engineer Blake Lemoine went public with claims that the program was sentient. It was built on the Seq2Seq architecture.\nLlama\nLarge Language Model Meta AI (Llama) is Meta's LLM which was first released in 2023. The Llama 3.1 models were released in July 2024, including both a 405 billion and 70 billion parameter model.\nThe most recent version is Llama 3.2 which was released in September 2024, initially with smaller parameter counts of 11 billion and 90 billion.\nLlama uses a transformer architecture and was trained on a variety of public data sources, including webpages from CommonCrawl, GitHub, Wikipedia and Project Gutenberg. Llama was effectively leaked and spawned many descendants, including Vicuna and Orca. Llama is available under an open license, allowing for free use of the models. Lllama models are available in many locations including llama.com and Hugging Face.\nMistral\nMistral is a family of a mixture of expert models from Mistral AI. Among the newest models is Mistral Large 2 which was first released in July 2024. The model operates with 123 billion parameters and a 128k context window, supporting dozens of languages including French, German, Spanish, Italian, and many others, along with more than 80 coding languages.\nIn November 2024, Mistral released Pixtral Large, a 124-billion-parameter multimodal model that can handle text and visual data. Mistral models are available via Mistral's API on its Le Platforme-managed web service.\no1\nThe OpenAI o1 model family was first introduced in Sept. 2024. The o1 model's focus is to provide what OpenAI refers to as - reasoning models, that can reason through a problem or query before offering a response.\nThe o1 models excel in STEM fields, with strong results in mathematical reasoning (scoring 83% on the International Mathematics Olympiad compared to GPT-4o's 13%), code generation and scientific research tasks. While they offer enhanced reasoning and improved safety features, they operate more slowly than previous models due to their thorough reasoning processes and come with certain limitations, such as restricted access features and higher API costs. The models are available to ChatGPT Plus and Team users, with varying access levels for different user categories.\no3\nOpenAI introduced the successor model, o3, in December 2024. According to OpenAI, o3 is designed to handle tasks with more analytical thinking, problem-solving and complex reasoning and will improve o1's capabilities and performance. The o3 model is in safety testing mode and is currently not available to the public.\nOrca\nOrca was developed by Microsoft and has 13 billion parameters, meaning it's small enough to run on a laptop. It aims to improve on advancements made by other open source models by imitating the reasoning procedures achieved by LLMs. Orca achieves the same performance as GPT-4 with significantly fewer parameters and is on par with GPT-3.5 for many tasks. Orca is built on top of the 13 billion parameter version of Llama.\nPalm\nThe Pathways Language Model is a 540 billion parameter transformer-based model from Google powering its AI chatbot Bard. It was trained across multiple TPU 4 Pods -- Google's custom hardware for machine learning. Palm specializes in reasoning tasks such as coding, math, classification and question answering. Palm also excels at decomposing complex tasks into simpler subtasks.\nPaLM gets its name from a Google research initiative to build Pathways, ultimately creating a single model that serves as a foundation for multiple use cases. There are several fine-tuned versions of Palm, including Med-Palm 2 for life sciences and medical information as well as Sec-Palm for cybersecurity deployments to speed up threat analysis.\nPhi\nPhi is a transformer-based language model from Microsoft. The Phi 3.5 models were first released in August 2024.\nThe series includes Phi-3.5-mini-instruct (3.82 billion parameters), Phi-3.5-MoE-instruct (41.9 billion parameters), and Phi-3.5-vision-instruct (4.15 billion parameters), each designed for specific tasks ranging from basic reasoning to vision analysis. All three models support a 128k token context length.\nReleased under a Microsoft-branded MIT License, they are available for developers to download, use, and modify without restrictions, including for commercial purposes.\nQwen\nQwen is large family of open models developed by Chinese internet giant Alibaba Cloud. The newest set of models are the Qwen2.5 suite, which support 29 different languages and currently scale up to 72 billion parameters. These models are suitable for a wide range of tasks, including code generation, structured data understanding, mathematical problem-solving as well as general language understanding and generation.\nStableLM\nStableLM is a series of open language models developed by Stability AI, the company behind image generator Stable Diffusion.\nStableLM 2 debuted in January 2024 initially with a 1.6 billion parameter model. In April 2024 that was expanded to also include a 12 billion parameter model. StableLM 2 supports seven languages: English, Spanish, German, Italian, French, Portuguese, and Dutch. Stability AI positions these models as offering different options for various use cases, with the 1.6B model suitable for specific, narrow tasks and faster processing while the 12B model provides more capability but requires more computational resources.\nTülu 3\nAllen Institute for AI's Tülu 3 is an open-source 405 billion-parameter LLM. The Tülu 3 405B model has post-training methods that combine supervised fine-tuning and reinforcement learning at a larger scale. Tülu 3 uses a \"reinforcement learning from verifiable rewards\" framework for fine-tuning tasks with verifiable outcomes -- such as solving mathematical problems and following instructions.\nVicuna 33B\nVicuna is another influential open source LLM derived from Llama. It was developed by LMSYS and was fine-tuned using data from sharegpt.com. It is smaller and less capable that GPT-4 according to several benchmarks, but does well for a model of its size. Vicuna has only 33 billion parameters, whereas GPT-4 has trillions.\nLLM precursors\nAlthough LLMs are a recent phenomenon, their precursors go back decades. Learn how recent precursor Seq2Seq and distant precursor ELIZA set the stage for modern LLMs.\nSeq2Seq\nSeq2Seq is a deep learning approach used for machine translation, image captioning and natural language processing. It was developed by Google and underlies some of their modern LLMs, including LaMDA. Seq2Seq also underlies AlexaTM 20B, Amazon's large language model. It uses a mix of encoders and decoders.\nEliza\nEliza was an early natural language processing program created in 1966. It is one of the earliest examples of a language model. Eliza simulated conversation using pattern matching and substitution. Eliza, running a certain script, could parody the interaction between a patient and therapist by applying weights to certain keywords and responding to the user accordingly. The creator of Eliza, Joshua Weizenbaum, wrote a book on the limits of computation and artificial intelligence.\nSean Michael Kerner is an IT consultant, technology enthusiast and tinkerer. He has pulled Token Ring, configured NetWare and been known to compile his own Linux kernel. He consults with industry and media organizations on technology issues.\nBen Lutkevich is site editor for Informa TechTarget Software Quality. Previously, he wrote definitions and features for Whatis.com.\nNext Steps\nGenerative AI challenges that businesses should consider\nGenerative AI ethics: Biggest concerns\nGenerative AI landscape: Potential future trends\nGenerative models: VAEs, GANs, diffusion, transformers, NeRFs\nAI content generators to explore\nRelated Resources\n\nScale your fundraising: Bonterra 1H 2025 product updates –Video\nFive data quality trends to prepare for in the year ahead –Video\nThe Digital Transformation And Innovation Landscape –Wipro\nImprove customer satisfaction or cut costs? Who says you have to choose? –Video\n\nDig Deeper on Data analytics and AI\n\n ##### GPT-3.5 vs. GPT-4: Biggest differences to consider  By: Leah Zitter, Ph.D.\n ##### What is GPT-3? Everything you need to know  By: Nick Barney\n ##### What is a small language model (SLM)?  By: Sean Kerner\n ##### GPT-4  By: Ben Lutkevich\n\nSponsored News\n\nThree Innovative AI Use Cases for Natural Language Processing –Dell Technologies\nAutonomous coding: The future of the revenue cycle –Solventum\nHybrid Work Drives New Criteria for VDI and DaaS –Dell Technologies\n\nRelated Content\n\nExploring GPT-3 architecture – Search Enterprise AI\nWhat is GPT-3? Everything you need to know – Search Enterprise AI\nMicrosoft exclusively licenses OpenAI's GPT-3 ... – Search Enterprise AI\n\nLatest TechTarget resources\n\nNetworking\nSecurity\nCIO\nHR Software\nCustomer Experience\n\nSearch Networking\n\n\nWhat are port numbers and how do they work?A port number is a way to identify a specific process to which an internet or other network message is to be forwarded when it ...\n\n\nWhat is a router?A router is a physical or virtual appliance that passes information between two or more packet-switched computer networks.\n\n\nWhat is east-west traffic?East-west traffic refers to the transfer of data packets that move from server to server within a network's data center.\n\n\nSearch Security\n\n\nWhat is cyberstalking and how to prevent it?Cyberstalking is a crime in which someone harasses or stalks a victim using electronic or digital means, such as social media, ...\n\n\nWhat is a watering hole attack?A watering hole attack is a security exploit in which the attacker seeks to compromise a specific group of end users by infecting...\n\n\nWhat is multifactor authentication?Multifactor authentication (MFA) is an IT security technology that requires multiple sources of unique information from ...\n\n\nSearch CIO\n\n\nWhat is a think tank?A think tank is an organization that gathers a group of interdisciplinary scholars to perform research around particular policies...\n\n\nWhat is emotional intelligence (EI)?Emotional intelligence (EI) is the area of cognitive ability that facilitates interpersonal behavior.\n\n\nWhat are agreed-upon procedures (AUPs)?Agreed-upon procedures are a standard a company or client outlines in an engagement letter or other written agreement when it ...\n\n\nSearch HRSoftware\n\n\nWhat is gamification? How it works and how to use itGamification is a strategy that integrates entertaining and immersive gaming elements into nongame contexts to enhance engagement...\n\n\nWhat is employee self-service (ESS)?Employee self-service (ESS) is a widely used human resources technology that enables employees to perform many job-related ...\n\n\nWhat is DEI? Diversity, equity and inclusion explainedDiversity, equity and inclusion is a term used to describe policies and programs that promote the representation and ...\n\n\nSearch Customer Experience\n\n\nWhat is voice of the customer? A guide to VOC StrategyVoice of the customer (VOC) is the component of customer experience (CX) that focuses on customer needs, wants, expectations and ...\n\n\nWhat is high-touch customer service?High-touch customer service is a category of contact center interaction that requires human interaction.\n\n\nWhat is CRM (customer relationship management)?CRM (customer relationship management) is the combination of practices, strategies and technologies that companies use to manage ...\n\n\nBrowse by Topic\n\n\nBrowse Resources\n\n\nAbout Us\n\nMeet The Editors\nEditorial Ethics Policy\nContact Us\nAdvertisers\nBusiness Partners\nEvents\nMedia Kit\nCorporate Site\nReprints\n\nAll Rights Reserved, Copyright 1999 - 2025, TechTarget  \nPrivacy Policy\nCookie Preferences\nCookie Preferences\nDo Not Sell or Share My Personal Information\nClose"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build a Weather Tool"
      ],
      "metadata": {
        "id": "3km3-7WcnYk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "@tool\n",
        "def get_weather(query: str) -> list:\n",
        "    \"\"\"Search weatherapi to get the current weather.\"\"\"\n",
        "    base_url = \"http://api.weatherapi.com/v1/current.json\"\n",
        "    complete_url = f\"{base_url}?key={WEATHER_API_KEY}&q={query}\"\n",
        "\n",
        "    response = requests.get(complete_url)\n",
        "    data = response.json()\n",
        "    if data.get(\"location\"):\n",
        "        return data\n",
        "    else:\n",
        "        return \"Weather Data Not Found\""
      ],
      "metadata": {
        "id": "ZM8R-JgOnXdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_weather.invoke(\"Zurich\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12xdLSJ3ZUiv",
        "outputId": "27a3f799-7844-4944-9e15-d3f6b5ad1e54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'location': {'name': 'Zurich',\n",
              "  'region': '',\n",
              "  'country': 'Switzerland',\n",
              "  'lat': 47.3667,\n",
              "  'lon': 8.55,\n",
              "  'tz_id': 'Europe/Zurich',\n",
              "  'localtime_epoch': 1741429361,\n",
              "  'localtime': '2025-03-08 11:22'},\n",
              " 'current': {'last_updated_epoch': 1741428900,\n",
              "  'last_updated': '2025-03-08 11:15',\n",
              "  'temp_c': 11.1,\n",
              "  'temp_f': 52.0,\n",
              "  'is_day': 1,\n",
              "  'condition': {'text': 'Sunny',\n",
              "   'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png',\n",
              "   'code': 1000},\n",
              "  'wind_mph': 2.2,\n",
              "  'wind_kph': 3.6,\n",
              "  'wind_degree': 32,\n",
              "  'wind_dir': 'NNE',\n",
              "  'pressure_mb': 1015.0,\n",
              "  'pressure_in': 29.97,\n",
              "  'precip_mm': 0.0,\n",
              "  'precip_in': 0.0,\n",
              "  'humidity': 47,\n",
              "  'cloud': 0,\n",
              "  'feelslike_c': 11.5,\n",
              "  'feelslike_f': 52.6,\n",
              "  'windchill_c': 11.5,\n",
              "  'windchill_f': 52.8,\n",
              "  'heatindex_c': 11.2,\n",
              "  'heatindex_f': 52.1,\n",
              "  'dewpoint_c': -0.5,\n",
              "  'dewpoint_f': 31.1,\n",
              "  'vis_km': 10.0,\n",
              "  'vis_miles': 6.0,\n",
              "  'uv': 2.9,\n",
              "  'gust_mph': 2.6,\n",
              "  'gust_kph': 4.1}}"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import rich\n",
        "\n",
        "result = get_weather.invoke(\"Zurich\")\n",
        "rich.print_json(data=result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 769
        },
        "id": "dyL1L1ifn0mj",
        "outputId": "cd9d2eda-779a-46fb-9927-787cbde3932a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m{\u001b[0m\n",
              "  \u001b[1;34m\"location\"\u001b[0m: \u001b[1m{\u001b[0m\n",
              "    \u001b[1;34m\"name\"\u001b[0m: \u001b[32m\"Zurich\"\u001b[0m,\n",
              "    \u001b[1;34m\"region\"\u001b[0m: \u001b[32m\"\"\u001b[0m,\n",
              "    \u001b[1;34m\"country\"\u001b[0m: \u001b[32m\"Switzerland\"\u001b[0m,\n",
              "    \u001b[1;34m\"lat\"\u001b[0m: \u001b[1;36m47.3667\u001b[0m,\n",
              "    \u001b[1;34m\"lon\"\u001b[0m: \u001b[1;36m8.55\u001b[0m,\n",
              "    \u001b[1;34m\"tz_id\"\u001b[0m: \u001b[32m\"Europe/Zurich\"\u001b[0m,\n",
              "    \u001b[1;34m\"localtime_epoch\"\u001b[0m: \u001b[1;36m1741429361\u001b[0m,\n",
              "    \u001b[1;34m\"localtime\"\u001b[0m: \u001b[32m\"2025-03-08 11:22\"\u001b[0m\n",
              "  \u001b[1m}\u001b[0m,\n",
              "  \u001b[1;34m\"current\"\u001b[0m: \u001b[1m{\u001b[0m\n",
              "    \u001b[1;34m\"last_updated_epoch\"\u001b[0m: \u001b[1;36m1741428900\u001b[0m,\n",
              "    \u001b[1;34m\"last_updated\"\u001b[0m: \u001b[32m\"2025-03-08 11:15\"\u001b[0m,\n",
              "    \u001b[1;34m\"temp_c\"\u001b[0m: \u001b[1;36m11.1\u001b[0m,\n",
              "    \u001b[1;34m\"temp_f\"\u001b[0m: \u001b[1;36m52.0\u001b[0m,\n",
              "    \u001b[1;34m\"is_day\"\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
              "    \u001b[1;34m\"condition\"\u001b[0m: \u001b[1m{\u001b[0m\n",
              "      \u001b[1;34m\"text\"\u001b[0m: \u001b[32m\"Sunny\"\u001b[0m,\n",
              "      \u001b[1;34m\"icon\"\u001b[0m: \u001b[32m\"//cdn.weatherapi.com/weather/64x64/day/113.png\"\u001b[0m,\n",
              "      \u001b[1;34m\"code\"\u001b[0m: \u001b[1;36m1000\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[1;34m\"wind_mph\"\u001b[0m: \u001b[1;36m2.2\u001b[0m,\n",
              "    \u001b[1;34m\"wind_kph\"\u001b[0m: \u001b[1;36m3.6\u001b[0m,\n",
              "    \u001b[1;34m\"wind_degree\"\u001b[0m: \u001b[1;36m32\u001b[0m,\n",
              "    \u001b[1;34m\"wind_dir\"\u001b[0m: \u001b[32m\"NNE\"\u001b[0m,\n",
              "    \u001b[1;34m\"pressure_mb\"\u001b[0m: \u001b[1;36m1015.0\u001b[0m,\n",
              "    \u001b[1;34m\"pressure_in\"\u001b[0m: \u001b[1;36m29.97\u001b[0m,\n",
              "    \u001b[1;34m\"precip_mm\"\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
              "    \u001b[1;34m\"precip_in\"\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
              "    \u001b[1;34m\"humidity\"\u001b[0m: \u001b[1;36m47\u001b[0m,\n",
              "    \u001b[1;34m\"cloud\"\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
              "    \u001b[1;34m\"feelslike_c\"\u001b[0m: \u001b[1;36m11.5\u001b[0m,\n",
              "    \u001b[1;34m\"feelslike_f\"\u001b[0m: \u001b[1;36m52.6\u001b[0m,\n",
              "    \u001b[1;34m\"windchill_c\"\u001b[0m: \u001b[1;36m11.5\u001b[0m,\n",
              "    \u001b[1;34m\"windchill_f\"\u001b[0m: \u001b[1;36m52.8\u001b[0m,\n",
              "    \u001b[1;34m\"heatindex_c\"\u001b[0m: \u001b[1;36m11.2\u001b[0m,\n",
              "    \u001b[1;34m\"heatindex_f\"\u001b[0m: \u001b[1;36m52.1\u001b[0m,\n",
              "    \u001b[1;34m\"dewpoint_c\"\u001b[0m: \u001b[1;36m-0.5\u001b[0m,\n",
              "    \u001b[1;34m\"dewpoint_f\"\u001b[0m: \u001b[1;36m31.1\u001b[0m,\n",
              "    \u001b[1;34m\"vis_km\"\u001b[0m: \u001b[1;36m10.0\u001b[0m,\n",
              "    \u001b[1;34m\"vis_miles\"\u001b[0m: \u001b[1;36m6.0\u001b[0m,\n",
              "    \u001b[1;34m\"uv\"\u001b[0m: \u001b[1;36m2.9\u001b[0m,\n",
              "    \u001b[1;34m\"gust_mph\"\u001b[0m: \u001b[1;36m2.6\u001b[0m,\n",
              "    \u001b[1;34m\"gust_kph\"\u001b[0m: \u001b[1;36m4.1\u001b[0m\n",
              "  \u001b[1m}\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
              "  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"location\"</span>: <span style=\"font-weight: bold\">{</span>\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"name\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Zurich\"</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"region\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"\"</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"country\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Switzerland\"</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"lat\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">47.3667</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"lon\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.55</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"tz_id\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Europe/Zurich\"</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"localtime_epoch\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1741429361</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"localtime\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"2025-03-08 11:22\"</span>\n",
              "  <span style=\"font-weight: bold\">}</span>,\n",
              "  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"current\"</span>: <span style=\"font-weight: bold\">{</span>\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"last_updated_epoch\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1741428900</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"last_updated\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"2025-03-08 11:15\"</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"temp_c\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11.1</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"temp_f\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">52.0</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"is_day\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"condition\"</span>: <span style=\"font-weight: bold\">{</span>\n",
              "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"text\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Sunny\"</span>,\n",
              "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"icon\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"//cdn.weatherapi.com/weather/64x64/day/113.png\"</span>,\n",
              "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"code\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1000</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"wind_mph\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.2</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"wind_kph\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.6</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"wind_degree\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"wind_dir\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"NNE\"</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"pressure_mb\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1015.0</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"pressure_in\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29.97</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"precip_mm\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"precip_in\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"humidity\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">47</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"cloud\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"feelslike_c\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11.5</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"feelslike_f\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">52.6</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"windchill_c\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11.5</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"windchill_f\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">52.8</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"heatindex_c\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11.2</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"heatindex_f\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">52.1</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"dewpoint_c\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.5</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"dewpoint_f\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31.1</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"vis_km\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10.0</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"vis_miles\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.0</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"uv\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.9</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"gust_mph\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.6</span>,\n",
              "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"gust_kph\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.1</span>\n",
              "  <span style=\"font-weight: bold\">}</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explore LLM tool calling with custom tools\n",
        "\n",
        "An agent is basically an LLM which has the capability to automatically call relevant functions to perform complex or tool-based tasks based on input human prompts.\n",
        "\n",
        "Tool calling also popularly known as function calling is the ability to reliably enable such LLMs to call external tools and APIs.\n",
        "\n",
        "We will leverate the custom tools we created earlier in the previous section and try to see if the LLM can automatically call the right tools based on input prompts"
      ],
      "metadata": {
        "id": "bIOhB430gpW9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tool calling for LLMs"
      ],
      "metadata": {
        "id": "4Y26Ohn3P54j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tool calling allows a model to respond to a given prompt by generating output that matches a user-defined schema. While the name implies that the model is performing some action, this is actually not the case! The model is coming up with the arguments to a tool, and actually running the tool (or not) is up to the user or agent defined by the user.\n",
        "\n",
        "Many LLM providers, including Anthropic, Cohere, Google, Mistral, OpenAI, and others, support variants of a tool calling feature. These features typically allow requests to the LLM to include available tools and their schemas, and for responses to include calls to these tools.\n",
        "\n"
      ],
      "metadata": {
        "id": "1nACq0NgL5yM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chatgpt = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
      ],
      "metadata": {
        "id": "s0wCNpzCBvtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [multiply, search_web_extract_info, get_weather]\n",
        "chatgpt_with_tools = chatgpt.bind_tools(tools)"
      ],
      "metadata": {
        "id": "rjKWxFNgB2t_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLMs are still not perfect in tool calling so you might need to play around with the following prompt\n",
        "prompt = \"\"\"\n",
        "            Given only the tools at your disposal, mention tool calls for the following tasks:\n",
        "            Do not change the query given for any search tasks\n",
        "            1. What is 2.1 times 3.5\n",
        "            2. What is the current weather in Greenland today\n",
        "            3. What are the top LLMs released in 2025\n",
        "         \"\"\"\n",
        "\n",
        "results = chatgpt_with_tools.invoke(prompt)"
      ],
      "metadata": {
        "id": "hJ271K_tB9K7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "kLoZWknjCNlC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75bc0a01-2e10-485f-f453-054175697294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_KTXXcEOIpVvSQTyQXofEGRhR', 'function': {'arguments': '{\"a\": 2.1, \"b\": 3.5}', 'name': 'multiply'}, 'type': 'function'}, {'id': 'call_VuPvwHEaIzLCkqBDKjrKjQnr', 'function': {'arguments': '{\"query\": \"Greenland\"}', 'name': 'get_weather'}, 'type': 'function'}, {'id': 'call_SSV6y5dsRub04UPoltPidTor', 'function': {'arguments': '{\"query\": \"top LLMs released in 2025\"}', 'name': 'search_web_extract_info'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 77, 'prompt_tokens': 184, 'total_tokens': 261, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb9dce56a8', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-be935ade-1d5c-4dcd-95d5-83709e52b5b6-0', tool_calls=[{'name': 'multiply', 'args': {'a': 2.1, 'b': 3.5}, 'id': 'call_KTXXcEOIpVvSQTyQXofEGRhR', 'type': 'tool_call'}, {'name': 'get_weather', 'args': {'query': 'Greenland'}, 'id': 'call_VuPvwHEaIzLCkqBDKjrKjQnr', 'type': 'tool_call'}, {'name': 'search_web_extract_info', 'args': {'query': 'top LLMs released in 2025'}, 'id': 'call_SSV6y5dsRub04UPoltPidTor', 'type': 'tool_call'}], usage_metadata={'input_tokens': 184, 'output_tokens': 77, 'total_tokens': 261, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results.tool_calls"
      ],
      "metadata": {
        "id": "ckZz5pcpCQau",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51210802-80b3-4f3f-8f4f-989127a79945"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'multiply',\n",
              "  'args': {'a': 2.1, 'b': 3.5},\n",
              "  'id': 'call_KTXXcEOIpVvSQTyQXofEGRhR',\n",
              "  'type': 'tool_call'},\n",
              " {'name': 'get_weather',\n",
              "  'args': {'query': 'Greenland'},\n",
              "  'id': 'call_VuPvwHEaIzLCkqBDKjrKjQnr',\n",
              "  'type': 'tool_call'},\n",
              " {'name': 'search_web_extract_info',\n",
              "  'args': {'query': 'top LLMs released in 2025'},\n",
              "  'id': 'call_SSV6y5dsRub04UPoltPidTor',\n",
              "  'type': 'tool_call'}]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multiply"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLxNMPTegDed",
        "outputId": "4de97360-94ca-4c36-c258-38592135e3d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructuredTool(name='multiply', description='use to multiply numbers', args_schema=<class '__main__.CalculatorInput'>, return_direct=True, func=<function multiply at 0x7fa0f8b6d8a0>)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "toolkit = {\n",
        "    \"multiply\": multiply,\n",
        "    \"search_web_extract_info\": search_web_extract_info,\n",
        "    \"get_weather\": get_weather\n",
        "}\n",
        "\n",
        "for tool_call in results.tool_calls:\n",
        "    selected_tool = toolkit[tool_call[\"name\"].lower()]\n",
        "    print(f\"Calling tool: {tool_call['name']}\")\n",
        "    tool_output = selected_tool.invoke(tool_call[\"args\"])\n",
        "    print(tool_output)\n",
        "    print()"
      ],
      "metadata": {
        "id": "POvGp_xZCpSg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eac5c6f9-1dd9-4611-d279-d34432e01eae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling tool: multiply\n",
            "7.3500000000000005\n",
            "\n",
            "Calling tool: get_weather\n",
            "{'location': {'name': 'Nuuk', 'region': 'Vestgronland', 'country': 'Greenland', 'lat': 64.183, 'lon': -51.75, 'tz_id': 'America/Nuuk', 'localtime_epoch': 1741429362, 'localtime': '2025-03-08 08:22'}, 'current': {'last_updated_epoch': 1741428900, 'last_updated': '2025-03-08 08:15', 'temp_c': -2.7, 'temp_f': 27.1, 'is_day': 1, 'condition': {'text': 'Blowing snow', 'icon': '//cdn.weatherapi.com/weather/64x64/day/227.png', 'code': 1114}, 'wind_mph': 12.8, 'wind_kph': 20.5, 'wind_degree': 60, 'wind_dir': 'ENE', 'pressure_mb': 1001.0, 'pressure_in': 29.56, 'precip_mm': 0.36, 'precip_in': 0.01, 'humidity': 86, 'cloud': 100, 'feelslike_c': -8.7, 'feelslike_f': 16.3, 'windchill_c': -10.9, 'windchill_f': 12.4, 'heatindex_c': -4.4, 'heatindex_f': 24.1, 'dewpoint_c': -6.1, 'dewpoint_f': 21.0, 'vis_km': 10.0, 'vis_miles': 6.0, 'uv': 0.0, 'gust_mph': 23.8, 'gust_kph': 38.3}}\n",
            "\n",
            "Calling tool: search_web_extract_info\n",
            "['## Title\\n\\nBest 39 Large Language Models (LLMs) in 2025 - Exploding Topics\\n\\n## Content\\n\\nBest 22 Large Language Models (LLMs) (February 2025)\\n\\n\\n\\nAbout\\nNewsletter\\nBlog\\n\\n\\nBest 22 Large Language Models (LLMs) (February 2025)\\n\\nby Anthony Cardillo\\nFebruary 7, 2025\\nLarge language models are pre-trained on large datasets and use natural language processing to perform linguistic tasks such as text generation, code completion, paraphrasing, and more.\\nThe initial release of ChatGPT sparked the rapid adoption of generative AI, which has led to large language model innovations and industry growth.\\nIn fact, 92% of Fortune 500 firms have started using generative AI in their workflows.\\nAs adoption continues to grow, so does the LLM industry. The global large language model market is projected to grow from $6.5 billion in 2024 to $140.8 billion by 2033.\\nWith that, here is a list of the top 21 LLMs available in September 2024.\\nLLM NameDeveloperRelease DateAccessParametersDeepSeek R1DeepSeekJanuary 20, 2025Open-Source671 billionGPT-4oOpenAIMay 13, 2024APIUnknownClaude 3.5AnthropicJune 20, 2024APIUnknownGrok-1xAINovember 4, 2023Open-Source314 billionMistral 7BMistral AISeptember 27, 2023Open-Source7.3 billionPaLM 2GoogleMay 10, 2023Open-Source340 billionFalcon 180BTechnology Innovation InstituteSeptember 6, 2023Open-Source180 billionStable LM 2Stability AIJanuary 19, 2024Open-Source1.6 billion, 12 billionGemini 1.5Google DeepMindFebruary 2nd, 2024APIUnknownLlama 3.1Meta AIJune 23, 2024Open-Source405 billionMixtral 8x22BMistral AIApril 10, 2024Open-Source141 billionInflection-2.5Inflection AIMarch 10, 2024ProprietaryUnknownJambaAI21 LabsMarch 29, 2024Open-Source52 billionCommand RCohereMarch 11, 2024Both35 billionGemmaGoogle DeepMindFebruary 21, 2024Open-Source2 billion, 7 billionPhi-3MicrosoftApril 23, 2024Both3.8 billionXGen-7BSalesforceJuly 3, 2023Open-Source7 billionDBRXDatabricks\\' Mosaic MLMarch 27, 2024Open-Source132 billionPythiaEleutherAIFebruary 13, 2023Open-Source70 million to 12 billionSoraOpenAIFebruary 15, 2024 (announced)APIUnknownAlpaca 7BStanford CRFMMarch 13, 2023Open-Source7 billionNemotron-4NvidiaJune 14, 2024Open-Source340 billion\\n1. DeepSeek R1\\n\\nDeveloper: DeepSeek\\nRelease date: January 2025\\nNumber of Parameters: 671B total, 37B active\\nWhat is it? DeepSeek R1 is a reasoning model that excels in math and coding. It beats or matches OpenAI o1 in several benchmarks, including MATH-500 and AIME 2024.\\nOn its release, DeepSeek immediately hit headlines due to the low cost of training compared to most major LLMs.\\nDeepSeek R1 is free to use and open-source. It\\'s accessible via the API, the DeepSeek website, and mobile apps.\\n2. GPT-4o\\n\\nDeveloper: OpenAI\\nRelease date: May 13, 2024\\nNumber of Parameters: Unknown\\nWhat is it? GPT-4o is the latest and most advanced OpenAI language model, succeeding GPT-4, GPT-3.5, and GPT-3. OpenAI claims that GPT-4o is 50% cheaper than GPT-4 despite being 2x faster at generating tokens. This multimodal model includes text, image, video, and voice capabilities packaged into one.\\nGPT-4o\\'s biggest upgrade is the Voice-to-Voice function, which will improve input response times to an average of 320 milliseconds (compared to a few seconds with GPT-4). This feature is expected to be released in the coming weeks.\\n3. Claude 3.5\\n\\nDeveloper: Anthropic\\nRelease date: March 14, 2024\\nNumber of Parameters: Unknown\\nWhat is it?\\xa0As a new upgrade from the highly rated\\xa0Claude 3, Claude 3.5 Sonnet is the first release of the new Claude 3.5 model family. Similar to Claude 3, it\\'ll also include the Haiku and Opus models. As debatably the biggest competitor to GPT-4 and ChatGPT, Claude made even bigger improvements to this model by maintaining the 200,000 token context window at a lower cost. This is much larger than GPT-4\\'s 32,000 token capabilities.\\nAccording to Anthropic\\'s report, Claude 3.5 Sonnet outperformed GPT-4o in major benchmarks like coding and text reasoning. Plus, this is Claude\\'s most advanced vision model, with the ability to transcribe text from images or generate insights from charts.\\nAmazon has invested over $4 billion in Anthropic, bringing the startup\\'s valuation to $15 billion. The Claude mobile app was also released in May 2024.\\n4. Grok-1\\n\\nDeveloper: xAI\\nRelease date: November 4, 2023\\nNumber of Parameters: 314 billion\\nWhat is it? Created by Elon Musk\\'s artificial intelligence startup xAI, Grok-1 is currently the largest open-source LLM released to date at 314 billion parameters. Grok directly integrates with X (Twitter), and users must pay for an X Premium+ subscription to gain access.\\nBecause of the model’s size, Grok has a mixture-of-experts (MoE) architecture that only uses 25% of its weights for any given input token to maximize calculation efficiency.\\nIn August 2024, both Grok-2 and Grok-2 mini were released to X users in beta. According to xAI\\'s reports, Grok-2 outperforms GPT-4o in numerous categories, such as GPQA, MMLU-Pro, and DocVQA.\\n5. Mistral 7B\\n\\nDeveloper: Mistral AI\\nRelease date: September 27, 2023\\nNumber of Parameters: 7.3 billion\\nWhat is it? Mistral 7B is an open-source language model with 32 layers, 32 attention heads, and eight key-value heads. Despite running with fewer parameters, they outperformed the Llama 2 family of models in nearly all metrics, including MMLU, reading comprehension, math, coding, etc.\\nMistral 7B is released under an Apache 2.0 license. Customers are free to download it locally, deploy it on the cloud, or run it on HuggingFace. The Paris-based startup is close to securing a new $600 million funding round that would value the company at $6 billion.\\n6. PaLM 2\\n\\nDeveloper: Google\\nRelease date: May 10, 2023\\nNumber of Parameters: 340 billion\\nWhat is it? PaLM 2 is an advanced large language model developed by Google. As the successor to the original Pathways Language Model (PaLM), it’s trained on 3.6 trillion tokens (compared to 780 billion) and 340 billion parameters (compared to 540 billion). PaLM 2 was originally used to power Google\\'s first generative AI chatbot, Bard (rebranded to Gemini in February 2024).\\n7. Falcon 180B\\n\\nDeveloper: Technology Innovation Institute (TII)\\nRelease date: September 6, 2023\\nNumber of Parameters: 180 billion\\nWhat is it? Developed and funded by the Technology Innovation Institute, Falcon 180B is an upgraded version of the earlier Falcon 40B LLM. It has 180 billion parameters, which is 4.5 times larger than the 40 billion parameters of Falcon 40B.\\nIn addition to Falcon 40B, it also outperforms other large language models like GPT-3.5 and LLaMA 2 on tasks such as reasoning, question answering, and coding. In February 2024, the UAE-based Technology Innovation Institute (TII) committed $300 million in funding to the Falcon Foundation.\\n8. Stable LM 2\\n\\nDeveloper: Stability AI\\nRelease date: January 19, 2024\\nNumber of Parameters: 1.6 billion and 12 billion\\nWhat is it? Stability AI, the creators of the Stable Diffusion text-to-image model, are the developers behind Stable LM 2. This series of large language models includes Stable LM 2 12B (12 billion parameters) and Stable LM 2 1.6B (1.6 billion parameters). Released in April 2024, the larger 12B model outperforms models like LLaMA 2 70B on key benchmarks despite being much smaller.\\n9. Gemini 1.5\\n\\nDeveloper: Google DeepMind\\nRelease date: February 2nd, 2024\\nNumber of Parameters: Unknown\\nWhat is it? Gemini 1.5 is Google\\'s next-generation large language model, offering a significant upgrade over its predecessor, Gemini 1.0. While it’s only available for early testing, Gemini 1.5 Pro provides a one million-token context window (1 hour of video, 700,000 words, or 30,000 lines of code), the largest to date compared to alternative LLMs and chatbots. This upgrade is 35 times larger than Gemini 1.0 Pro and surpasses the previous largest record of 200,000 tokens held by Anthropic’s Claude 2.1.\\n10. Llama 3.1\\n\\nDeveloper: Meta AI\\nRelease date: June 23, 2024\\nNumber of Parameters: 405 billion\\nWhat is it? Llama 3, the predecessor to Llama 3.1, was available in both 70B and 8B versions that outperformed other open-source models like Mistral 7B and Google\\'s Gemma 7B on MMLU, reasoning, coding, and math benchmarks. Now, users will notice major upgrades to the latest version, including 405 billion parameters and an expended context length of 128,000.\\nUsers will also notice more accuracy because of the impressive knowledge base, which has been trained on over 15 trillion tokens. Plus, Meta added eight additional languages for this model. The increased size of this model makes it the largest open-source model released to date.\\nCustomers can still access its predecessor, Llama 2, which is available in three versions: 7 billion, 13 billion, and 70 billion parameters.\\n11. Mixtral 8x22B\\n\\nDeveloper: Mistral AI\\nRelease date: April 10, 2024\\nNumber of Parameters: 141 billion\\nWhat is it? Mixtral 8x22B is Mistral AI\\'s latest and most advanced large language model. This sparse Mixture-of-Experts (SMoE) model has 141 billion total parameters but only uses 39B active parameters to focus on improving the model’s performance-to-cost ratio.\\nThe startup also recently released Mistral Large, a ChatGPT alternative that ranks second behind GPT-4 among API-based LLMs.\\n12. Inflection-2.5\\n\\nDeveloper: Inflection AI\\nRelease date: March 10, 2024\\nNumber of Parameters: Unknown\\nWhat is it? Inflection-2.5 is the latest large language model (LLM) developed by Inflection AI to power its conversational AI assistant, Pi. Significant upgrades have been made, as the model currently achieves over 94% of GPT-4’s average performance while only having 40% of the training FLOPs. In March 2024, the Microsoft-backed startup reached 1+ million daily active users on Pi.\\n13. Jamba\\n\\nDeveloper: AI21 Labs\\nRelease date: March 29, 2024\\nNumber of Parameters: 52 billion\\nWhat is it? AI21 Labs created Jamba, the world\\'s first production-grade Mamba-style large language model. It integrates SSM technology with elements of a traditional transformer model to create a hybrid architecture. The model is efficient and highly scalable, with a context window of 256K and deployment support of 140K context on a single GPU.\\n14. Command R\\n\\nDeveloper: Cohere\\nRelease date: March 11, 2024\\nNumber of Parameters: 35 billion\\nWhat is it? Command R is a series of scalable LLMs from Cohere that support ten languages and 128,000-token context length (around 100 pages of text). This model primarily excels at retrieval-augmented generation, code-related tasks like explanations or rewrites, and reasoning. In April 2024, Command R+ was released to support larger workloads and provide real-world enterprise support.\\n15. Gemma\\n\\nDeveloper: Google DeepMind\\nRelease date: February 21, 2024\\nNumber of Parameters: 2 billion and 7 billion\\nWhat is it? Gemma is a series of lightweight open-source language models developed and released by Google DeepMind. The Gemma models are built with similar tech to the Gemini models, but Gemma is limited to text inputs and outputs only. The models have a context window of 8,000 tokens and are available in 2 billion and 7 billion parameter sizes.\\n16. Phi-3\\n\\nDeveloper: Microsoft\\nRelease date: April 23, 2024\\nNumber of Parameters: 3.8 billion\\nWhat is it? Classified as a small language model (SLM), Phi-3 is Microsoft\\'s latest release with 3.8 billion parameters. Despite the smaller size, it\\'s been trained on 3.3 trillion tokens of data to compete with Mistral 8x7B and GPT-3.5 performance on MT-bench and MMLU benchmarks.\\nTo date, Phi-3-mini is the only model available. However, Microsoft plans to release the Phi-3-small and Phi-3-medium models later this year.\\n17. XGen-7B\\n\\nDeveloper: Salesforce\\nRelease date: July 3, 2023\\nNumber of Parameters: 7 billion\\nWhat is it? XGen-7B is a large language model from Salesforce with 7 billion parameters and an 8k context window. The model was trained on 1.37 trillion tokens from various sources, such as RedPajama, Wikipedia, and Salesforce\\'s own Starcoder dataset.\\nSalesforce has released two open-source versions, a 4,000 and 8,000 token context window base, hosted under an Apache 2.0 license.\\n18. DBRX\\n\\nDeveloper: Databricks\\' Mosaic ML\\nRelease date: March 27, 2024\\nNumber of Parameters: 132 billion\\nWhat is it? DBRX is an open-source LLM built by Databricks and the Mosaic ML research team. The mixture-of-experts architecture has 36 billion (of 132 billion total) active parameters on an input. DBRX has 16 experts and chooses 4 of them during inference, providing 65 times more expert combinations compared to similar models like Mixtral and Grok-1\\n19. Pythia\\n\\nDeveloper: EleutherAI\\nRelease date: February 13, 2023\\nNumber of Parameters: 70 million to 12 billion\\nWhat is it? Pythia is a series of 16 large language models developed and released by EleutherAI, a non-profit AI research lab. There are eight different model sizes: 70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, and 12B. Because of Pythia\\'s open-source license, these LLMs serve as a base model for fine-tuned, instruction-following LLMs like Dolly 2.0 by Databricks.\\n20. Sora\\n\\nDeveloper: OpenAI\\nRelease date: February 15, 2024 (announced)\\nNumber of Parameters: Unknown\\nWhat is it? OpenAI\\'s latest development is Sora, a text-to-video model that combines LLMs and generative AI to turn text prompts into realistic videos up to 60 seconds long. The model uses a transformer architecture that operates on \"spacetime patches\" of video and image data rather than text tokens like other LLMs. No official release date for Sora has been announced, but OpenAI expects it to open to the public in late 2024.\\n21. Alpaca 7B\\n\\nDeveloper: Stanford CRFM\\nRelease date: March 27, 2024\\nNumber of Parameters: 7 billion\\nWhat is it? Alpaca is a 7 billion-parameter language model developed by a Stanford research team and fine-tuned from Meta\\'s LLaMA 7B model. Users will notice that although being much smaller, Alpaca performs similarly to text-DaVinci-003 (ChatGPT 3.5). However, Alpaca 7B is available for research purposes, and no commercial licenses are available.\\n22. Nemotron-4 340B\\n\\nDeveloper: NVIDIA\\nRelease date: June 14, 2024\\nNumber of Parameters: 340 billion\\nWhat is it? Nemotron-4 340B\\xa0is a family of large language models for synthetic data generation and AI model training. These models help businesses create new LLMs without larger and more expensive datasets. Instead, Nemotron-4 can create high-quality synthetic data to train other AI models, which reduces the need for extensive human-annotated data.\\nThe model family includes Nemotron-4-340B-Base (foundation model), Nemotron-4-340B-Instruct (fine-tuned chatbot), and Nemotron-4-340B-Reward (quality assessment and preference ranking). Due to the 9 trillion tokens used in training, which includes English, multilingual, and coding language data, Nemotron-4 matches GPT-4\\'s high-quality synthetic data generation capabilities.\\nConclusion\\nThe landscape of large language models is rapidly evolving, with new breakthroughs and innovations emerging at an unprecedented pace.\\nFrom compact models like Phi-2 and Alpaca 7B to cutting-edge architectures like Jamba and DBRX, the field of LLMs is pushing the boundaries of what\\'s possible in natural language processing (NLP).\\nWe will keep this list regularly updated with new models. If you liked learning about these LLMs, check out our lists of generative AI startups and AI startups.\\nFind Thousands of Trending Topics With Our Platform\\nTry Exploding Topics Pro\\n\\nExploding Topics\\n\\nJoin Pro\\nNewsletter\\nTrending Topics\\nAdd a Topic\\nCustomer Login\\n\\nCompany\\n\\nAbout Us\\nContact\\nMethodology\\nCookie Settings\\n\\nFree Tools\\n\\nKeyword Research\\nBacklink Checker\\nSERP Checker\\nKeyword Rank Checker\\nFree SEO Tools\\n\\nConnect\\n\\nYouTube\\nInstagram\\nX (Twitter)\\n\\nResources\\n\\nBlog\\nMarketing Academy\\nFree Webinars\\n\\n\\n\\n© 2025 \\xa0Exploding Topics is a Trademark of Semrush Inc\\n\\nPrivacy Policy\\nTerms of Service\\n', '## Title\\n\\n25 of the best large language models in 2025 - TechTarget\\n\\n## Content\\n\\nPublished Time: 2025-01-31T06:00Z\\n25 of the best large language models in 2025\\nWhatIs\\nSearch the TechTarget Network \\nBrowse Definitions :\\n\\nA\\nB\\nC\\nD\\nE\\nF\\nG\\nH\\nI\\nJ\\nK\\nL\\nM\\nN\\nO\\nP\\nQ\\nR\\nS\\nT\\nU\\nV\\nW\\nX\\nY\\nZ\\n#\\n\\nLogin Register\\n\\nTechTarget Network\\nTech Accelerator\\nNews\\n2024 IT Salary Survey Results\\n\\nRSS\\n\\n\\nWhatIs\\n\\n\\nBrowse Definitions Data analytics and AI\\nTopics View All\\n\\nBusiness software\\nCloud computing\\nComputer science\\nData centers\\nIT management\\nNetworking\\nSecurity\\nSoftware development\\n\\nPlease select a category\\n\\nTopics\\n\\n\\n\\nBrowse Features Resources\\n\\nBusiness strategies\\nCareer resources\\nEmerging tech\\nTech explainers\\n\\n\\n\\nFollow:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHome\\n\\nData analytics and AI\\n\\nTech Accelerator What is Gen AI? Generative AI explained\\nPrev Next Will AI replace jobs? 17 job types that might be affected Pros and cons of AI-generated content\\nDownload this guide1\\nX\\nFree Download What is generative AI? Everything you need to know\\nThe potential of AI technology has been percolating in the background for years. But when ChatGPT, the AI chatbot, began grabbing headlines in early 2023, it put generative AI in the spotlight. This guide is your go-to manual for generative AI, covering its benefits, limits, use cases, prospects and much more.\\nFeature\\n25 of the best large language models in 2025\\nLarge language models have been affecting search for years and have been brought to the forefront by ChatGPT and other chatbots.\\n\\nShare this item with your network:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy\\n\\nSean Michael Kerner\\nBen Lutkevich, Site Editor\\n\\nPublished: 31 Jan 2025\\nLarge language models are the dynamite behind the\\xa0generative AI\\xa0boom. However, they\\'ve been around for a while.\\nLLMs\\xa0are black box AI systems that use deep learning on extremely large datasets to understand and generate new text. Modern LLMs began taking shape in 2014 when the attention mechanism -- a machine learning technique designed to mimic human cognitive attention -- was introduced in a\\xa0research paper\\xa0titled \"Neural Machine Translation by Jointly Learning to Align and Translate.\" In 2017, that attention mechanism was honed with the introduction of the transformer model in another\\xa0paper, \"Attention Is All You Need.\"\\nSome of the most well-known language models today are based on the transformer model, including the\\xa0generative pre-trained transformer series\\xa0of LLMs and bidirectional encoder representations from transformers (BERT).\\nChatGPT, which runs on a set of language models from OpenAI, attracted more than 100 million users just two months after its release in 2022. Since then, many competing models have been released. Some belong to big companies such as Google, Amazon and Microsoft; others are open source.\\nConstant developments in the field can be difficult to keep track of. Here are some of the most influential models, both past and present. Included in it are models that paved the way for today\\'s leaders as well as those that could have a significant effect in the future.\\nThis article is part of\\nWhat is Gen AI? Generative AI explained\\n\\nWhich also includes:\\n8 top generative AI tool categories for 2025\\nWill AI replace jobs? 17 job types that might be affected\\n25 of the best large language models in 2025\\n\\nTop current LLMs\\nBelow are some of the most relevant large language models today. They do natural language processing and influence the architecture of future models.\\nBERT\\nBERT\\xa0is a family of LLMs that Google introduced in 2018. BERT is a\\xa0transformer-based\\xa0model that can convert sequences of data to other sequences of data. BERT\\'s architecture is a stack of transformer encoders and features 342 million parameters. BERT was pre-trained on a large corpus of data then fine-tuned to perform specific tasks along with natural language inference and sentence text similarity. It was used to improve query understanding in the 2019 iteration of Google search.\\nClaude\\nThe\\xa0Claude LLM\\xa0focuses on constitutional AI, which shapes AI outputs guided by a set of principles that help the AI assistant it powers helpful, harmless and accurate. Claude was created by the company Anthropic.\\nThere are three primary branches of Claude -- Opus, Haiku and Sonnet. The latest iteration of the Claude LLM is the Claude 3.5 Sonnet. It understands nuance, humor and complex instructions better than earlier versions of the LLM. It also has broad programming capabilities that make it well-suited for application development. In October 2024, Claude added a computer-use AI tool, that enables the LLM to use a computer like a human does. It\\'s available via Claude.ai, the Claude iOS app and through an API.\\nCohere\\nCohere is an enterprise AI platform that provides several LLMs including Command, Rerank and Embed. These\\xa0LLMs can be custom-trained\\xa0and fine-tuned to a specific company\\'s use case. The company that created the Cohere LLM was founded by one of the authors of Attention Is All You Need.\\nDeepSeek-R1\\nDeepSeek-R1 is an open-source reasoning model for tasks with complex reasoning, mathematical problem-solving and logical inference. The model uses reinforcement learning techniques to refine its reasoning ability and solve complex problems. DeepSeek-R1 can perform critical problem-solving through self-verification, chain-of-thought reasoning and reflection.\\nErnie\\nErnie is Baidu\\'s large language model which powers the Ernie 4.0 chatbot. The bot was released in August 2023 and has garnered more than 45 million users. Ernie is rumored to have 10 trillion parameters. The bot works best in Mandarin but is capable in other languages.\\nFalcon\\nFalcon is a family of transformer-based models developed by the Technology Innovation Institute. It is open source and has multi-lingual capabilities. Falcon 2 is available in an 11 billion parameter version that provide multimodal capabilities for both text and vision.\\nThe Falcon 1 series includes a pair of larger models with Falcon 40B and Falcon 180B. Falcon models are available on GitHub as well as on cloud provider including Amazon.\\nGemini\\nGemini\\xa0is Google\\'s family of LLMs that power the company\\'s chatbot of the same name. The model replaced Palm in powering the chatbot, which was rebranded from Bard to Gemini upon the model switch. Gemini models are multimodal, meaning they can handle images, audio and video as well as text. Gemini is also integrated in many Google applications and products. It comes in three sizes -- Ultra, Pro and Nano. Ultra is the largest and most capable model, Pro is the mid-tier model and Nano is the smallest model, designed for efficiency with on-device tasks.\\nAmong the most recent models is the Gemini 1.5 Pro update that debuted in May 2024 Gemini is available as a web chatbot, the Google Vertex AI service and via API. Early previews of Gemini 2.0 Flash became available in December 2024 with updated multimodal generation capabilities.\\nGemma\\nGemma\\xa0is a family of open-source language models from Google that were trained on the same resources as Gemini. Gemma 2 was released in June 2024 in two sizes -- a 9 billion parameter model and a 27 billion parameter model. Gemma models can be\\xa0run locally\\xa0on a personal computer, and are also available in Google Vertex AI.\\nGPT-3\\nGPT-3\\xa0is OpenAI\\'s large language model with more than 175 billion parameters, released in 2020. GPT-3 uses a decoder-only transformer architecture. In September 2022, Microsoft announced it had exclusive use of GPT-3\\'s underlying model. GPT-3 is 10 times larger than its predecessor. GPT-3\\'s training data includes Common Crawl, WebText2, Books1, Books2 and Wikipedia.\\nGPT-3 is the last of the GPT series of models in which OpenAI made the parameter counts publicly available. The GPT series was first introduced in 2018 with OpenAI\\'s paper \"Improving Language Understanding by Generative Pre-Training.\"\\nGPT-3.5\\nGPT-3.5 is an upgraded version of GPT-3 with fewer parameters. GPT-3.5 was fine-tuned using\\xa0reinforcement learning from human feedback. GPT-3.5 is the version of GPT that powers ChatGPT. There are several models, with GPT-3.5 turbo being the most capable, according to OpenAI. GPT-3.5\\'s training data extends to September 2021.\\nIt was also integrated into the Bing search engine but has since been replaced with GPT-4.\\nGPT-4\\nGPT-4\\xa0, was released in 2023 and like the others in the OpenAI GPT family, it\\'s a\\xa0transformer-based model. Unlike the others, its parameter count has not been released to the public, though there are rumors that the model has more than 170 trillion. OpenAI describes GPT-4 as a multimodal model, meaning it can\\xa0process and generate both language and images\\xa0as opposed to being limited to only language. GPT-4 also introduced a system message, which lets users specify tone of voice and task.\\nGPT-4 demonstrated human-level performance in multiple academic exams. At the model\\'s release, some speculated that GPT-4 came close to\\xa0artificial general intelligence, which means it is as smart or smarter than a human. That speculation turned out to be unfounded.\\nGPT-4o\\nGPT-4 Omni (GPT-4o) is OpenAI\\'s successor to GPT-4 and offers several improvements over the previous model. GPT-4o creates a more natural human interaction for ChatGPT and is a large multimodal model, accepting various inputs including audio, image and text. The conversations let users engage as they would in a normal human conversation, and the real-time interactivity can also pick up on emotions. GPT-4o can see photos or screens and ask questions about them during interaction.\\nGPT-4o can respond in 232 milliseconds, similar to human response time and faster than GPT-4 Turbo.\\nGranite\\nThe IBM Granite family of models are fully open source models under the Apache v.2 license. The first iteration of the open source model models debuted in May 2024, followed by Granite 3.0 in October and Granite 3.1 in December 2024.\\nThere are multiple variants in the Granite model family including General-purpose models (8B and 2B variants), guardrail model and Mixture-of-Experts models. While the model can be used for general purpose deployments, IBM itself is focusing deployment and optimization for enterprise use cases like customer service, IT automation and cybersecurity.\\nLamda\\nLamda (Language Model for Dialogue Applications) is a family of LLMs developed by Google Brain announced in 2021. Lamda used a decoder-only transformer language model and was pre-trained on a large corpus of text. In 2022, LaMDA gained widespread attention when then-Google engineer Blake Lemoine went public with claims that the\\xa0program was sentient. It was built on the Seq2Seq architecture.\\nLlama\\nLarge Language Model Meta AI (Llama) is Meta\\'s LLM which was first released in 2023. The Llama 3.1 models were released in July 2024, including both a 405 billion and 70 billion parameter model.\\nThe most recent version is Llama 3.2 which was released in September 2024, initially with smaller parameter counts of 11 billion and 90 billion.\\nLlama uses a transformer architecture and was trained on a variety of public data sources, including webpages from CommonCrawl, GitHub, Wikipedia and Project Gutenberg. Llama was effectively leaked and spawned many descendants, including Vicuna and Orca. Llama is available under an open license, allowing for free use of the models. Lllama models are available in many locations including llama.com and Hugging Face.\\nMistral\\nMistral is a family of a mixture of expert models from Mistral AI. Among the newest models is Mistral Large 2 which was first released in July 2024. The model operates with 123 billion parameters and a 128k context window, supporting dozens of languages including French, German, Spanish, Italian, and many others, along with more than 80 coding languages.\\nIn November 2024, Mistral released Pixtral Large, a 124-billion-parameter multimodal model that can handle text and visual data. Mistral models are available via Mistral\\'s API on its Le Platforme-managed web service.\\no1\\nThe OpenAI o1 model family was first introduced in Sept. 2024. The o1 model\\'s focus is to provide what OpenAI refers to as - reasoning models, that can reason through a problem or query before offering a response.\\nThe o1 models excel in STEM fields, with strong results in mathematical reasoning (scoring 83% on the International Mathematics Olympiad compared to GPT-4o\\'s 13%), code generation and scientific research tasks. While they offer enhanced reasoning and improved safety features, they operate more slowly than previous models due to their thorough reasoning processes and come with certain limitations, such as restricted access features and higher API costs. The models are available to ChatGPT Plus and Team users, with varying access levels for different user categories.\\no3\\nOpenAI introduced the successor model, o3, in December 2024. According to OpenAI, o3 is designed to handle tasks with more analytical thinking, problem-solving and complex reasoning and will improve o1\\'s capabilities and performance. The o3 model is in safety testing mode and is currently not available to the public.\\nOrca\\nOrca was developed by Microsoft and has 13 billion parameters, meaning it\\'s small enough to run on a laptop. It aims to improve on advancements made by other open source models by imitating the reasoning procedures achieved by LLMs. Orca achieves the same performance as GPT-4 with significantly fewer parameters and is on par with GPT-3.5 for many tasks. Orca is built on top of the 13 billion parameter version of Llama.\\nPalm\\nThe\\xa0Pathways Language Model\\xa0is a 540 billion parameter transformer-based model from Google powering its AI chatbot\\xa0Bard. It was trained across multiple\\xa0TPU\\xa04 Pods -- Google\\'s custom hardware for machine learning. Palm specializes in reasoning tasks such as coding, math, classification and question answering. Palm also excels at decomposing complex tasks into simpler subtasks.\\nPaLM gets its name from a Google research initiative to build Pathways, ultimately creating a single model that serves as a foundation for multiple use cases. There are\\xa0several fine-tuned versions\\xa0of Palm, including Med-Palm 2 for life sciences and medical information as well as Sec-Palm for cybersecurity deployments to speed up threat analysis.\\nPhi\\nPhi is a transformer-based language model from Microsoft. The Phi 3.5 models were first released in August 2024.\\nThe series includes Phi-3.5-mini-instruct (3.82 billion parameters), Phi-3.5-MoE-instruct (41.9 billion parameters), and Phi-3.5-vision-instruct (4.15 billion parameters), each designed for specific tasks ranging from basic reasoning to vision analysis. All three models support a 128k token context length.\\nReleased under a Microsoft-branded MIT License, they are available for developers to download, use, and modify without restrictions, including for commercial purposes.\\nQwen\\nQwen is large family of open models developed by Chinese internet giant Alibaba Cloud. The newest set of models are the Qwen2.5 suite, which support 29 different languages and currently scale up to 72 billion parameters. These models are suitable for a wide range of tasks, including code generation, structured data understanding, mathematical problem-solving as well as general language understanding and generation.\\nStableLM\\nStableLM is a series of open language models developed by Stability AI, the company behind image generator Stable Diffusion.\\nStableLM 2 debuted in January 2024 initially with a 1.6 billion parameter model. In April 2024 that was expanded to also include a 12 billion parameter model. StableLM 2 supports seven languages: English, Spanish, German, Italian, French, Portuguese, and Dutch. Stability AI positions these models as offering different options for various use cases, with the 1.6B model suitable for specific, narrow tasks and faster processing while the 12B model provides more capability but requires more computational resources.\\nTülu 3\\nAllen Institute for AI\\'s Tülu 3 is an open-source 405 billion-parameter LLM. The Tülu 3 405B model has post-training methods that combine supervised fine-tuning and reinforcement learning at a larger scale. Tülu 3 uses a \"reinforcement learning from verifiable rewards\" framework for fine-tuning tasks with verifiable outcomes -- such as solving mathematical problems and following instructions.\\nVicuna 33B\\nVicuna is another influential open source LLM derived from Llama. It was developed by LMSYS and was fine-tuned using data from sharegpt.com. It is smaller and less capable that GPT-4 according to several benchmarks, but does well for a model of its size. Vicuna has only 33 billion parameters, whereas GPT-4 has trillions.\\nLLM precursors\\nAlthough LLMs are a recent phenomenon, their precursors go back decades. Learn how recent precursor Seq2Seq and distant precursor ELIZA set the stage for modern LLMs.\\nSeq2Seq\\nSeq2Seq is a deep learning approach used for machine translation, image captioning and natural language processing. It was developed by Google and underlies some of their modern LLMs, including LaMDA. Seq2Seq also underlies AlexaTM 20B, Amazon\\'s large language model. It uses a mix of encoders and decoders.\\nEliza\\nEliza was an\\xa0early natural language processing program\\xa0created in 1966. It is one of the earliest examples of a language model. Eliza simulated conversation using pattern matching and substitution. Eliza, running a certain script, could parody the interaction between a patient and therapist by applying weights to certain keywords and responding to the user accordingly. The creator of Eliza, Joshua Weizenbaum, wrote a book on the limits of computation and artificial intelligence.\\nSean Michael Kerner is an IT consultant, technology enthusiast and tinkerer. He has pulled Token Ring, configured NetWare and been known to compile his own Linux kernel. He consults with industry and media organizations on technology issues.\\nBen Lutkevich is site editor for Informa TechTarget Software Quality. Previously, he wrote definitions and features for Whatis.com.\\nNext Steps\\nGenerative AI challenges that businesses should consider\\nGenerative AI ethics: Biggest concerns\\nGenerative AI landscape: Potential future trends\\nGenerative models: VAEs, GANs, diffusion, transformers, NeRFs\\nAI content generators to explore\\nRelated Resources\\n\\nScale your fundraising: Bonterra 1H 2025 product updates –Video\\nFive data quality trends to prepare for in the year ahead –Video\\nThe Digital Transformation And Innovation Landscape –Wipro\\nImprove customer satisfaction or cut costs? Who says you have to choose? –Video\\n\\nDig Deeper on Data analytics and AI\\n\\n ##### GPT-3.5 vs. GPT-4: Biggest differences to consider  By: Leah Zitter, Ph.D.\\n ##### What is GPT-3? Everything you need to know  By: Nick Barney\\n ##### What is a small language model (SLM)?  By: Sean Kerner\\n ##### GPT-4  By: Ben Lutkevich\\n\\nSponsored News\\n\\nThree Innovative AI Use Cases for Natural Language Processing –Dell Technologies\\nAutonomous coding: The future of the revenue cycle –Solventum\\nHybrid Work Drives New Criteria for VDI and DaaS –Dell Technologies\\n\\nRelated Content\\n\\nExploring GPT-3 architecture – Search Enterprise AI\\nWhat is GPT-3? Everything you need to know – Search Enterprise AI\\nMicrosoft exclusively licenses OpenAI\\'s GPT-3 ... – Search Enterprise AI\\n\\nLatest TechTarget resources\\n\\nNetworking\\nSecurity\\nCIO\\nHR Software\\nCustomer Experience\\n\\nSearch Networking\\n\\n\\nWhat are port numbers and how do they work?A port number is a way to identify a specific process to which an internet or other network message is to be forwarded when it ...\\n\\n\\nWhat is a router?A router is a physical or virtual appliance that passes information between two or more packet-switched computer networks.\\n\\n\\nWhat is east-west traffic?East-west traffic refers to the transfer of data packets that move from server to server within a network\\'s data center.\\n\\n\\nSearch Security\\n\\n\\nWhat is cyberstalking and how to prevent it?Cyberstalking is a crime in which someone harasses or stalks a victim using electronic or digital means, such as social media, ...\\n\\n\\nWhat is a watering hole attack?A watering hole attack is a security exploit in which the attacker seeks to compromise a specific group of end users by infecting...\\n\\n\\nWhat is multifactor authentication?Multifactor authentication (MFA) is an IT security technology that requires multiple sources of unique information from ...\\n\\n\\nSearch CIO\\n\\n\\nWhat is a think tank?A think tank is an organization that gathers a group of interdisciplinary scholars to perform research around particular policies...\\n\\n\\nWhat is emotional intelligence (EI)?Emotional intelligence (EI) is the area of cognitive ability that facilitates interpersonal behavior.\\n\\n\\nWhat are agreed-upon procedures (AUPs)?Agreed-upon procedures are a standard a company or client outlines in an engagement letter or other written agreement when it ...\\n\\n\\nSearch HRSoftware\\n\\n\\nWhat is gamification? How it works and how to use itGamification is a strategy that integrates entertaining and immersive gaming elements into nongame contexts to enhance engagement...\\n\\n\\nWhat is employee self-service (ESS)?Employee self-service (ESS) is a widely used human resources technology that enables employees to perform many job-related ...\\n\\n\\nWhat is DEI? Diversity, equity and inclusion explainedDiversity, equity and inclusion is a term used to describe policies and programs that promote the representation and ...\\n\\n\\nSearch Customer Experience\\n\\n\\nWhat is voice of the customer? A guide to VOC StrategyVoice of the customer (VOC) is the component of customer experience (CX) that focuses on customer needs, wants, expectations and ...\\n\\n\\nWhat is high-touch customer service?High-touch customer service is a category of contact center interaction that requires human interaction.\\n\\n\\nWhat is CRM (customer relationship management)?CRM (customer relationship management) is the combination of practices, strategies and technologies that companies use to manage ...\\n\\n\\nBrowse by Topic\\n\\n\\nBrowse Resources\\n\\n\\nAbout Us\\n\\nMeet The Editors\\nEditorial Ethics Policy\\nContact Us\\nAdvertisers\\nBusiness Partners\\nEvents\\nMedia Kit\\nCorporate Site\\nReprints\\n\\nAll Rights Reserved, Copyright 1999 - 2025, TechTarget  \\nPrivacy Policy\\nCookie Preferences\\nCookie Preferences\\nDo Not Sell or Share My Personal Information\\nClose', '## Title\\n\\nThe Best LLMs for Enhanced Language Processing in 2025 - ELEKS\\n\\n## Content\\n\\nPublished Time: 2025-01-10T09:42:00+00:00\\nThe Best LLMs for Enhanced Language Processing in 2025\\nSkip to main content\\n\\n\\n\\nServices\\n\\n\\nEngineering\\nEnd-to-end engineering services for seamless software delivery.\\n\\n Application development Bring your software vision to life with a tailored solution and deliver an industry-leading user experience.\\n PoC development Safely explore business-boosting concepts with robust testing and expert road mapping.\\n Product-oriented delivery Ensure timely and cost-effective product delivery while prioritizing your business objectives.\\n\\n Enterprise applications\\n\\nERP consulting\\nCRM consulting\\n\\n\\n\\n Application re‑engineering Transform your core legacy systems to elevate performance, agility, scalability, and UX.\\n\\n Cloud migration Boost agility, scalability and cost effectiveness by integrating your IT infrastructure with the cloud.\\n\\n\\n\\nAdvisory\\nStrategic guidance for top-notch products and services.\\n\\n Product and service design Validate niche ideas and create innovative products and services that scale as your business does.\\n Cyber security Proactively identify threats to your digital infrastructure to futureproof your IT ecosystem.\\n Technical feasibility study Explore new technologies and their potential for your business before making an investment.\\n Sustainability consulting Reach your net-zero goals and seize new, sustainable business growth opportunities.\\n Agile transformation Transform your organization to achieve agility, resilience, and sustainable business growth.\\n AI consulting Get strategic guidance on implementing AI solutions for scalable business growth.\\n\\n\\n\\nData & AI\\nCustom solutions to maximize the value of your data.\\n\\n Data science Boost your business performance and achieve optimization through tailored data-driven solutions.\\n\\n Artificial intelligence Transform your industry with AI-driven innovations and custom data-centric solutions.\\n\\nGenerative AI\\nMachine learning\\nConversational AI\\n\\n\\n\\n MLOps Achieve seamless integration and maximum ROI for your machine learning models.\\n\\n Data platforms Enable actionable insights and unmatched strategic impact with a custom data platform from ELEKS.\\n Data strategy Enable smart, insight-driven business decisions with our end-to-end data strategy consulting.\\n Business intelligence Maximize your data potential, make smarter decisions, and strategize for long-term success.\\n\\n Intelligent automation\\n\\nIntelligent document processing\\n\\n\\n\\n\\n\\nOptimisation\\nExpert help for flawless performance of your products and services.\\n\\n Software audit Assess your software products and processes to mitigate risks and minimize revenue loss.\\n Quality assurance Delver flawless products and seamless user experiences with our expert QA services.\\n Support Efficiently handle technical issues and system changes with our IT support services.\\n FinOps Get expert guidance to maximise your cloud infrastructure value, optimise costs, and boost ROI.\\n\\n\\n\\nExpertise\\nLatest technologies and innovative approaches to drive your business growth forward.\\n\\n DevOps Full-cycle DevOps solutions to optimise your SDLC for greater agility and cost-efficiency.\\n VR/AR/MR Utilize virtual reality technologies to deliver brand-defining, immersive user experiences.\\n Internet of Things Embrace the potential of IoT for better efficiency, smoother collaboration and powerful data insights.\\n Market research Evaluate your business landscape to capitalise on promising niches and get ahead of the competition.\\n Customer Experience Refine every customer interaction to enhance satisfaction and drive sustainable revenue growth.\\n Digital enterprise Introduce digital transformation to your enterprise to drive efficiency, productivity, and revenue.\\n UX consulting Launch successful products and deliver services that your customers will truly want to use.\\n Nearshore development Partner with a trusted nearshore software development company to deliver your software project.\\n\\n\\n\\n\\n\\nIndustries\\nInnovative solutions across industriesExpert software services tailored to meet the unique needs of every sector.\\n\\n\\n Fintech FintechEffectively manage risks, protect your assets against fraud and maximize the potential of your data. \\n Healthcare HealthcareLeverage new technologies to provide outstanding patient care focused on improving clinical results. \\n Energy EnergyElevate your power system’s productivity, safety and sustainability with energy management software. \\n Government GovernmentDigitise your public services to create a transparent, efficient, and data-centric government agency. \\n Insurance InsuranceAdopt data-driven insurance software solutions boosting efficiency, profitability, and security in your sector. \\n Retail RetailTransform customer journeys and engage shoppers in new ways, increasing sales and enhancing profitability. \\n Logistics LogisticsStreamline your supply chain, fleets, and warehousing for industry-leading logistics services. \\n Automotive AutomotiveIntroduce intelligent features and process optimizations to deliver a new level of driver experience. \\n Agriculture AgricultureAdopt data-driven innovations to balance supply and optimise production under increasing pressures. \\n Media & Entertainment Media & EntertainmentReach wider audiences with unparalleled digital experiences crafted for maximum engagement. \\n\\n\\n\\nClients\\n\\nOur clients We are proud of contributing to the success of the world’s leading brands. \\nCase studies See how ELEKS has helped its clients achieve their vision of digital innovation. \\n\\n20+\\nyears of partnership with clients\\n\\n\\n120+\\nactive client accounts\\n        *   1000+\\nend-to-end projects\\n\\n\\n\\n\\n\\n\\nAbout us\\n\\nAbout us Meet ELEKS, a trusted partner for software engineering and technology consulting services. \\nHow we work Learn how we help our clients address complex business problems with technology solutions. \\nAwards and partners The recognition our solutions receive from prestigious associations and award programs. \\n Leadership team\\n\\nCSR\\n\\nCareers\\nPress kit\\n\\n\\n\\n Our products\\n\\n\\n\\n\\nBlog\\n\\n\\nContact us\\nSearch request  Search \\nContact us\\n\\n\\nArticle\\nThe Best LLMs for Enhanced Language Processing in 2025\\nHome Article AI development\\nRelated services\\n\\nAI development\\nConversational AI solutions\\nGenerative AI software development\\n\\nLarge Language Models (LLMs) have emerged as advanced artificial intelligence systems that can process and generate text with logical communication.\\nAs a cornerstone of modern generative AI software development, LLMs often approach human-level proficiency across a variety of language-related tasks. In this article, we\\'ll overview top LLMs and their features, explore challenges and trends, and consider industry-specific applications of LLMs.\\nHow LLMs work\\nData collection\\nIt starts with collecting a wide range of text from global sources, including books, research papers, news, and websites. Depending on the industry, the model can also train on various types of data organisations own, such as financial reports, customer behaviour data, patient records, equipment data, and even weather data. The more diverse the data, the better the model can learn.\\nGenerally, LLMs have anywhere from 8 billion to 70 billion parameters and are trained on vast amounts of data. For example, Crawl, one of the largest datasets, includes web pages and information from the past decade, holding several petabytes of data.\\nTokenisation\\nAt this step, data is broken into tokens, words, or parts of words. In this way, the model processes and analyses the text.\\nPre-training or knowledge distillation\\nIn pre-training, the model learns by predicting the next token in a sequence and grasping language patterns, grammar, and word relationships. For example, given \"The sky is,\" it predicts \"blue.\" Using a transformer architecture, it processes tokens and applies self-attention to focus on the most important words in a sentence. This approach boosts the model\\'s language skills and lets intelligent automation handle tasks with less human input.\\nOn the other hand, Knowledge Distillation allows smaller models (like LLaMA or Mistral) to learn from larger and more complex models (like GPT-4). KD helps smaller models perform well with fewer resources. The smaller model is essentially \"taught\" by the larger one, which improves the smaller model\\'s efficiency and performance while reducing its computational cost.\\nFine-tuning\\nAfter pre-training, the model is fine-tuned for specific tasks like question answering or summarising text. This involves training the model on smaller, task-specific datasets. Fine-tuning helps the model specialise in particular tasks and improve its performance.\\nInference\\nThe model processes input, such as a question or prompt, and gives a relevant response. It understands language and context to provide accurate answers or generate text. Conversational AI systems, such as chatbots, use this process to interact meaningfully with users.\\nResponse generation\\nThe model creates text one token at a time, predicting each next token based on the input and its acquired knowledge. The output layer creates tokens and forms them into sentences. Methods like beam search are used to find the best and most coherent response.\\nFor more insights into how generative AI is shaping the future of software development, check out this article: Expert Insights on Generative AI: Evolution, Challenges, and Future Trends\\nTop LLM models\\n1. GPT\\nGPT Models (OpenAI): OpenAI created the GPT series, which includes some of the most widely known and used language models. The GPT o1 and GPT o3 models, developed by OpenAI, build on previous versions with improved learning from human feedback. The latest GPT o3 processes both text and images. It has over 170 billion parameters, making it incredibly powerful for a wide range of tasks.\\n2. Gemini\\nGemini is Google\\'s family of large language models that can process text, images, and other media. The Gemini family includes different versions: Ultra (the largest and most capable), Pro (mid-tier), and Nano (efficient for on-device processing). Gemini 2.0 Flash builds on the success of 1.5 Flash, offering faster performance and even outperforming 1.5 Pro in key benchmarks.\\nIn addition to handling multimodal inputs like images, video, and audio, 2.0 Flash supports new features like generating pictures mixed with text, steerable text-to-speech (TTS) multilingual audio and calling tools like Google Search, code execution, and third-party functions.\\n3. Claude\\nClaude is an LLM developed by Anthropic. It is built to focus on ethical and safe AI through constitutional AI principles. Claude 3.5 Sonnet is the latest iteration. It\\'s designed to offer safer, more reliable interactions, especially for enterprise applications, and is available through platforms like Claude.ai and its iOS app.\\n4. Command\\nCommand by Cohere blends real-time data with natural language generation to provide accurate, up-to-date responses. The Command R is built to scale, delivering fast, reliable results for complex tasks like customer support or content creation. Cohere\\'s open-source approach lets users easily customise the models to fit their needs without being tied to a specific vendor. Command easily integrates with existing systems, helping businesses quickly innovate and stay competitive.\\n5. Llama 3.3\\nLLaMA (Large Language Model Meta AI) is Meta\\'s series of open-source large language models. The latest version, LLaMA 3.1, was released in July 2024 and introduces an expanded context length of up to 128,000 tokens, multilingual support across eight languages, and improved reasoning and coding capabilities. LLaMA models range from 8 billion to 405 billion parameters. Meta emphasises accessibility and innovation, allowing developers to fine-tune these models for diverse applications while fostering collaboration in the AI community.\\n6. R1\\nR1 is a high-performance language model developed by DeepSeek. It is designed for real-time interactions, providing fast, accurate responses to complex queries. R1 is known for its ability to process and understand a wide range of topics with high precision, making it suitable for applications that require dynamic, real-time problem-solving.\\n7. Qwen Max\\nQwen Max is a large-scale language model developed by Alibaba\\'s Qwen team. It is part of the Qwen 2.5 series, which includes models ranging from 3B to 72B parameters. Qwen Max is designed for both text and image processing and excels in multimodal tasks. The model is part of the Qwen-VL-Max release, which outperforms previous open-source vision-language models in tasks involving text and images.\\nRole of LLM APIs in application development\\nLLM APIs act as a communication channel between applications and the LLM models. With the help of APIs, developers don\\'t need to understand the complexities of LLMs. Instead, developers interact with the API. They send text-based inputs and receive responses.\\nHow LLM API works\\n\\nData transmission: The user provides a text input like a question or command. The application formats this input and transmits it to the LLM API.\\nNatural language processing by the LLM: Upon receiving the input, the API forwards it to the LLM model. The model processes the language.\\nAPI response generation: LLM generates an appropriate response, from simple facts to creative content.\\nApplication integration: The response is returned to the app. It then integrates it into the user experience. This could mean showing the response on the screen, playing it as audio, or triggering actions in the app.\\n\\nKey considerations for choosing the right LLM API\\nBefore exploring the different language model providers, understand your project\\'s needs.\\n\\nWhat do you want the LLM to do? Think about the specific tasks it will handle.\\nWho will use it, and what do they need? Consider your audience and what they expect.\\nHow much will you use it? Estimate how often you\\'ll send requests to the API.\\nWhat\\'s your budget? Decide how much money you can spend on monthly or yearly LLM services.\\n\\nNarrow down your choices and focus on models that suit your needs. Then, you can compare the features and abilities of different LLMs to find the best fit.\\nThe factors influencing the selection of the right large language model (LLM) begin with a clear understanding of the domain and the specific task. Beyond that, considerations such as the intended usage, the organisation\\'s FinOps strategy, and the model\\'s positioning within competitive arenas—like the Chatbot Arena or Language Model Arena—play a critical role. Choosing the right model is about its capabilities and aligning it with business goals, operational requirements, and cost-efficiency strategies to ensure optimal performance and scalability.\\n\\nVolodymyr Getmanskyi\\nHead of Data Science at ELEKS\\nThe tables below list large language models, their API providers, and key metrics for evaluating them for different use cases.\\nQuality overview\\n| Model | API Providers | Arena Score | Latency (s) | Context Window |\\n| --- | --- | --- | --- | --- |\\n| o1-preview | OpenAI | 1334 | 23.57 | 128k |\\n| o1-mini | OpenAI | 1306 | 9.44 | 128k |\\n| GPT-4o-2024-08-06 | Microsoft Azure | 1265 | 0.83 | 128k |\\n| Claude 3.5 Sonnet (20241022) | AWS | 1283 | 1.01 | 200k |\\n| Claude 3 Opus | AWS | 1248 | 1.61 | 200k |\\n| Claude 3 Haiku | Anthropic | 1179 | 0.51 | 200k |\\n| Command R+ (04-2024) | Cohere | 1190 | 0.32 | 128k |\\n| Llama-3.1-Nemotron-70B-Instruct | Nebius | 1269 | 0.33 | 128k |\\n| Llama-3.3-70B-Instruct | Microsoft Azure | 1256 | 0.44 | 128k |\\n| Gemini-1.5-Flash-002 | Google (AI Studio) | 1271 | 0.35 | 1m |\\n| DeepSeek R1 | DeepSeek | 1357 | 25.47 | 64k |\\n| Qwen2.5-72B-Instruct | Nebius | 1282 | 0.62 | 131k |\\n| Qwen2.5-Max | Alibaba Cloud | 1183 | 3.00 | 32k |\\nCost and volumes overview\\n| Model | API Providers | Blended Price (USD/1m tokens) | Input Price (USD/1m tokens) | Output Price (USD/1m tokens) | Latency (s) |\\n| --- | --- | --- | --- | --- | --- |\\n| o1-preview | OpenAI | $26.25 | $15.00 | $60.00 | 23.57 |\\n| o1-mini | OpenAI | $5.25 | $3.00 | $12.00 | 9.44 |\\n| GPT-4o-2024-08-06 | Microsoft Azure | $4.38 | $2.50 | $10.00 | 0.83 |\\n| Claude 3.5 Sonnet (20241022) | AWS | $6.00 | $3.00 | $15.00 | 1.01 |\\n| Claude 3 Opus | AWS | $30.00 | $15.00 | $75.00 | 1.61 |\\n| Claude 3 Haiku | Anthropic | $0.50 | $0.25 | $1.25 | 0.51 |\\n| Command R+ (04-2024) | Cohere | $6.00 | $3.00 | $15.00 | 0.32 |\\n| Llama-3.1-Nemotron-70B-Instruct | Nebius | $0.20 | $0.13 | $0.40 | 0.33 |\\n| Llama-3.3-70B-Instruct | Microsoft Azure | $0.71 | $0.71 | $0.71 | 0.44 |\\n| Gemini-1.5-Flash-002 | Google (AI Studio) | $0.13 | $0.13 | $0.30 | 0.35 |\\n| DeepSeek R1 | DeepSeek | $0.96 | $0.55 | $2.19 | 25.47 |\\n| Qwen2.5-72B-Instruct | Nebius | $0.20 | $0.13 | $0.40 | 0.62 |\\n| Qwen2.5-Max | Alibaba Cloud | $20.00 | $10.00 | $30.00 | 3.00 |\\n\\nArena score is a performance metric used to evaluate and rank models based on their effectiveness in a competitive or benchmark setting.\\nContext window represents the number of tokens the model can handle in a single session.\\nBlended price is the average cost per million tokens.\\nInput price is the cost of processing one million tokens sent as input to the model.\\nOutput price is the cost of generating one million tokens as a response from the model.\\nLatency is the average time (in seconds) it takes for the model to process input and deliver output.\\n\\nIt\\'s important to note that the models and providers listed in the tables are just a selection, and many more options are available in the market. For a more extended comparison, check the LLM API Providers Leaderboard and Chatbot Arena LLM Leaderboard\\nWe understand that navigating these metrics can be complex, so you can contact our team for assistance in selecting the best model for your use case.\\nChallenges and limitations of LLM tools\\nModel bias and hallucinations\\nOne important issue with LLMs is their tendency to \"hallucinate.\" LLMs predict the next word in a sequence. This can make them sound believable, but they may generate false or nonsensical responses. This can be especially problematic in applications where accuracy is crucial. To avoid misinformation, users should verify LLMs\\' output with other sources.\\n\\nFor instance, our data science engineers have encountered cases where models sometimes confused financial data from different companies. Even with instructions to admit uncertainty or missing data, the models still gave wrong answers. It shows how hard it is to ensure models provide accurate results in complex situations.\\n\\nInput and output length limitations\\nLarge language models are limited by the number of tokens they can process in a single instance. It restricts both the length of the input and the output. This limitation can be a challenge for processing long documents or generating detailed responses.\\n\\nResearchers are working on optimising models to process longer text sequences. In the meantime, users can break up lengthy inputs into smaller ones.\\n\\nLimited multimodal capabilities\\nMost LLMs are focused on text and do not yet handle other forms of media effectively. Full integration across modalities is still developing.\\n\\nLarge language models are being updated to handle both text and other media, like images or audio. Models like GPT-4 and Google Gemini are already starting to process multiple types of data, with plans for more advanced media handling in the future.\\n\\nVulnerability to misuse and ethical risks\\nLLM tools are also vulnerable to misuse. There are concerns about generated code vulnerabilities, contradictory suggestions from models, and unethical usage, such as using AI to cheat on exams or gain instructions on illegal activities. These issues highlight the need for careful oversight and regulation to prevent harmful or unintended uses of AI technologies.\\nIndustry-specific applications of LLM tools\\nHealthcare\\nLLM-driven AI chatbot assistants in the healthcare software help facilitate patient-doctor communication. These chatbots are being created for different fields, from helping patients and doctors communicate to improving internal processes. AI chatbots boost patient engagement, offer quick 24/7 assessments, reduce administrative tasks, and improve planning, thus making the work of healthcare providers more efficient and patient-centric.\\nRetail\\nLLMs analyse consumer behaviour in retail software to improve marketing strategies and campaign precision. Building a chain of LLM-based agents that automates internal processes, from ordering and communication to hiring, significantly reduces operating costs.\\nFinance\\nLLMs act as financial advisors, tailoring investment recommendations and strategies based on customer preferences and historical trends. They also gather market data and expert opinions to generate actionable insights, helping financial institutions make informed investment decisions in the fintech solutions.\\nMedia and entertainment\\nIn the media and entertainment software, SOTA (State-of-the-Art) LLMs are used to create personalised advertising and dynamically adjust the appearance of websites, apps, and marketing materials such as tailored ads and content for specific audiences. It leads to higher click-through rates (CTR) and improved engagement metrics.\\nInsurance\\nPersonalised insurance software products involve creating an LLM-based recommender system that combines underwriting policies with recognised consumption patterns and customer needs. This system analyses the limitations and possibilities of available policies and tailors recommendations to individual customers.\\nAutomotive\\nLLM-based agents are used in the automotive software for automated contractors\\' information search, filtering, and ranking based on usefulness and predefined conditions. This helps businesses find suppliers more efficiently and improve their internal processes. The automation allows for smoother negotiations and quicker RFQ preparation, ultimately leading to higher efficiency in operations.\\nTesting an LLM for healthcare: ELEKS case study\\nAt ELEKS, we have developed a generative AI-powered solution for medical document summarisation. This solution aims to organise and manage large volumes of unstructured healthcare data.\\nOur team began by researching and selecting the task\\'s best large language models (LLMs). We compared general-purpose models like GPT-3.5 and GPT-4 with specialised medical LLMs such as DHEIVER and MedLlama2.\\nWe strictly adhered to HIPAA and GDPR regulations. We also implemented Optical Character Recognition (OCR) to convert unstructured medical documents into searchable text and a classification module to identify document types for targeted summarisation.\\nOur solution is built on a flexible tech stack. It uses Microsoft Azure and .NET to manage workflows and scalability. We refined the tool based on testing and feedback. We switched to GPT-4o to handle larger data volumes. Future upgrades include integrating the solution with electronic medical records (EMR) systems.\\nTo learn more about our experience developing this innovative solution, read our full article: Generative AI in Healthcare: Solving Medical Staff Performance Issue\\nFuture of LLMs\\nGPT-4 and Google\\'s Gemini models are among the first LMMs to be widely deployed. Their full capabilities are still being rolled out.\\nHowever, in the near future, we will see more large language models (LLMs), especially from tech giants like Apple, Amazon, IBM, Intel, and NVIDIA. These models may be less known than some popular ones. Large companies will likely use them for internal tasks and customer support.\\nWe may also see more efficient LLMs for smartphones and other lightweight devices. Google has already started this trend with Gemini Nano, which operates some features on the Google Pixel Pro 8. Similarly, Apple introduced Apple Intelligence.\\nAnother trend is the rise of multimodal models that combine text generation with other media, including images and audio. These models will allow users to ask a chatbot about an image or receive an audio response.\\nSummary and final thoughts\\nLarge Language Models (LLMs) are at the forefront of artificial intelligence. These models are changing how businesses and individuals interact with a language.\\nLLM APIs help organisations stay ahead in today\\'s competitive landscape, improve user experiences, and automate routine tasks.\\nThe future of LLMs looks bright as research continues to overcome their limitations. As we improve knowledge cutoffs, hallucinations, and multimodal skills, LLMs will evolve and help organisations be more productive and creative.\\n\\nLooking forward to applying LLM solution in your business?\\nContact an expert\\nAI development\\nPartner with ELEKS to implement AI-powered strategies that drive breakthrough performance.\\nView service\\nData science\\nDeep-dive into your data and boost business performance by understanding what your users really want.\\nView expertise\\nSkip the section\\nFAQs\\nIs ChatGPT is LLM?\\nYes, ChatGPT is an AI-powered large language model. It uses deep learning and neural networks to let you have human-like conversations with a chatbot.\\nIs LLM free?\\nYes, there are free options available! While many advanced large language models require payment, there are open-source models trained on extensive training data that can be used for free.\\nWhat are LLM apps?\\nLLM apps are applications that use large language models (LLMs) and AI models to perform various tasks, including language translation, content generation, and other language processing tasks. These apps are often built on the latest breakthroughs in AI research.\\nWhat are LLM model tools?\\nLLM model tools are software applications powered by advanced artificial intelligence models. These tools can understand and generate human-like text, as well as perform language processing tasks, and are the result of ongoing AI research.\\nIs Bert an LLM?\\nYes, BERT was one of the first modern LLMs. It uses neural networks and deep learning, making it widely used and very successful.\\nWhat are the three features of a smart grid?\\nLLMs are a type of generative AI that focuses on creating text. Generative AI, however, can produce many types of outputs, including text, images, audio, and code.\\nWhat is conversational AI?\\nConversational AI is a technology that allows computers to understand and respond to human language in real-time, often through chatbots or voice assistants, leveraging deep learning and extensive training data.\\nHow do AI LLM models and machine learning work?\\nAI LLM models and machine learning use deep learning and neural networks to process language and perform language processing tasks, enabling accurate and natural conversations.\\xa0\\n Olha Zhydik Content Marketing Manager\\nPublished:\\nJanuary 10, 2025\\nUpdated:\\nFebruary 17, 2025\\nAI artificial intelligence data science machine learning\\n\\n\\n\\n\\n\\nSkip the section\\nRelated Insights\\n Article Understanding Agentic AI: Benefits, Applications, and Future Trends View article Article Essential Guide to LLMOps: Key Insights and Implementation Strategies View article Article Strategic Technology in 2025: An Expert Assessment of Market Predictions View article Case studies Enhancing Customer Support Efficiency with AI-Powered Knowledge Management View article Article Supervised vs Unsupervised Learning: Differences, Applications, and Market Trends View article Article Nuclear Power Plants for AI Data Centres: a Solution to Growing Energy Challenge View article Article Enhancing Patient Experience in Healthcare: The Role of Experience Platforms View article Article Edge Computing for Industry 5.0: Enabling Next-Generation Industrial Intelligence View article\\nDiscover more insights\\nTalk to experts\\nSkip the section\\nContact Us\\n\\n\\nFull name*\\n\\n\\nWe need your name to know how to address you\\n\\n\\nEmail*\\n\\n\\nWe need your email to respond to your request\\n\\n\\nPhone number*\\n\\n\\nWe need your phone number to reach you with response to your request\\n\\n\\nCountry*\\n\\n\\nWe need your country of business to know from what office to contact you\\n\\n\\nCompany*\\n\\n\\nWe need your company name to know your background and how we can use our experience to help you\\n\\n\\nMessage*\\n\\n\\nAttach file\\nAccepted file types: jpg, gif, png, pdf, doc, docx, xls, xlsx, ppt, pptx, Max. file size: 10 MB.\\n\\n\\nAdd an attachment\\n(jpg, gif, png, pdf, doc, docx, xls, xlsx, ppt, pptx, PNG)\\n\\n\\n\\n[ ]  I want to receive news and updates once in a while\\n\\n\\n\\nWe will add your info to our CRM for contacting you regarding your request. For more info please consult our privacy policy\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPhone\\nThis field is for validation purposes and should be left unchanged.\\n\\n\\nΔ\\nWhat our customers say\\nThe breadth of knowledge and understanding that ELEKS has within its walls allows us to leverage that expertise to make superior deliverables for our customers. When you work with ELEKS, you are working with the top 1% of the aptitude and engineering excellence of the whole country.\\n\\nSam Fleming\\nPresident, Fleming-AOD\\nRight from the start, we really liked ELEKS’ commitment and engagement. They came to us with their best people to try to understand our context, our business idea, and developed the first prototype with us. They were very professional and very customer oriented. I think, without ELEKS it probably would not have been possible to have such a successful product in such a short period of time.\\n\\nCaroline Aumeran\\nHead of Product Development, appygas\\nELEKS has been involved in the development of a number of our consumer-facing websites and mobile applications that allow our customers to easily track their shipments, get the information they need as well as stay in touch with us. We’ve appreciated the level of ELEKS’ expertise, responsiveness and attention to details.\\n\\nSamer Awajan\\nCTO, Aramex\\n\\nAddress:\\nViru väljak 2, Tallinn, Harju maakond, 10111\\nEleks, Inc.\\nCAGE/NCAGE: 7W6F0\\nSAM Unique Entity ID: NQ9PRQMMSJG4\\n\\n\\nServices\\n\\n\\nEngineering\\n\\nPoC development\\nApplication development\\nProduct-oriented delivery\\nEnterprise applications\\nApplication re‑engineering\\nCloud migration\\n\\n\\n\\nData & AI\\n\\nData science\\nData strategy\\nArtificial intelligence\\nGenerative AI\\nMachine learning\\nConversational AI\\nIntelligent automation\\nMLOps\\nBusiness intelligence\\nData platforms\\n\\n\\n\\nAdvisory\\n\\nProduct and service design\\nCyber security\\nTechnical feasibility study\\nSustainability consulting\\nAgile transformation\\n\\n\\n\\nOptimisation\\n\\nFinOps\\nSoftware audit\\nQuality assurance\\nSupport\\n\\n\\n\\n\\n\\nExpertise\\n\\nDevOps\\nVR/AR/MR\\nInternet of Things\\nMarket research\\nCustomer experience\\nDigital enterprise\\nNearshore development\\nUX consulting\\nSoftware development\\n\\n\\n\\nIndustries\\n\\nFintech\\nHealthcare\\nEnergy\\nGovernment\\nInsurance\\nRetail\\nLogistics\\nAutomotive\\nAgriculture\\nMedia & Entertainment\\n\\n\\n\\nCompany\\n\\nAbout us\\nServices\\nHow we work\\nAwards and partners\\nOur clients\\nCase studies\\nBlog\\nCareers\\nContact us\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTerms of Use\\n\\nPrivacy policy\\nSite Map\\n\\n© 1991-2025 ELEKS, All rights reserved', '## Title\\n\\nBest LLMs!? (Focus: Best & 7B-32B) 02/21/2025 - Reddit\\n\\n## Content\\n\\nReddit - Dive into anything\\nSkip to main content\\nOpen menu Open navigation  Go to Reddit Home\\nr/LocalLLaMA A chip A close button\\nGet App Get the Reddit app Log In Log in to Reddit\\nExpand user menu Open settings menu\\n Go to LocalLLaMA\\nr/LocalLLaMA\\nr/LocalLLaMA\\nSubreddit to discuss about Llama, the large language model created by Meta AI.\\n\\nMembers Online\\n•\\nDeadlyHydra8630\\nBest LLMs!? (Focus: Best & 7B-32B) 02/21/2025\\nResources\\nHey everyone!\\nI am fairly new to this space and this is my first post here so go easy on me 😅\\nFor those who are also new!\\nWhat does this 7B, 14B, 32B parameters even mean?\\n  - It represents the number of trainable weights in the model, which determine how much data it can learn and process.\\n  - Larger models can capture more complex patterns but require more compute, memory, and data, while smaller models can be faster and more efficient.\\nWhat do I need to run Local Models?\\n  - Ideally you\\'d want the most VRAM GPU possible allowing you to run bigger models\\n  - Though if you have a laptop with a NPU that\\'s also great!\\n  - If you do not have a GPU focus on trying to use smaller models 7B and lower!\\n  - (Reference the Chart below)\\nHow do I run a Local Model?\\n  - Theres various guides online\\n  - I personally like using LMStudio it has a nice interface\\n  - I also use Ollama\\nQuick Guide!\\nIf this is too confusing, just get LM Studio; it will find a good fit for your hardware!\\nDisclaimer: This chart could have issues, please correct me! Take it with a grain of salt\\nYou can run models as big as you want on whatever device you want; I\\'m not here to push some \"corporate upsell.\"\\nNote: For Android, Smolchat and Pocketpal are great apps to download models from Huggingface\\n| Device Type | VRAM/RAM | Recommended Bit Precision | Max LLM Parameters (Approx.) | Notes |\\n| --- | --- | --- | --- | --- |\\n| Smartphones |  |  |  |  |\\n| Low-end phones | 4 GB RAM | 2 bit to 4-bit | ~1-2 billion | For basic tasks. |\\n| Mid-range phones | 6-8 GB RAM | 2-bit to 8-bit | ~2-4 billion | Good balance of performance and model size. |\\n| High-end phones | 12 GB RAM | 2-bit to 8-bit | ~6 billion | Can handle larger models. |\\n| x86 Laptops |  |  |  |  |\\n| Integrated GPU (e.g., Intel Iris) | 8 GB RAM | 2-bit to 8-bit | ~4 billion | Suitable for smaller to medium-sized models. |\\n| Gaming Laptops (e.g., RTX 3050) | 4-6 GB VRAM + RAM | 4-bit to 8-bit | ~4-14 billion | Seems crazy ik but we aim for model size that runs smoothly and responsively |\\n| High-end Laptops (e.g., RTX 3060) | 8-12 GB VRAM | 4-bit to 8-bit | ~4-14 billion | Can handle larger models, especially with 16-bit for higher quality. |\\n| ARM Devices |  |  |  |  |\\n| Raspberry Pi 4 | 4-8 GB RAM | 4-bit | ~2-4 billion | Best for experimentation and smaller models due to memory constraints. |\\n| Apple M1/M2 (Unified Memory) | 8-24 GB RAM | 4-bit to 8-bit | ~4-12 billion | Unified memory allows for larger models. |\\n| GPU Computers |  |  |  |  |\\n| Mid-range GPU (e.g., RTX 4070) | 12 GB VRAM | 4-bit to 8-bit | ~7-32 billion | Good for general LLM tasks and development. |\\n| High-end GPU (e.g., RTX 3090) | 24 GB VRAM | 4-bit to 16-bit | ~14-32 billion | Big boi territory! |\\n| Server GPU (e.g., A100) | 40-80 GB VRAM | 16-bit to 32-bit | ~20-40 billion | For the largest models and research. |\\nIf this is too confusing, just get LM Studio; it will find a good fit for your hardware!\\nThe point of this post is to essentially find and keep updating this post with the best new models most people can actually use.\\nWhile sure the 70B, 405B, 671B and Closed sources models are incredible, some of us don\\'t have the facilities for those huge models and don\\'t want to give away our data 🙃\\nI will put up what I believe are the best models for each of these categories CURRENTLY.\\n(Please, please, please, those who are much much more knowledgeable, let me know what models I should put if I am missing any great models or categories I should include!)\\nDisclaimer: I cannot find RRD2.5 for the life of me on HuggingFace.\\nI will have benchmarks, so those are more definitive. some other stuff will be subjective I will also have links to the repo (I\\'m also including links; I am no evil man but don\\'t trust strangers on the world wide web)\\nFormat: {Parameter}: {Model} - {Score}\\n------------------------------------------------------------------------------------------\\nMMLU-Pro (language comprehension and reasoning across diverse domains):\\nBest: DeepSeek-R1 - 0.84\\n32B: QwQ-32B-Preview - 0.7097\\n14B: Phi-4 - 0.704\\n7B: Qwen2.5-7B-Instruct - 0.4724\\n------------------------------------------------------------------------------------------\\nMath:\\nBest: Gemini-2.0-Flash-exp - 0.8638\\n32B: Qwen2.5-32B - 0.8053\\n14B: Qwen2.5-14B - 0.6788\\n7B: Qwen2-7B-Instruct - 0.5803\\nNote: DeepSeek\\'s Distilled variations are also great if not better!\\n------------------------------------------------------------------------------------------\\nCoding (conceptual, debugging, implementation, optimization):\\nBest: Claude 3.5 Sonnet, OpenAI O1 - 0.981 (148/148)\\n32B: Qwen2.5-32B Coder - 0.817\\n24B: Mistral Small 3 - 0.692\\n14B: Qwen2.5-Coder-14B-Instruct - 0.6707\\n8B: Llama3.1-8B Instruct - 0.385\\nHM:\\n32B: DeepSeek-R1-Distill - (148/148)\\n9B: CodeGeeX4-All - (146/148)\\n------------------------------------------------------------------------------------------\\nCreative Writing:\\nLM Arena Creative Writing:\\nBest: Grok-3 - 1422, OpenAI 4o - 1420\\n9B: Gemma-2-9B-it-SimPO - 1244\\n24B: Mistral-Small-24B-Instruct-2501 - 1199\\n32B: Qwen2.5-Coder-32B-Instruct - 1178\\nEQ Bench (Emotional Intelligence Benchmarks for LLMs):\\nBest: DeepSeek-R1 - 87.11\\n9B: gemma-2-Ifable-9B - 84.59\\n------------------------------------------------------------------------------------------\\nLonger Query (>\\\\= 500 tokens)\\nBest: Grok-3 - 1425, Gemini-2.0-Pro/Flash-Thinking-Exp - 1399/1395\\n24B: Mistral-Small-24B-Instruct-2501 - 1264\\n32B: Qwen2.5-Coder-32B-Instruct - 1261\\n9B: Gemma-2-9B-it-SimPO - 1239\\n14B: Phi-4 - 1233\\n------------------------------------------------------------------------------------------\\nHeathcare/Medical (USMLE, AIIMS & NEET PG, College/Profession level quesions):\\n(8B) Best Avg.: ProbeMedicalYonseiMAILab/medllama3-v20 - 90.01\\n(8B) Best USMLE, AIIMS & NEET PG: ProbeMedicalYonseiMAILab/medllama3-v20 - 81.07\\n------------------------------------------------------------------------------------------\\nBusiness*\\nBest: Claude-3.5-Sonnet - 0.8137\\n32B: Qwen2.5-32B - 0.7567\\n14B: Qwen2.5-14B - 0.7085\\n9B: Gemma-2-9B-it - 0.5539\\n7B: Qwen2-7B-Instruct - 0.5412\\n------------------------------------------------------------------------------------------\\nEconomics*\\nBest: Claude-3.5-Sonnet - 0.859\\n32B: Qwen2.5-32B - 0.7725\\n14B: Qwen2.5-14B - 0.7310\\n9B: Gemma-2-9B-it - 0.6552\\nNote*: Both of these are based on the benchmarked scores; some online LLMs aren\\'t tested, particularly DeepSeek-R1 and OpenAI o1-mini. So if you plan to use online LLMs you can choose to Claude-3.5-Sonnet or DeepSeek-R1 (which scores better overall)\\n------------------------------------------------------------------------------------------\\nSources:\\nhttps://huggingface.co/spaces/TIGER-Lab/MMLU-Pro\\nhttps://huggingface.co/spaces/finosfoundation/Open-Financial-LLM-Leaderboard\\nhttps://huggingface.co/spaces/openlifescienceai/open_medical_llm_leaderboard\\nhttps://lmarena.ai/?leaderboard\\nhttps://paperswithcode.com/sota/math-word-problem-solving-on-math\\nhttps://paperswithcode.com/sota/code-generation-on-humaneval\\nhttps://eqbench.com/creative_writing.html\\nRead more\\nNew to Reddit?Create your account and connect with a world of communities.\\nContinue with Email\\nContinue With Phone Number\\nBy continuing, you agree to our User Agreement and acknowledge that you understand the Privacy Policy.\\nTop 1% Rank by size\\nPublic\\nAnyone can view, post, and comment to this community\\nReddit Rules Privacy Policy User Agreement Reddit, Inc. © 2025. All rights reserved.\\nExpand Navigation Collapse Navigation\\n\\n\\n\\xa0\\n\\n\\n\\xa0\\n\\n\\n\\n\\n\\nTOPICS\\n\\n\\nInternet Culture (Viral)\\n\\n\\nAmazing Animals & Pets Cringe & Facepalm Funny Interesting Memes Oddly Satisfying Reddit Meta Wholesome & Heartwarming\\n\\n\\nGames\\n\\n\\nAction Games Adventure Games Esports Gaming Consoles & Gear Gaming News & Discussion Mobile Games Other Games Role-Playing Games Simulation Games Sports & Racing Games Strategy Games*   Tabletop Games\\n\\n\\nQ&As\\n\\n\\nQ&As*   Stories & Confessions\\n\\n\\nTechnology\\n\\n\\n3D Printing Artificial Intelligence & Machine Learning Computers & Hardware Consumer Electronics DIY Electronics Programming Software & Apps Streaming Services Tech News & Discussion*   Virtual & Augmented Reality\\n\\n\\nPop Culture\\n\\n\\nCelebrities Creators & Influencers Generations & Nostalgia Podcasts Streamers*   Tarot & Astrology\\n\\n\\nMovies & TV\\n\\n\\nAction Movies & Series Animated Movies & Series Comedy Movies & Series Crime, Mystery, & Thriller Movies & Series Documentary Movies & Series Drama Movies & Series Fantasy Movies & Series Horror Movies & Series Movie News & Discussion Reality TV Romance Movies & Series Sci-Fi Movies & Series Superhero Movies & Series*   TV News & Discussion\\n\\n\\n\\n\\n\\nRESOURCES\\n\\n\\nAbout Reddit Advertise Reddit Pro BETA Help Blog Careers Press\\n\\n\\n\\n\\nCommunities Best of Reddit Topics\\n\\nTradeStation • Official • Promoted\\nTrade CME Group futures products with competitive margin rates.\\nSign Up\\ntradestation.com\\n Use-Paragon • Promoted\\nUsers will never stop asking for more integrations, but integrations costs months of engineering (API research, building auth, error handling etc.). 29% of B2B SaaS engineering teams use an embedded iPaaS - to save 70% of the engineering, but should you? Read the detailed build vs. buy comparison.\\nLearn More\\nuseparagon.com\\n', '## Title\\n\\nThe Top 10 Small and Large Language Models Kicking Off 2025\\n\\n## Content\\n\\nThe Top 10 Small and Large Language Models Kicking Off 2025 | by ODSC - Open Data Science | Jan, 2025 | Medium\\nOpen in app\\nSign up\\nSign in\\n\\nWrite\\n\\nSign up\\nSign in\\n\\nThe Top 10 Small and Large Language Models Kicking Off 2025\\n\\nODSC - Open Data Science\\n·Follow\\n4 min read\\n·\\nJan 23, 2025\\n\\n--\\n1\\n\\nListen\\nShare\\n\\nArtificial intelligence has seen rapid advancements in recent years, with language models at the forefront. Large language models and small language models are revolutionizing how we process and generate text, contributing to various domains, from research to customer service. But why are these models so valuable and what are the top LLMs and SLMs that I should be using?\\nLarge Language Models (LLMs)\\nOpenAI o3: Enhancing Reasoning and Usability\\nOpenAI’s o3 stands out for its advanced reasoning capabilities, making it a game-changer for tasks requiring critical thinking and analysis. It extends beyond mere text generation, offering potential breakthroughs in areas like research synthesis and logical problem-solving. For data scientists, it means more nuanced, context-aware assistance in their workflows.\\nGoogle Gemini 2.0: Multimodal Mastery\\nGoogle’s Gemini 2.0 pushes boundaries with its ability to process text, images, audio, and video. This multimodal functionality opens doors to seamless integration across various media, streamlining workflows in creative industries and enhancing accessibility. For example, combining visual data with textual insights can revolutionize how we analyze complex datasets.\\nMeta’s Llama 3.1: Open-Source Scalability\\nMeta’s Llama 3.1, with up to 405 billion parameters, combines scalability and accessibility. Its open-source nature fosters community-driven innovation, allowing organizations to tailor the model for specific tasks, such as fine-tuning customer interactions or improving recommendation systems.\\nAnthropic’s Claude 3: Safety and Reliability\\nAnthropic’s Claude 3, available in configurations like Opus and Haiku, emphasizes ethical AI use. Designed for safety and reliability, it minimizes risks like biased or harmful outputs, crucial for sensitive applications like healthcare or legal advising.\\nCohere’s Command R+: Retrieval-Optimized Excellence\\nCohere’s Command R+ shines with retrieval-augmented generation (RAG), enhancing its ability to access external knowledge sources. For professionals, this means real-time, context-aware insights that bridge the gap between AI predictions and actionable information.\\nSmall Language Models (SLMs)\\nOpenAI GPT-4o Mini: Cost-Effective Precision\\nThe GPT-4o Mini offers a cost-efficient alternative without compromising on quality. Its reduced computational requirements make it ideal for startups or teams looking to deploy AI tools on a budget, democratizing access to cutting-edge technology.\\nMicrosoft’s Phi-4: Focused Accessibility\\nPhi-4 demonstrates Microsoft’s commitment to accessible AI, excelling in text generation and mathematical problem-solving. It’s an open-source marvel, enabling broader community engagement and use in educational environments.\\nGoogle’s Gemini Nano: Resource-Efficient Power\\nGemini Nano balances high performance with low resource demands, making it perfect for on-device tasks. This compact model is a boon for mobile applications, ensuring efficient processing without compromising user experience.\\nMistral 7B: Versatility in Compact Design\\nMistral 7B offers impressive versatility with its 7 billion parameters, delivering reliable results for various natural language processing tasks. Its adaptability makes it an asset for businesses needing targeted AI capabilities.\\nGemma 2: Lightweight and Robust\\nGoogle’s Gemma 2 series, with sizes ranging from 2B to 7B parameters, emphasizes lightweight models without sacrificing depth. Trained on extensive datasets, it serves diverse applications, from summarization tools to language translation.\\nWhy These Models Matter\\nEnhanced Productivity: Both LLMs and SLMs streamline repetitive tasks, freeing professionals to focus on strategic, high-impact activities. This efficiency is invaluable across industries, from healthcare to marketing.\\nAccessibility: The scalability and modularity of these models ensure that even resource-constrained teams can benefit from AI.\\nInnovation Catalyst: Open-source and multimodal models encourage experimentation, leading to tailored solutions and industry-specific applications.\\nSafety and Reliability: The focus on ethical AI, particularly with models like Claude 3, ensures that technological advancements align with societal values.\\nFuture-Proofing: The ability to handle diverse tasks, from anomaly detection to creative content generation, prepares organizations for the rapidly evolving digital landscape.\\nConclusion on the Top LLMs and SLMs in 2025\\nThe advancements in LLMs and SLMs reflect a dynamic AI landscape filled with promise and potential. By leveraging these models, data scientists and professionals can redefine productivity, innovation, and safety in their work. As the technology evolves, staying informed and adaptable will ensure we harness its full potential. And if you’re ready to harness their full potential, then attending ODSC East is a must this May 13th-15th.\\nAt ODSC East, you deepen your understanding of these models with hands-on workshops, talks, and expert guidance by the pioneers leading the way in AI. Head to ODSC East and explore training opportunities so you can stay ahead of the curve in the ever-changing world of AI!\\n\\nSign up to discover human stories that deepen your understanding of the world.\\nFree\\nDistraction-free reading. No ads.\\nOrganize your knowledge with lists and highlights.\\nTell your story. Find your audience.\\nSign up for free\\nMembership\\nRead member-only stories\\nSupport writers you read most\\nEarn money for your writing\\nListen to audio narrations\\nRead offline with the Medium app\\nTry for $5/month\\nData Science\\nArtificial Intelligence\\nLlm\\nLanguage Model\\nOpen Source\\n\\n--\\n\\n--\\n1\\n\\n\\n\\nFollow\\nWritten by ODSC - Open Data Science -----------------------------------\\n140K Followers\\n·135 Following\\nOur passion is bringing thousands of the best and brightest data scientists together under one roof for an incredible learning and networking experience.\\nFollow\\nResponses (1)\\n\\nWhat are your thoughts?\\nAlso publish to my profile\\nRespond\\nRespond\\nSee all responses\\nHelp\\nStatus\\nAbout\\nCareers\\nPress\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams', '## Title\\n\\nTop 10 open source LLMs for 2025 - Instaclustr\\n\\n## Content\\n\\nTop 10 open source LLMs for 2025\\nSearch\\nSearch   Search\\n\\nContact Us\\nSupport\\nSign In\\n\\n\\n\\nPlatform +\\nLeft Column\\nPlatformIntelligent, open source application data infrastructure\\nExplore our platform\\n\\n\\nSecurity and trustEnterprise-grade security\\nLearn more\\n\\n\\nHostingData infrastructure management in the cloud and on-prem\\nLearn more\\n\\n\\n\\n\\nRight Column\\nStore\\nPostgreSQL®\\nApache Cassandra®\\nValkey™\\n\\n\\nStream\\nApache Kafka®\\nKafka® Connect\\n\\n\\nOrchestrate\\nCadence\\nApache ZooKeeper™\\n\\n\\nAnalyze\\nClickHouse®\\n\\n\\nSearch\\nOpenSearch\\n\\n\\n\\n\\n\\n\\nPricing\\nServices +\\nSupport\\nConsulting\\nTraining\\n\\n\\nAbout +\\nAbout us\\nCustomers\\nOur commitment to open source\\n\\n\\nResources +\\nGetting started\\nSign up\\nDocumentation\\nQuick start videos\\nIntegrations\\nSupport portal\\n\\n\\nDiscover\\nBlog\\nEvents\\nContent library\\nGlossary\\n\\n\\nEducation hub\\nStream +\\nApache Kafka®\\n\\n\\nSearch +\\nOpenSearch\\n\\n\\nStore +\\nPostgreSQL®\\nApache Cassandra®\\n\\n\\nData infrastructure +\\nVector search\\nData streaming\\n\\n\\nAnalyze +\\nApache Spark™\\n\\n\\n\\n\\n\\n\\nFree trial\\n\\nFree trial Sign in\\n\\nPlatform +\\nLeft Column\\nPlatformIntelligent, open source application data infrastructure\\nExplore our platform\\n\\n\\nSecurity and trustEnterprise-grade security\\nLearn more\\n\\n\\nHostingData infrastructure management in the cloud and on-prem\\nLearn more\\n\\n\\n\\n\\nRight Column\\nStore\\nPostgreSQL®\\nApache Cassandra®\\nValkey™\\n\\n\\nStream\\nApache Kafka®\\nKafka® Connect\\n\\n\\nOrchestrate\\nCadence\\nApache ZooKeeper™\\n\\n\\nAnalyze\\nClickHouse®\\n\\n\\nSearch\\nOpenSearch\\n\\n\\n\\n\\n\\n\\nPricing\\nServices +\\nSupport\\nConsulting\\nTraining\\n\\n\\nAbout +\\nAbout us\\nCustomers\\nOur commitment to open source\\n\\n\\nResources +\\nGetting started\\nSign up\\nDocumentation\\nQuick start videos\\nIntegrations\\nSupport portal\\n\\n\\nDiscover\\nBlog\\nEvents\\nContent library\\nGlossary\\n\\n\\nEducation hub\\nStream +\\nApache Kafka®\\n\\n\\nSearch +\\nOpenSearch\\n\\n\\nStore +\\nPostgreSQL®\\nApache Cassandra®\\n\\n\\nData infrastructure +\\nVector search\\nData streaming\\n\\n\\nAnalyze +\\nApache Spark™\\n\\n\\n\\n\\n\\n\\nFree trial\\n\\nSearch\\nSearch   Search\\n\\nSupport\\nContact us\\n\\nTop 10 open source LLMs for 2025\\nLarge Language Models (LLMs) are machine learning models that can understand and generate human language based on large-scale datasets.\\nTalk to a consultant\\n\\nWhat are open source LLMs?\\nOpen source vs closed source LLMs\\nBenefits of using open source LLMs\\nTips from the expert\\nTop open source LLMs in 2024\\nNetApp Instaclustr: Empowering open source large language models\\n\\nWhat are open source LLMs?\\nLarge Language Models (LLMs) are machine learning models that can understand and generate human language based on large-scale datasets. Unlike proprietary models developed by companies like OpenAI and Google, open source LLMs are licensed to be freely used, modified, and distributed by anyone. They offer transparency and flexibility, which can be particularly useful for research, development, and customization in various applications.\\nResearchers and developers can access the underlying code, training mechanisms, and datasets, enabling them to deeply understand and improve these models. This openness fosters a community-driven approach to innovation, which can lead to rapid advancements not possible with closed source models.\\nThis is part of a series of articles about open source AI.\\nOpen source vs closed source LLMs\\nOpen source LLMs are fully accessible for anyone to use, modify, and distribute (although some models require prior approval to use, and some might restrict commercial use of the model). This transparency allows for extensive customization and examination, enabling users to adapt the models to their needs. Open source models offer more freedom, often requiring less financial investment and enabling users to mitigate vendor lock-in risks.\\nClosed source LLMs are proprietary, with restricted access to the code, training methods, and datasets, limiting user control and customization. Closed source LLMs often provide improved performance and capabilities due to significant resources invested by their creators. However, this comes at a cost—both literally and figuratively. Commercial models are typically priced per token, which can be significant for large-scale usage, and users are dependent on the vendor for updates and support.\\nBenefits of using open source LLMs\\nOpen source large language models offer several advantages:\\n\\nEnhanced data security and privacy: Users have full control over the data processed by these models, eliminating concerns of third-party access or data mishandling. Organizations can deploy open source LLMs on their private infrastructure, ensuring sensitive information remains in-house and complies with data protection requirements.\\nCost savings and reduced vendor dependency: Since the code and models are freely available, organizations save on pay-per-use and licensing fees and can allocate resources toward customizing and optimizing the models to meet their needs. They can also avoid vendor lock-in scenarios where they are tied to a specific provider for updates, support, and future developments.\\nCode transparency: Users have full visibility into the model’s architecture, training data, and algorithms. This transparency fosters trust and enables detailed audits to ensure the model’s integrity and performance. Developers can modify the code to fix bugs or improve features.\\nLanguage model customization: Organizations can tweak the models to better suit their requirements, from adjusting the training processes to incorporating domain-specific knowledge. With closed source models, customization is often limited and might require special permissions and additional costs.\\n\\nTips from the expert\\n\\nSharath Punreddy\\nSolution Architect\\nSharath Punreddy is a Solutions Engineer with extensive experience in cloud engineering and a proven track record in optimizing infrastructure for enterprise clients\\nIn my experience, here are tips that can help you better leverage open source large language models (LLMs):\\n\\nOptimize for hardware compatibility: While deploying LLMs, ensure you tailor model configurations to leverage the specific capabilities of your hardware, such as GPUs or TPUs, to achieve maximum efficiency.\\nUtilize model quantization: Implement quantization techniques to reduce model size and computational requirements without significantly compromising performance, making deployment on edge devices feasible.\\nFine-tune with domain-specific data: Enhance the relevance and accuracy of LLMs by fine-tuning them with data specific to your industry or application domain, improving their contextual understanding and performance.\\nIntegrate with complementary tools: Combine LLMs with other AI tools such as vector databases for improved search capabilities or knowledge graphs for enhanced reasoning and contextualization.\\nImplement differential privacy: Apply differential privacy techniques to ensure that the model does not inadvertently expose sensitive information from the training data, enhancing data security.\\n\\nTop open source LLMs in 2024\\n1. LLaMA 3\\n\\nMeta developed the LLaMA 3 family of large language models, which includes a collection of pretrained and instruction-tuned generative text models available in 8 billion (8B) and 70 billion (70B) parameter sizes. These models are optimized for dialogue use cases, such as in conversational AI applications.\\nProject information:\\n\\nLicense: Meta Llama 3 community license\\nGitHub stars: 23.3K\\nContributors: Joseph Spisak et. al.\\nMain corporate sponsor: META\\nOfficial repo link: https://github.com/meta-llama/llama3\\n\\nFeatures:\\n\\nModel sizes: Available in two sizes: 8 billion (8B) and 70 billion (70B) parameters.\\nContext window: Earlier version of Meta LLaMA had a context window of 8K tokens. Version 3.2 upgraded this to 128K tokens.\\nInput and output: These models accept text input and are capable of generating both text and code, making them versatile for various applications such as content creation, code generation, and interactive dialogue.\\nArchitecture: Uses an optimized transformer architecture, which enhances the model’s ability to understand and generate human-like text.\\nTokenizer: Uses a tokenizer with a vocabulary of 128,000 tokens, which helps in efficiently processing and understanding diverse text inputs.\\nTraining procedure: Trained on sequences of 8,192 tokens, utilizing Grouped-Query Attention (GQA) for improved inference efficiency, allowing the models to handle longer contexts.\\n\\n2. Google Gemma 2\\n\\nGoogle DeepMind released Gemma 2, the latest addition to their family of open models designed for researchers and developers. Available in 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 models run at high speeds across different hardware platforms and integrate with popular AI tools.\\nProject information:\\n\\nLicense: Apache 2.0\\nGitHub stars: 5.2K (PyTorch implementation)\\nMain corporate sponsor: Google\\nOfficial repo link: https://huggingface.co/google/gemma-2b\\n\\nFeatures:\\n\\nModel sizes: Available in 9B and 27B parameters, providing options for various computational needs and performance requirements.\\nContext window: Gemma 2 has a context window of 8K tokens.\\nPerformance: According to benchmarks, the 27B model delivers performance similar to models more than twice its size.\\nEfficiency: Designed for efficient inference, the 27B model runs on single TPU hosts, NVIDIA A100 80GB Tensor Core GPUs, or NVIDIA H100 Tensor Core GPUs, reducing costs while maintaining high performance.\\nHardware compatibility: Optimized for fast inference across a range of hardware, from gaming laptops to cloud-based setups. Users can access the models in Google AI Studio or use the quantized version with Gemma.cpp on CPUs.\\nIntegration: Compatible with major AI frameworks like Hugging Face Transformers, JAX, PyTorch, and TensorFlow via Keras 3.0, vLLM, Gemma.cpp, Llama.cpp, and Ollama. It also integrates with NVIDIA TensorRT-LLM and is optimized for NVIDIA NeMo.\\n\\n\\nSource: Google\\n3. Command R+\\n\\nCohere’s Command R+ is built for enterprise use cases and optimized for conversational interactions and long-context tasks. It is recommended for workflows that rely on sophisticated Retrieval Augmented Generation (RAG) functionality and multi-step tool use (agents).\\nProject information:\\nCommand R+ is part of the proprietary Cohere platform. However, Cohere has released an open research version of the model on Hugging Face, which is available for non-commercial use. You can get the open version here.\\nFeatures:\\n\\nModel capabilities: Follows instructions and performs language tasks with high quality and reliability.\\nContext window: Supports a context length of 128k tokens and can generate up to 4k output tokens, making it suitable for complex RAG workflows and multi-step tool use.\\nMultilingual support: The model is optimized for English, French, Spanish, Italian, German, Brazilian Portuguese, Japanese, Korean, Simplified Chinese, and Arabic. It also includes pre-training data for 13 additional languages.\\nRetrieval augmented generation: Can ground its English-language generations by generating responses based on supplied document snippets and including citations to indicate the source of the information.\\nMulti-step tool use: Can connect to external tools like search engines, APIs, functions, and databases. The model can call more than one tool in a sequence of steps, reason dynamically, and adapt based on external information.\\n\\n4. Mistral-8x22b\\n\\nMixtral-8x22B is a sparse Mixture-of-Experts (SMoE) model that leverages 39 billion active parameters out of a total 141 billion. It can handle NLP tasks in multiple languages and has strong capabilities in mathematics and coding.\\nProject information:\\n\\nLicense: Apache 2.0\\nGitHub stars: 9.2K (Mistral AI)\\nMain corporate sponsor: Mistral AI\\nOfficial repo link: https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1\\n\\nFeatures:\\n\\nLanguage proficiency: Fluent in English, French, Italian, German, and Spanish, enabling effective communication and understanding across these major languages.\\nContext window: 64K tokens.\\nMathematics and coding: Supports complex problem-solving and software development tasks.\\nFunction calling: Natively capable of function calling, enhanced by a constrained output mode implemented on la Plateforme, enabling large-scale application development and tech stack modernization.\\n\\n\\nSource: Mistral\\n5. Falcon 2\\n\\nFalcon 2 is an AI model providing multilingual and multimodal capabilities, including unique vision-to-language functionality. Available in two versions, Falcon 2 11B and Falcon 2 11B VLM, it is independently verified by the Hugging Face Leaderboard.\\nProject information:\\n\\nLicense: Apache 2.0\\nMain corporate sponsor: Technology Innovation Institute\\nOfficial repo link: https://github.com/falconpl/Falcon2\\n\\nFeatures:\\n\\nModel versions: Falcon 2 11B is a language model trained on 5.5 trillion tokens with 11 billion parameters. Falcon 2 11B VLM is a vision-to-language model, enabling the conversion of visual inputs into textual outputs.\\nContext window: 8K tokens.\\nMultilingual: Supports multiple languages, including English, French, Spanish, German, and Portuguese.\\nMultimodal capabilities: The VLM version can interpret images and convert them to text, supporting applications across healthcare, finance, eCommerce, education, and legal sectors. It is suitable for document management, digital archiving, and context indexing.\\nEfficiency: Operates on a single GPU, supporting scalability and deployment on lighter infrastructure like laptops and other devices.\\n\\n\\nSource: Falcon\\n6. Grok 1.5\\n\\nGrok-1.5, developed by Elon Musk’s xAI, builds on the foundation of Grok-1. Grok-1.5V expands traditional text-based LLM capabilities to include visual understanding. This multimodal model can interpret various image types and perform complex reasoning tasks by combining linguistic skills with visual analysis.\\nFeatures:\\n\\nContext window: 128K tokens.\\nMultimodal capabilities: Processes and understands a range of visual information, including documents, diagrams, and photographs. It can analyze documents, interpret user interface elements, understand photographs, and handle dynamic visual content such as videos and animations.\\nMulti-disciplinary reasoning: Can combine visual and textual information to perform complex reasoning tasks. It can answer questions about scientific diagrams, follow instructions involving text and images, and provide diagnostic insights in medical imaging by analyzing scans and patient records.\\nReal-world spatial understanding: Performs strongly on the RealWorldQA benchmark, which measures an AI model’s ability to understand and interact with real-world environments.\\n\\n\\nSource: X.ai\\n7. Qwen1.5\\n\\nQwen1.5, developed by Chinese cloud service provider Alibaba Cloud, is the latest update in the Qwen series, offering base and chat models in a range of sizes: 0.5B, 1.8B, 4B, 7B, 14B, 32B, 72B, and 110B. It also includes a Mixture of Experts (MoE) model. All versions are open-sourced and available in various quantized formats to improve usability.\\nProject information:\\n\\nLicense: Tongyi Qianwen research license\\nGitHub stars: 6.3K\\nContributors: Qwen team\\nMain corporate sponsor: Alibaba China\\nOfficial repo link: https://github.com/QwenLM/Qwen2\\n\\nFeatures:\\n\\nModel versions: Available in sizes from 0.5B to 110B parameters, including a Mixture of Experts (MoE) model. Quantized versions include Int4, Int8, GPTQ, AWQ, and GGUF models.\\nContext window: Supports contexts up to 32K tokens, performing well on the L-Eval benchmark, which measures long-context generation capabilities.\\nIntegration: Qwen1.5’s code is integrated with Hugging Face Transformers (version 4.37.0 and above). The models are also supported by frameworks like vLLM, SGLang, AutoAWQ, AutoGPTQ, Axolotl, and LLaMA-Factory for fine-tuning, and llama.cpp for local inference.\\nPlatform support: Available on platforms such as Ollama, LMStudio, and API services via DashScope and together.ai.\\nMultilingual capabilities: Evaluated across 12 languages, demonstrating strong performance in exams, understanding, translation, and math tasks.\\n\\n8. BLOOM\\n\\nBLOOM, developed through a large collaboration of AI researchers, aims to democratize access to LLMs, making it possible for academia, nonprofits, and smaller research labs to create, study, and use these models. It is the first model of its size for many languages, including Spanish, French, and Arabic.\\nProject information:\\n\\nLicense: BigScience RAIL license\\nGitHub stars: 129K\\nContributors: Margaret Mitchell et. al.\\nMain corporate sponsor: HuggingFace, BigScience\\nOfficial repo link: Click here\\n\\nFeatures:\\n\\nMultilingual capabilities: Supports 46 natural languages and 13 programming languages.\\nParameter size: Includes 176 billion parameters.\\nAccessibility: Available under the Responsible AI License, allowing individuals and institutions to use and build upon the model. It can be easily integrated into applications via the Hugging Face ecosystem using transformers and accelerators.\\nInference API: An inference API is being finalized to enable large-scale use without dedicated hardware.\\n\\n9. GPT-NeoX\\n\\nGPT-NeoX is a 20 billion parameter autoregressive language model developed by EleutherAI. Trained on the Pile dataset, GPT-NeoX-20B is a dense autoregressive model with publicly available weights. This model, made freely accessible under a permissive license, offers advanced capabilities in language understanding, mathematics, and knowledge-based tasks.\\nProject information:\\n\\nLicense: Apache 2.0\\nGitHub stars: 6.8K\\nMain corporate sponsor: EleutherAI\\nOfficial repo link: https://github.com/EleutherAI/gpt-neox\\n\\nFeatures:\\n\\nModel size: GPT-NeoX-20B has 20 billion parameters, making it one of the largest open-source models available.\\nTraining setup: It uses Megatron and DeepSpeed libraries for training across multiple GPUs, optimized for distributed computing. It supports parallelism techniques like tensor and pipeline parallelism to enhance efficiency.\\nPerformance: The model performs particularly well on natural language understanding and few-shot tasks, surpassing similarly sized models like GPT-3 Curie in some benchmarks.\\nDataset: The model was trained exclusively on English data from the Pile, and is not intended for multilingual tasks.\\nUsage: While versatile, GPT-NeoX-20B is not fine-tuned for consumer-facing tasks like chatbots and may require supervision when used in such settings.\\n\\n10. Vicuna-13B\\n\\nVicuna-13B is an open source chatbot model developed by fine-tuning the LLaMA model with user-shared conversations from ShareGPT. It has achieved over 90% of the quality of OpenAI’s ChatGPT, based on preliminary evaluations using GPT-4 as a judge. The development cost of Vicuna-13B was approximately $300, and both the code and weights are publicly available for non-commercial use.\\nProject information:\\n\\nLicense: Non-commercial license\\nGitHub stars: 35.8K\\nContributors: Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica\\nMain corporate sponsor: LMSYS\\nOfficial repo link: https://github.com/lm-sys/FastChat\\n\\nFeatures:\\n\\nPerformance: Preliminary evaluations using GPT-4 indicate that Vicuna-13B achieves over 90% of the quality of ChatGPT and early versions of Google Gemini. It also outperforms other models like LLaMA and Stanford Alpaca.\\nTraining: The model was trained using PyTorch FSDP on 8 A100 GPUs in one day, with a focus on multi-turn conversations and long sequence handling. It was trained on approximately 70,000 user-shared conversations from ShareGPT.\\nServing: A lightweight distributed serving system was implemented to serve multiple models with flexible GPU worker integration, using SkyPilot managed spot instances to reduce serving costs.\\n\\nNetApp Instaclustr: Empowering open source large language models\\nOpen source large language models have revolutionized natural language processing (NLP) and artificial intelligence (AI) applications by enabling advanced text generation, sentiment analysis, language translation, and more. However, training and deploying these models can be resource-intensive and complex. NetApp Instaclustr steps in to support open source large language models, providing a robust infrastructure and managed services that simplify the process. In this article, we will explore how NetApp Instaclustr empowers organizations to leverage the full potential of open source large language models.\\nTraining large language models requires substantial computational resources and storage capacity. NetApp Instaclustr offers a scalable and high-performance infrastructure that can handle the demanding requirements of model training. By leveraging the distributed computing capabilities and storage capacity provided by NetApp Instaclustr, organizations can efficiently train large language models, reducing the time and resources required for the training process.\\nOnce trained, deploying large language models can present challenges due to their size and resource requirements. NetApp Instaclustr simplifies the deployment process by offering managed services that handle the infrastructure and operational aspects. It takes care of provisioning the necessary compute resources, managing storage, and ensuring high availability and fault tolerance. This allows organizations to focus on utilizing the models for their specific NLP and AI applications without the burden of managing the underlying infrastructure.\\nNetApp Instaclustr leverages its scalable infrastructure to support the deployment of open source large language models. As the demand for processing power and storage increases, organizations can easily scale their infrastructure up or down to accommodate the workload. This scalability ensures optimal performance, enabling efficient and fast processing of text data using large language models.\\nOpen source large language models often deal with sensitive data, and ensuring data security is crucial. NetApp Instaclustr prioritizes data security by providing robust security measures, including encryption at rest and in transit, role-based access control, and integration with identity providers. These security features help organizations protect their data and comply with industry regulations and privacy standards.\\nNetApp Instaclustr offers comprehensive monitoring and support services for open source large language models. It provides real-time monitoring capabilities, allowing organizations to track the performance and health of their models. In case of any issues or concerns, NetApp Instaclustr’s support team is readily available to provide assistance and ensure minimal downtime, enabling organizations to maintain the reliability and availability of their language models.\\nManaging the infrastructure for open source large language models can be costly. NetApp Instaclustr helps organizations optimize costs by offering flexible pricing models. With pay-as-you-go options, organizations can scale their resources based on demand and pay only for what they use. This eliminates the need for upfront investments and provides cost predictability, making it more accessible for organizations of all sizes to leverage open source large language models.\\nFor more information:\\n\\nUse Your Data in LLMs With the Vector Database You Already Have: The New Stack\\nHow To Improve Your LLM Accuracy and Performance With PGVector and PostgreSQL®: Introduction to Embeddings and the Role of PGVector\\nPowering AI Workloads with Intelligent Data Infrastructure and Open Source\\nVector Search in Apache Cassandra® 5.0\\n\\nRelated content\\nVector databases and LLMs: Better together A vector database handles high-dimensional vectors from machine learning models. LLMs are advanced models that understand and ... Read more\\nHow To Improve Your LLM Accuracy and Performance With PGVector and PostgreSQL®: Introduction to Embeddings and the Role of PGVector Instaclustr has recently introduced the PGVector extension to our PostgreSQL Managed Service... Read more\\nUse Your Data in LLMs With the Vector Database You Already Have: The New Stack Open source vector databases are among the top options out there for AI development, including some you may already be familiar ... Read more\\nSpin up a cluster\\nIn minutes\\n\\nCheck it out\\n\\n\\n\\nPlatform +\\nExplore our platform\\nSecurity and trust\\nHosting\\nStream\\nApache Kafka®\\nKafka® Connect\\n\\n\\nStore\\nPostgreSQL®\\nApache Cassandra®\\nValkey™\\n\\n\\nAnalyze\\nClickHouse®\\n\\n\\nSearch\\nOpenSearch\\n\\n\\nOrchestrate\\nCadence\\nApache ZooKeeper™\\n\\n\\n\\n\\nColumn 2\\nPricing +\\nExplore our pricing\\n\\n\\nServices +\\nSupport\\nConsulting\\nTraining\\n\\n\\nAbout +\\nAbout us\\nCustomers\\nCareers\\nOur commitment to open source\\n\\n\\n\\n\\nResources +\\nSign up\\nDocumentation\\nQuick start videos\\nIntegrations\\nSupport portal\\nBlog\\nEvents\\nContent library\\nGlossary\\nStream\\nApache Kafka®\\n\\n\\nStore\\nPostgreSQL®\\nApache Cassandra®\\nValkey™\\n\\n\\nAnalyze\\nApache Spark™\\n\\n\\nSearch\\nOpenSearch\\n\\n\\nData infrastructure\\nVector search\\nData streaming\\n\\n\\n\\n\\n\\nPolicies +\\n\\nTerms of service\\nSecurity and trust\\nSecurity policy\\nSupport inclusions\\nSubscription specifications\\nService-level agreements\\nCancellation policy\\nCookie declaration\\nPrivacy policy\\n\\n\\n\\nPlatform +\\n\\nExplore our platform\\nSecurity and trust\\nHosting\\nStream\\nApache Kafka®\\nKafka® Connect\\n\\n\\nStore\\nPostgreSQL®\\nApache Cassandra®\\nValkey™\\n\\n\\nAnalyze\\nClickHouse®\\n\\n\\nSearch\\nOpenSearch\\n\\n\\nOrchestrate\\nCadence\\nApache ZooKeeper™\\n\\n\\n\\n\\nPricing\\nExplore our pricing\\n\\n\\nServices +\\nSupport\\nConsulting\\nTraining\\n\\n\\nAbout +\\nAbout us\\nCustomers\\nCareers\\nOur commitment to open source\\n\\n\\nResources +\\nSign up\\nDocumentation\\nQuick start videos\\nIntegrations\\nSupport portal\\nBlog\\nEvents\\nContent library\\nGlossary\\nStream\\nApache Kafka®\\n\\n\\nStore\\nPostgreSQL®\\nApache Cassandra®\\nValkey™\\n\\n\\nAnalyze\\nApache Spark™\\n\\n\\nSearch\\nOpenSearch\\n\\n\\nData infrastructure\\nVector search\\nData streaming\\n\\n\\n\\n\\nPolicies +\\nTerms of service\\nSecurity and trust\\nSecurity policy\\nSupport inclusions\\nSubscription specifications\\nService-level agreements\\nCancellation policy\\nCookie declaration\\nPrivacy policy\\n\\n\\n\\n©2025 NetApp Copyright. NETAPP, the NETAPP logo, Instaclustr and the marks listed at https://www.netapp.com/TM are trademarks of NetApp, Inc. Other company and product names may be trademarks of their respective owners. Apache®, Apache Cassandra®, Apache Kafka®, Apache Spark™, and Apache ZooKeeper™ are trademarks of The Apache Software Foundation.', '## Title\\n\\nTop 9 Large Language Models as of March 2025 | Shakudo\\n\\n## Content\\n\\nTop 9 Large Language Models as of Feburary 2025 | Shakudo\\nLatest in Insights : When to Choose Deep Learning Over Machine Learning (And Vice Versa)\\n\\n\\nWhy SHakudo\\n\\n Data & AI OS Build your ideal data stack on one unified platform Learn more >\\nshakudo AI Applications\\n Text to SQL Workflow Automation Vector Database Reverse ETLSee all >\\nComponents\\nSolutions\\n\\nShakudo for Industries\\nAerospace\\nAutomotive & Transportation\\nClimate & Energy\\nFinancial Services\\nHealthcare & Life Sciences\\nHigher Education\\nManufacturing\\nReal Estate\\nRetail\\nSports\\nTechnology & Software\\nShakudo Use Cases\\nAutomate Custom Sustainability Report Population\\nChat with Enterprise Knowledge Base Using AI Assistants\\nGenerate Real-World Evidence for Healthcare Decisions\\nOptimize Ticket Pricing with Dynamic Demand Modeling\\nDetect Hidden Red Flags in Company Data\\nMonitor Market Sentiment Across Multiple Sources\\nSee all >\\nResources\\n\\n Case Studies Learn how leading companies leverage data & AI on Shakudo blog Read what\\'s new at Shakudo and the data and AI world white papers Access in-depth reports and guides on data & AI solutions Docs Explore comprehensive guides on the Shakudo platform\\n  Case Study How CentralReach uses Shakudo to Cut Time-To-Deployment to Launch New AI- Powered Solutions\\n  Case Study How AI is Changing the Game for the Cleveland Cavaliers\\nCompany\\n\\n ABout Us Learn about our mission and values Careers Join us in building the next-gen data stack Partners Learn about the relationships that make it happen Contact us Have a question? We\\'re here to help\\nAI WorkshopGet a Demo\\n\\n← Back to Blog\\nInsights\\nTop 9 Large Language Models as of Feburary 2025\\nAuthor(s):\\n\\nNo items found.\\nUpdated on:\\nFebruary 7, 2025\\n\\nTable of contents\\nExample H2\\nExample H3\\nMentioned Components\\nNo items found.\\n<>\\nGet the latest updates in Data & AI straight to your inboxWe’ll email you once per week—and never share your information.\\n🎉 Success! You\\'re now signed up for the Shakudo newsletter.\\nOops! Something went wrong while submitting the form.\\nIntroduction\\nIf we had to choose one word to describe the rapid evolution of AI today, it would probably be something along the lines of explosive. As predicted by the Market Research Future report, the large language model (LLM) market in North America alone is expected to reach $105.5 billion by 2030. The exponential growth of AI tools combined with access to massive troves of text data has opened gates for better and more advanced content generation than we had ever hoped. Yet, such rapid expansion also makes it harder than ever to navigate and select the right tools among the diverse LLM models available.\\nThe goal of this post is to keep you, the AI enthusiast and professional, up-to-date with current trends and essential innovations in the field. Below, we highlighted the top 9 LLMs that we think are currently making waves in the industry, each with distinct capabilities and specialized strengths, excelling in areas such as natural language processing, code synthesis, few-shot learning, or scalability. While we believe there is no one-size-fits-all LLM for every use case, we hope that this list can help you identify the most current and well-suited LLM model that meets your business’s unique requirements.\\n1. GPT\\n\\nOur list kicks off with OpenAI\\'s Generative Pre-trained Transformer (GPT) models, which have consistently exceeded their previous capabilities with each new release. Compared to its prior models, the latest ChatGPT-4o and ChatGPT-4o mini models offer significantly faster processing speeds and enhanced capabilities across text, voice, and vision.\\nThe latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data. Both of these models are equipped with multimodal capabilities to handle images as well as audio data.\\nDespite having advanced conversational and reasoning capabilities, note that GPT is a proprietary model, meaning that the training data and parameters are kept confidential by OpenAI, and access to full functionality is restricted–a commercial license or subscription is often required to unlock the complete range of features. In this case, we recommend this model for businesses looking to adopt an LLM that excels in conversational dialogue, multi-step reasoning, efficient computation, and real-time interactions without the constraints of a budget.\\nFor companies who are curious to try out the proprietary models on the market before fully committing to one due to budget constraints or uncertainties about its long-term integration, Shakudo offers a compelling alternative. Our platform currently features a diverse selection of advanced LLMs with simplified deployment and scalability. With a simple subscription, you can access and assess the value of proprietary models, like GPT, before making a substantial investment.\\n2.DeepSeek\\n\\nDeepseek-R1 Benchmark. Source: deepseek.com\\nWith its latest R1 model, the Chinese AI company DeepSeek has once again set new benchmarks for innovation in the AI community. As of January 24th, the DeepSeek-R1 model is ranked fourth on Chatbot Arena, and top as the best open-source LM.\\nThe DeepSeek-R1 is a 671B parameter Mixture-of-Experts (MoE) model with 37B activated parameters per token, trained through large-scale reinforcement learning with a strong focus on reasoning capabilities. The model excels at understanding and handling long-form content and demonstrates superior performance in complex tasks such as mathematics and code generation. The model is approximately 30 times more cost-efficient than OpenAI-o1 and 5 times faster, offering groundbreaking performance at a fraction of the cost. Moreover, it has shown exceptional precision in tasks requiring complex pattern recognition, such as genomic data analysis, medical imaging, and large-scale scientific simulations.\\nDeepSeek-R1’s capabilities are transformative when it comes to integration with proprietary enterprise data such as PII and financial records. Leveraging retrieval-augmented generation (RAG), enterprises can connect the model to their internal data sources to enable highly personalized, context-aware interactions—all while maintaining stringent security and compliance standards. With Shakudo, you can streamline the deployment and integration of advanced AI models like DeepSeek by automating the setup, deployment, and management processes. This eliminates the need for businesses to invest in and maintain extensive computing infrastructure. By operating within your existing infrastructure, the platform ensures seamless integration, enhanced security, and optimal performance without requiring significant in-house resources or specialized expertise.\\n3. Qwen\\n\\n\\u200d\\nAlibaba released Qwen2.5-Max earlier last month, the new model designed to deliver enhanced performance for large-scale natural language processing tasks. In instruct model evaluations, this new model outperforms DeepSeek V3 in benchmarks such as Arena-Hard, LiveBench, LiveCodeBench, and GPQA-Diamond, while also delivering strong performance in other assessments like MMLU-Pro.\\nQwen2.5-Max is pretrained on over 20 trillion tokens. While specific details about its parameter count and token window size are not publicly disclosed, Qwen2.5-Max is designed for low-latency, high-efficiency tasks, making it suitable for applications requiring quick, accurate responses with low latency. Its smaller size allows for deployment on devices with limited computational resources.\\nFor businesses and users looking for higher-performance models for natural language processing and AI-driven tasks, the Qwen 2.5 model is available on platforms like Hugging Face and ModelScope. This model boasts from 0.5 billion to 72 billion parameters, featuring context windows of up to 128,000 tokens, and is excellent for code generation, debugging, and automated forecasting.\\n4. LG AI\\n\\nsource\\nEXAONE 3.0 is a bilingual LLM developed by LG AI Research. The company released its latest model in December 2024 with excellent performance across various benchmarks and real-world applications.\\nWith 7.8 billion parameters, EXAONE 3.0 is capable of understanding and generating human-like text in multiple languages across complex domains, including coding, mathematics, patents, and chemistry. This model has also been optimized to reduce inference processing time by 56%, memory usage by 35%, and operating costs by 72%, ensuring that it remains cost-effective while maintaining high performance.\\nLG AI Research has open-sourced the 7.8B parameter instruction-tuned version of EXAONE 3.0 for non-commercial research purposes. This is a model we recommend to software companies or tech startups for generating Python code, assisting developers in troubleshooting, and creating APIs or other backend components.\\n5. LlaMA\\n\\nMeta is still leading the front with their state-of-the-art LlaMa models. The company released its latest LlaMA 3.3 model in December 2024, featuring multimodal capabilities that can process both text and image for in-depth analysis and response generation, such as interpreting charts, maps, or translating texts identified in an image.\\nLlaMA 3.3 improves on previous models with a longer context window of up to 128,000 tokens and an optimized transformer architecture. With a parameter of 70 billion, this model outperforms open-source and proprietary alternatives in areas such as multilingual dialogue, reasoning, and coding.\\nUnlike ChatGPT models, LlaMA 3 is open-source, giving users the flexibility to access and deploy freely on their cloud depending on the specific requirements of their infrastructure, security preferences, or customization needs. We recommend this model to businesses looking for advanced content generation and language understanding, such as those in customer service, education, marketing, and consumer markets. The openness of these models also allows for your greater control over the model’s performance, tuning, and integration into existing workflows.\\n6. Claude\\n\\nNext on our list is Claude, more specifically, the latest Claude 3.5 Sonnet model developed by Anthropic. We believe that Claude is arguably one of the most significant competitors to GPT since all of its current models–Claude 3 Haiku, Claude 3.5 Sonnet, and Claude 3 Opus–are designed with incredible contextual understanding capabilities that position themselves as the top conversational AI closely aligned with nuanced human interactions.\\nWhile the specific parameters of Claude 3.5 Sonnet remain undisclosed, the model boasts an impressive context window of 200,000 tokens, equivalent to approximately 150,000 words or 300 pages of text.\\nThe current Claude subscription service is credit-based, and the cost can go as high as $2,304/month for enterprise plans tailored to high-volume users. We recommend Claude to mature or mid-stage businesses looking not only to adopt an AI that facilitates human-like interactions but also to enhance their coding capabilities since the Claude 3.5 Sonnet model is currently reaching a 49.0% performance score on the SWE-bench Verified benchmark, placing it as third among all publicly available models, including reasoning models and systems specifically designed for agentic coding.\\n7. Mistral\\n\\nMistral\\'s latest model – Mistral Small 3, a latency-optimized model was released under the Apache 2.0 license at the end of January. This 24-billion-parameter model is designed for low-latency, high-efficiency tasks. It processes approximately 150 tokens per second, making it over three times faster than Llama 3.3 70B on the same hardware.\\nThis new model is ideal for applications requiring quick, accurate responses with low latency, such as virtual assistants, real-time data processing, and on-device command and control. Its smaller size allows for deployment on devices with limited computational resources.\\nMistral Small 3 is currently open-source under the Apache 2.0 license. This means you can freely access and use the model for your own applications, provided you comply with the license terms. Since it is designed to be easily deployable, including on hardware with limited resources like a single GPU or even a MacBook with 32GB RAM, we\\'d recommend this to early-stage businesses looking to implement low-latency AI solutions without the need for extensive hardware infrastructure.\\n8. Gemini\\n\\nGemini is a family of closed-source LLM models developed by Google. The latest model—Gemini 2.0 Flash—operates at twice the speed of Gemini 1.5 Pro, offering substantial improvements in speed, reasoning, and multimodal processing capabilities.\\nWith that being said, Gemini remains a proprietary model; if your company deals with sensitive or confidential data regularly, you might be concerned about sending it to external servers due to security reasons. To address this concern, we recommend that you double-check vendor compliance regulations to ensure data privacy and security standards are met, such as adherence to GDPR, HIPAA, or other relevant data protection laws.\\nIf you’re looking for an open-source alternative that exhibits capabilities almost as good as Gemini, Google’s latest Gemma model, Gemma 2, offers three models available in 2 billion, 9 billion, and 27 billion parameters with a context window of 8,200. For businesses looking for a rather economic option, this is the optimal choice that interprets and understands messages with remarkable accuracy.\\n9. Command\\n\\nCommand R is a family of scalable models developed by Cohere with the goal of balancing high performance with strong accuracy, just like Claude. Both the Command R and Command R+ models offer APIs specifically optimized for Retrieval Augmented Generation (RAG). This means that these models can combine large-scale language generation with real-time information retrieval techniques for much more contextually aware outputs.\\nCurrently, the Command R+ model boasts 104 billion parameters and offers an industry-leading 128,000 token context window for enhanced long-form processing and multi-turn conversation capabilities.\\nOne of the perks of working with an open-source model is also to avoid vendor lock-in. Heavy reliance on a particular type of proprietary model may make it difficult for you to switch to alternative models when your business starts growing or the landscape changes. Cohere approaches this in a hybrid way, meaning that you can access and modify the model for personal usage but need a license for commercial use. In this case, we recommend this model for businesses that want flexibility in experimentation without a long-term commitment to a single vendor.\\n\\nWhitepaper\\nIntroduction\\nIf we had to choose one word to describe the rapid evolution of AI today, it would probably be something along the lines of explosive. As predicted by the Market Research Future report, the large language model (LLM) market in North America alone is expected to reach $105.5 billion by 2030. The exponential growth of AI tools combined with access to massive troves of text data has opened gates for better and more advanced content generation than we had ever hoped. Yet, such rapid expansion also makes it harder than ever to navigate and select the right tools among the diverse LLM models available.\\nThe goal of this post is to keep you, the AI enthusiast and professional, up-to-date with current trends and essential innovations in the field. Below, we highlighted the top 9 LLMs that we think are currently making waves in the industry, each with distinct capabilities and specialized strengths, excelling in areas such as natural language processing, code synthesis, few-shot learning, or scalability. While we believe there is no one-size-fits-all LLM for every use case, we hope that this list can help you identify the most current and well-suited LLM model that meets your business’s unique requirements.\\n1. GPT\\n\\nOur list kicks off with OpenAI\\'s Generative Pre-trained Transformer (GPT) models, which have consistently exceeded their previous capabilities with each new release. Compared to its prior models, the latest ChatGPT-4o and ChatGPT-4o mini models offer significantly faster processing speeds and enhanced capabilities across text, voice, and vision.\\nThe latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data. Both of these models are equipped with multimodal capabilities to handle images as well as audio data.\\nDespite having advanced conversational and reasoning capabilities, note that GPT is a proprietary model, meaning that the training data and parameters are kept confidential by OpenAI, and access to full functionality is restricted–a commercial license or subscription is often required to unlock the complete range of features. In this case, we recommend this model for businesses looking to adopt an LLM that excels in conversational dialogue, multi-step reasoning, efficient computation, and real-time interactions without the constraints of a budget.\\nFor companies who are curious to try out the proprietary models on the market before fully committing to one due to budget constraints or uncertainties about its long-term integration, Shakudo offers a compelling alternative. Our platform currently features a diverse selection of advanced LLMs with simplified deployment and scalability. With a simple subscription, you can access and assess the value of proprietary models, like GPT, before making a substantial investment.\\n2.DeepSeek\\n\\nDeepseek-R1 Benchmark. Source: deepseek.com\\nWith its latest R1 model, the Chinese AI company DeepSeek has once again set new benchmarks for innovation in the AI community. As of January 24th, the DeepSeek-R1 model is ranked fourth on Chatbot Arena, and top as the best open-source LM.\\nThe DeepSeek-R1 is a 671B parameter Mixture-of-Experts (MoE) model with 37B activated parameters per token, trained through large-scale reinforcement learning with a strong focus on reasoning capabilities. The model excels at understanding and handling long-form content and demonstrates superior performance in complex tasks such as mathematics and code generation. The model is approximately 30 times more cost-efficient than OpenAI-o1 and 5 times faster, offering groundbreaking performance at a fraction of the cost. Moreover, it has shown exceptional precision in tasks requiring complex pattern recognition, such as genomic data analysis, medical imaging, and large-scale scientific simulations.\\nDeepSeek-R1’s capabilities are transformative when it comes to integration with proprietary enterprise data such as PII and financial records. Leveraging retrieval-augmented generation (RAG), enterprises can connect the model to their internal data sources to enable highly personalized, context-aware interactions—all while maintaining stringent security and compliance standards. With Shakudo, you can streamline the deployment and integration of advanced AI models like DeepSeek by automating the setup, deployment, and management processes. This eliminates the need for businesses to invest in and maintain extensive computing infrastructure. By operating within your existing infrastructure, the platform ensures seamless integration, enhanced security, and optimal performance without requiring significant in-house resources or specialized expertise.\\n3. Qwen\\n\\n\\u200d\\nAlibaba released Qwen2.5-Max earlier last month, the new model designed to deliver enhanced performance for large-scale natural language processing tasks. In instruct model evaluations, this new model outperforms DeepSeek V3 in benchmarks such as Arena-Hard, LiveBench, LiveCodeBench, and GPQA-Diamond, while also delivering strong performance in other assessments like MMLU-Pro.\\nQwen2.5-Max is pretrained on over 20 trillion tokens. While specific details about its parameter count and token window size are not publicly disclosed, Qwen2.5-Max is designed for low-latency, high-efficiency tasks, making it suitable for applications requiring quick, accurate responses with low latency. Its smaller size allows for deployment on devices with limited computational resources.\\nFor businesses and users looking for higher-performance models for natural language processing and AI-driven tasks, the Qwen 2.5 model is available on platforms like Hugging Face and ModelScope. This model boasts from 0.5 billion to 72 billion parameters, featuring context windows of up to 128,000 tokens, and is excellent for code generation, debugging, and automated forecasting.\\n4. LG AI\\n\\nsource\\nEXAONE 3.0 is a bilingual LLM developed by LG AI Research. The company released its latest model in December 2024 with excellent performance across various benchmarks and real-world applications.\\nWith 7.8 billion parameters, EXAONE 3.0 is capable of understanding and generating human-like text in multiple languages across complex domains, including coding, mathematics, patents, and chemistry. This model has also been optimized to reduce inference processing time by 56%, memory usage by 35%, and operating costs by 72%, ensuring that it remains cost-effective while maintaining high performance.\\nLG AI Research has open-sourced the 7.8B parameter instruction-tuned version of EXAONE 3.0 for non-commercial research purposes. This is a model we recommend to software companies or tech startups for generating Python code, assisting developers in troubleshooting, and creating APIs or other backend components.\\n5. LlaMA\\n\\nMeta is still leading the front with their state-of-the-art LlaMa models. The company released its latest LlaMA 3.3 model in December 2024, featuring multimodal capabilities that can process both text and image for in-depth analysis and response generation, such as interpreting charts, maps, or translating texts identified in an image.\\nLlaMA 3.3 improves on previous models with a longer context window of up to 128,000 tokens and an optimized transformer architecture. With a parameter of 70 billion, this model outperforms open-source and proprietary alternatives in areas such as multilingual dialogue, reasoning, and coding.\\nUnlike ChatGPT models, LlaMA 3 is open-source, giving users the flexibility to access and deploy freely on their cloud depending on the specific requirements of their infrastructure, security preferences, or customization needs. We recommend this model to businesses looking for advanced content generation and language understanding, such as those in customer service, education, marketing, and consumer markets. The openness of these models also allows for your greater control over the model’s performance, tuning, and integration into existing workflows.\\n6. Claude\\n\\nNext on our list is Claude, more specifically, the latest Claude 3.5 Sonnet model developed by Anthropic. We believe that Claude is arguably one of the most significant competitors to GPT since all of its current models–Claude 3 Haiku, Claude 3.5 Sonnet, and Claude 3 Opus–are designed with incredible contextual understanding capabilities that position themselves as the top conversational AI closely aligned with nuanced human interactions.\\nWhile the specific parameters of Claude 3.5 Sonnet remain undisclosed, the model boasts an impressive context window of 200,000 tokens, equivalent to approximately 150,000 words or 300 pages of text.\\nThe current Claude subscription service is credit-based, and the cost can go as high as $2,304/month for enterprise plans tailored to high-volume users. We recommend Claude to mature or mid-stage businesses looking not only to adopt an AI that facilitates human-like interactions but also to enhance their coding capabilities since the Claude 3.5 Sonnet model is currently reaching a 49.0% performance score on the SWE-bench Verified benchmark, placing it as third among all publicly available models, including reasoning models and systems specifically designed for agentic coding.\\n7. Mistral\\n\\nMistral\\'s latest model – Mistral Small 3, a latency-optimized model was released under the Apache 2.0 license at the end of January. This 24-billion-parameter model is designed for low-latency, high-efficiency tasks. It processes approximately 150 tokens per second, making it over three times faster than Llama 3.3 70B on the same hardware.\\nThis new model is ideal for applications requiring quick, accurate responses with low latency, such as virtual assistants, real-time data processing, and on-device command and control. Its smaller size allows for deployment on devices with limited computational resources.\\nMistral Small 3 is currently open-source under the Apache 2.0 license. This means you can freely access and use the model for your own applications, provided you comply with the license terms. Since it is designed to be easily deployable, including on hardware with limited resources like a single GPU or even a MacBook with 32GB RAM, we\\'d recommend this to early-stage businesses looking to implement low-latency AI solutions without the need for extensive hardware infrastructure.\\n8. Gemini\\n\\nGemini is a family of closed-source LLM models developed by Google. The latest model—Gemini 2.0 Flash—operates at twice the speed of Gemini 1.5 Pro, offering substantial improvements in speed, reasoning, and multimodal processing capabilities.\\nWith that being said, Gemini remains a proprietary model; if your company deals with sensitive or confidential data regularly, you might be concerned about sending it to external servers due to security reasons. To address this concern, we recommend that you double-check vendor compliance regulations to ensure data privacy and security standards are met, such as adherence to GDPR, HIPAA, or other relevant data protection laws.\\nIf you’re looking for an open-source alternative that exhibits capabilities almost as good as Gemini, Google’s latest Gemma model, Gemma 2, offers three models available in 2 billion, 9 billion, and 27 billion parameters with a context window of 8,200. For businesses looking for a rather economic option, this is the optimal choice that interprets and understands messages with remarkable accuracy.\\n9. Command\\n\\nCommand R is a family of scalable models developed by Cohere with the goal of balancing high performance with strong accuracy, just like Claude. Both the Command R and Command R+ models offer APIs specifically optimized for Retrieval Augmented Generation (RAG). This means that these models can combine large-scale language generation with real-time information retrieval techniques for much more contextually aware outputs.\\nCurrently, the Command R+ model boasts 104 billion parameters and offers an industry-leading 128,000 token context window for enhanced long-form processing and multi-turn conversation capabilities.\\nOne of the perks of working with an open-source model is also to avoid vendor lock-in. Heavy reliance on a particular type of proprietary model may make it difficult for you to switch to alternative models when your business starts growing or the landscape changes. Cohere approaches this in a hybrid way, meaning that you can access and modify the model for personal usage but need a license for commercial use. In this case, we recommend this model for businesses that want flexibility in experimentation without a long-term commitment to a single vendor.\\nGet the whitepaper\\nTop 9 Large Language Models as of Feburary 2025\\nBy clicking \"Download,\" you agree to Shakudo processing your personal data in accordance with its Privacy Notice.\\nThank you for filling out the form. The whitepaper you have requested is available for download below.  \\nDownload White Paper\\nOops! Something went wrong while submitting the form.\\nGet the whitepaper\\nTop 9 Large Language Models as of Feburary 2025\\nThank you for your interest. Click the button below to download whitepaper you have requested.  \\nDownload White Paper\\n\\nTop 9 Large Language Models as of Feburary 2025\\nExplore the top 9 LLMs making waves in the AI world and what each of them excel at\\n\\n| Case Study\\nTop 9 Large Language Models as of Feburary 2025\\n\\nKey results\\nAbout\\nindustry\\nTech Stack\\nNo items found.\\n<>\\nIntroduction\\nIf we had to choose one word to describe the rapid evolution of AI today, it would probably be something along the lines of explosive. As predicted by the Market Research Future report, the large language model (LLM) market in North America alone is expected to reach $105.5 billion by 2030. The exponential growth of AI tools combined with access to massive troves of text data has opened gates for better and more advanced content generation than we had ever hoped. Yet, such rapid expansion also makes it harder than ever to navigate and select the right tools among the diverse LLM models available.\\nThe goal of this post is to keep you, the AI enthusiast and professional, up-to-date with current trends and essential innovations in the field. Below, we highlighted the top 9 LLMs that we think are currently making waves in the industry, each with distinct capabilities and specialized strengths, excelling in areas such as natural language processing, code synthesis, few-shot learning, or scalability. While we believe there is no one-size-fits-all LLM for every use case, we hope that this list can help you identify the most current and well-suited LLM model that meets your business’s unique requirements.\\n1. GPT\\n\\nOur list kicks off with OpenAI\\'s Generative Pre-trained Transformer (GPT) models, which have consistently exceeded their previous capabilities with each new release. Compared to its prior models, the latest ChatGPT-4o and ChatGPT-4o mini models offer significantly faster processing speeds and enhanced capabilities across text, voice, and vision.\\nThe latest models are believed to have more than 175 billion parameters—surpassing the parameter count of ChatGPT-3, which had 175 billion—and a substantial context window of 128,000 tokens, making them highly efficient at processing and generating large amounts of data. Both of these models are equipped with multimodal capabilities to handle images as well as audio data.\\nDespite having advanced conversational and reasoning capabilities, note that GPT is a proprietary model, meaning that the training data and parameters are kept confidential by OpenAI, and access to full functionality is restricted–a commercial license or subscription is often required to unlock the complete range of features. In this case, we recommend this model for businesses looking to adopt an LLM that excels in conversational dialogue, multi-step reasoning, efficient computation, and real-time interactions without the constraints of a budget.\\nFor companies who are curious to try out the proprietary models on the market before fully committing to one due to budget constraints or uncertainties about its long-term integration, Shakudo offers a compelling alternative. Our platform currently features a diverse selection of advanced LLMs with simplified deployment and scalability. With a simple subscription, you can access and assess the value of proprietary models, like GPT, before making a substantial investment.\\n2.DeepSeek\\n\\nDeepseek-R1 Benchmark. Source: deepseek.com\\nWith its latest R1 model, the Chinese AI company DeepSeek has once again set new benchmarks for innovation in the AI community. As of January 24th, the DeepSeek-R1 model is ranked fourth on Chatbot Arena, and top as the best open-source LM.\\nThe DeepSeek-R1 is a 671B parameter Mixture-of-Experts (MoE) model with 37B activated parameters per token, trained through large-scale reinforcement learning with a strong focus on reasoning capabilities. The model excels at understanding and handling long-form content and demonstrates superior performance in complex tasks such as mathematics and code generation. The model is approximately 30 times more cost-efficient than OpenAI-o1 and 5 times faster, offering groundbreaking performance at a fraction of the cost. Moreover, it has shown exceptional precision in tasks requiring complex pattern recognition, such as genomic data analysis, medical imaging, and large-scale scientific simulations.\\nDeepSeek-R1’s capabilities are transformative when it comes to integration with proprietary enterprise data such as PII and financial records. Leveraging retrieval-augmented generation (RAG), enterprises can connect the model to their internal data sources to enable highly personalized, context-aware interactions—all while maintaining stringent security and compliance standards. With Shakudo, you can streamline the deployment and integration of advanced AI models like DeepSeek by automating the setup, deployment, and management processes. This eliminates the need for businesses to invest in and maintain extensive computing infrastructure. By operating within your existing infrastructure, the platform ensures seamless integration, enhanced security, and optimal performance without requiring significant in-house resources or specialized expertise.\\n3. Qwen\\n\\n\\u200d\\nAlibaba released Qwen2.5-Max earlier last month, the new model designed to deliver enhanced performance for large-scale natural language processing tasks. In instruct model evaluations, this new model outperforms DeepSeek V3 in benchmarks such as Arena-Hard, LiveBench, LiveCodeBench, and GPQA-Diamond, while also delivering strong performance in other assessments like MMLU-Pro.\\nQwen2.5-Max is pretrained on over 20 trillion tokens. While specific details about its parameter count and token window size are not publicly disclosed, Qwen2.5-Max is designed for low-latency, high-efficiency tasks, making it suitable for applications requiring quick, accurate responses with low latency. Its smaller size allows for deployment on devices with limited computational resources.\\nFor businesses and users looking for higher-performance models for natural language processing and AI-driven tasks, the Qwen 2.5 model is available on platforms like Hugging Face and ModelScope. This model boasts from 0.5 billion to 72 billion parameters, featuring context windows of up to 128,000 tokens, and is excellent for code generation, debugging, and automated forecasting.\\n4. LG AI\\n\\nsource\\nEXAONE 3.0 is a bilingual LLM developed by LG AI Research. The company released its latest model in December 2024 with excellent performance across various benchmarks and real-world applications.\\nWith 7.8 billion parameters, EXAONE 3.0 is capable of understanding and generating human-like text in multiple languages across complex domains, including coding, mathematics, patents, and chemistry. This model has also been optimized to reduce inference processing time by 56%, memory usage by 35%, and operating costs by 72%, ensuring that it remains cost-effective while maintaining high performance.\\nLG AI Research has open-sourced the 7.8B parameter instruction-tuned version of EXAONE 3.0 for non-commercial research purposes. This is a model we recommend to software companies or tech startups for generating Python code, assisting developers in troubleshooting, and creating APIs or other backend components.\\n5. LlaMA\\n\\nMeta is still leading the front with their state-of-the-art LlaMa models. The company released its latest LlaMA 3.3 model in December 2024, featuring multimodal capabilities that can process both text and image for in-depth analysis and response generation, such as interpreting charts, maps, or translating texts identified in an image.\\nLlaMA 3.3 improves on previous models with a longer context window of up to 128,000 tokens and an optimized transformer architecture. With a parameter of 70 billion, this model outperforms open-source and proprietary alternatives in areas such as multilingual dialogue, reasoning, and coding.\\nUnlike ChatGPT models, LlaMA 3 is open-source, giving users the flexibility to access and deploy freely on their cloud depending on the specific requirements of their infrastructure, security preferences, or customization needs. We recommend this model to businesses looking for advanced content generation and language understanding, such as those in customer service, education, marketing, and consumer markets. The openness of these models also allows for your greater control over the model’s performance, tuning, and integration into existing workflows.\\n6. Claude\\n\\nNext on our list is Claude, more specifically, the latest Claude 3.5 Sonnet model developed by Anthropic. We believe that Claude is arguably one of the most significant competitors to GPT since all of its current models–Claude 3 Haiku, Claude 3.5 Sonnet, and Claude 3 Opus–are designed with incredible contextual understanding capabilities that position themselves as the top conversational AI closely aligned with nuanced human interactions.\\nWhile the specific parameters of Claude 3.5 Sonnet remain undisclosed, the model boasts an impressive context window of 200,000 tokens, equivalent to approximately 150,000 words or 300 pages of text.\\nThe current Claude subscription service is credit-based, and the cost can go as high as $2,304/month for enterprise plans tailored to high-volume users. We recommend Claude to mature or mid-stage businesses looking not only to adopt an AI that facilitates human-like interactions but also to enhance their coding capabilities since the Claude 3.5 Sonnet model is currently reaching a 49.0% performance score on the SWE-bench Verified benchmark, placing it as third among all publicly available models, including reasoning models and systems specifically designed for agentic coding.\\n7. Mistral\\n\\nMistral\\'s latest model – Mistral Small 3, a latency-optimized model was released under the Apache 2.0 license at the end of January. This 24-billion-parameter model is designed for low-latency, high-efficiency tasks. It processes approximately 150 tokens per second, making it over three times faster than Llama 3.3 70B on the same hardware.\\nThis new model is ideal for applications requiring quick, accurate responses with low latency, such as virtual assistants, real-time data processing, and on-device command and control. Its smaller size allows for deployment on devices with limited computational resources.\\nMistral Small 3 is currently open-source under the Apache 2.0 license. This means you can freely access and use the model for your own applications, provided you comply with the license terms. Since it is designed to be easily deployable, including on hardware with limited resources like a single GPU or even a MacBook with 32GB RAM, we\\'d recommend this to early-stage businesses looking to implement low-latency AI solutions without the need for extensive hardware infrastructure.\\n8. Gemini\\n\\nGemini is a family of closed-source LLM models developed by Google. The latest model—Gemini 2.0 Flash—operates at twice the speed of Gemini 1.5 Pro, offering substantial improvements in speed, reasoning, and multimodal processing capabilities.\\nWith that being said, Gemini remains a proprietary model; if your company deals with sensitive or confidential data regularly, you might be concerned about sending it to external servers due to security reasons. To address this concern, we recommend that you double-check vendor compliance regulations to ensure data privacy and security standards are met, such as adherence to GDPR, HIPAA, or other relevant data protection laws.\\nIf you’re looking for an open-source alternative that exhibits capabilities almost as good as Gemini, Google’s latest Gemma model, Gemma 2, offers three models available in 2 billion, 9 billion, and 27 billion parameters with a context window of 8,200. For businesses looking for a rather economic option, this is the optimal choice that interprets and understands messages with remarkable accuracy.\\n9. Command\\n\\nCommand R is a family of scalable models developed by Cohere with the goal of balancing high performance with strong accuracy, just like Claude. Both the Command R and Command R+ models offer APIs specifically optimized for Retrieval Augmented Generation (RAG). This means that these models can combine large-scale language generation with real-time information retrieval techniques for much more contextually aware outputs.\\nCurrently, the Command R+ model boasts 104 billion parameters and offers an industry-leading 128,000 token context window for enhanced long-form processing and multi-turn conversation capabilities.\\nOne of the perks of working with an open-source model is also to avoid vendor lock-in. Heavy reliance on a particular type of proprietary model may make it difficult for you to switch to alternative models when your business starts growing or the landscape changes. Cohere approaches this in a hybrid way, meaning that you can access and modify the model for personal usage but need a license for commercial use. In this case, we recommend this model for businesses that want flexibility in experimentation without a long-term commitment to a single vendor.\\nExplore more from Shakudo\\n How VPCs Enable AI Deployments with a Modern Data Stack Insights January 28, 2025\\n The Power of Simple Questions: How to Choose the Right Natural Language to SQL Query Tool Insights May 15, 2024\\n Bring Data and AI tooling right to MongoDB Atlas with Shakudo News August 26, 2024\\nTake the next step\\n\"Shakudo gave us the flexibility to use the data stack components that fit our needs and evolve the stack to keep up with the industry.\"\\n\\nNeal Gilmore\\nSenior Vice President, Enterprise Data & Analytics\\nDiscover Shakudo\\n\\nShakudo brings the best data and AI products into your VPC and operates them for you automatically achieving a more reliable, performant, and cost effective data stack than ever before.\\n\\n Book Demo Email X (Twitter) Linkedin Youtube\\nNewsletter\\nSign up for the latest Shakudo news:\\n🎉 Success! You\\'re now signed up for the Shakudo newsletter.\\nOops! Something went wrong while submitting the form.\\nApplications\\nData and AI OSStack ComponentsLanguage to SQLVector Database + LLMReverse ETLWorkflow Automation\\nIndustries\\nAutomotive & Transportation\\nAerospace\\nManufacturing\\nHigher Education\\nHealthcare & Life Sciences\\nClimate & Energy\\nTechnology & Software\\nSports\\nReal Estate\\nRetail\\nFinancial Services\\nResources\\nUse Cases\\nInsights\\nWhite Paper\\nCase Study\\nPress\\nProduct\\nTutorial\\nNews\\nWebinarGlossaryDocumentation\\nCompany\\nAboutPartnersDGX PartnerCareersMedia Kit\\nGet Started\\nSignupContact UsNewsletter\\n© 2025 Shakudo\\nToronto, CA\\nContact usPrivacy PolicyTerms/ConditionsSitemap\\nTrusted by industry leaders\\n\\n\\n\\n\\n\\n\\nSee Shakudo in Action  \\nWatch the 3 Minute Demo\\n\\nThis field is required\\n\\nFor information about how Shakudo handles your personal data, please see our Privacy Policy.\\nThank you for your submission. A Shakudo expert will be in touch with you shortly.  \\nIn the meantime, feel free to check out our data insights, case studies, and latest industry news that help data teams win.  \\n Live chat Live chat will provide the quickest answer to any of your questions.\\nOops! Something went wrong while submitting the form.\\n⨉', \"## Title\\n\\nThe best large language models (LLMs) in 2025 - Zapier\\n\\n## Content\\n\\nThe best large language models (LLMs)\\nSkip to content\\nLog inSign up\\nBlog\\nCategories\\n\\n\\nApp picks\\n\\nAll articles\\nBest apps\\nApp of the day\\nApp comparisons\\n\\n\\n\\nAutomation with Zapier\\n\\nAll articles\\nAutomation inspiration\\nZapier tutorials\\nZapier feature guides\\nCustomer stories\\n\\n\\n\\nProductivity\\n\\nAll articles\\nProductivity tips\\nApp tips\\nApp tutorials\\n\\n\\n\\nBusiness growth\\n\\nAll articles\\nMarketing tips\\nBusiness tips\\n\\n\\n\\nProduct & platform\\n\\nAll articles\\nPartner case studies\\nProduct news\\nPlatform tips\\n\\n\\n\\nCompany updates\\n\\nAll articles\\nCompany news\\nZapier initiatives\\n\\n\\n\\nRemote work\\n\\nAll articles\\nRemote work tips\\nHow we work at Zapier\\n\\n\\n\\nZapier guides\\n\\n\\nApp picks\\n\\nAll articles\\nBest apps\\nApp of the day\\nApp comparisons\\n\\n\\n\\nAutomation with Zapier\\n\\nAll articles\\nAutomation inspiration\\nZapier tutorials\\nZapier feature guides\\nCustomer stories\\n\\n\\n\\nProductivity\\n\\nAll articles\\nProductivity tips\\nApp tips\\nApp tutorials\\n\\n\\n\\nBusiness growth\\n\\nAll articles\\nMarketing tips\\nBusiness tips\\n\\n\\n\\nProduct & platform\\n\\nAll articles\\nPartner case studies\\nProduct news\\nPlatform tips\\n\\n\\n\\nCompany updates\\n\\nAll articles\\nCompany news\\nZapier initiatives\\n\\n\\n\\nRemote work\\n\\nAll articles\\nRemote work tips\\nHow we work at Zapier\\n\\n\\n\\nZapier guides\\n\\n\\nHome\\n\\n\\nProductivity\\n\\nApp tips\\n\\nApp tips13 min read\\nThe best large language models (LLMs)\\nThese are the most significant, interesting, and popular LLMs you can use right now.\\nBy Harry Guinness · August 5, 2024\\n\\n\\nGet productivity tips delivered straight to your inbox\\nSubscribe\\nWe’ll email you 1-3 times per week—and never share your information.\\n Harry Guinness Harry Guinness is a writer and photographer from Dublin, Ireland. His writing has appeared in the New York Times, Lifehacker, the Irish Examiner, and How-To Geek. His photos have been published on hundreds of sites—mostly without his permission.\\ntags\\n* Artificial intelligence (AI)\\nmentioned apps\\n* Anthropic (Claude)\\n* ChatGPT\\n* OpenAI\\n* Google Vertex AI\\n* Google AI Studio\\nRelated articles\\n\\n\\n\\nApp tipsWhat is DeepSeek and why does it matter? What is DeepSeek and why does it matter?\\n\\n\\n\\nApp tipsWhat are OpenAI o1 and o3-mini? And how do they compare to GPT-4o? What are OpenAI o1 and o3-mini? And how do...\\n\\n\\n\\nBest appsThe 6 best AI app builders in 2025 The 6 best AI app builders in 2025\\n\\n\\n\\nApp tipsWhat are Claude computer use and ChatGPT Operator? What are Claude computer use and ChatGPT...\\n\\n\\n\\nApp comparisonsClaude vs. ChatGPT: What's the difference? [2025] Claude vs. ChatGPT: What's the difference?...\\n\\n\\n\\nProduct newsIntroducing Zapier Agents: AI agents that automate work across your apps Introducing Zapier Agents: AI agents that...\\n\\n\\n\\nZapier tutorialsBuild an AI job search agent with Zapier Agents Build an AI job search agent with Zapier...\\n\\n\\n\\nAutomation inspirationWhy pairing AI with automation will change how you work Why pairing AI with automation will change...\\n\\n\\nImprove your productivity automatically. Use Zapier to get your apps working together.\\nSign up\\nSee how Zapier works\\n\\nFollow us\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPricing\\n\\nHelp\\nDeveloper Platform\\nPress\\nJobs\\nEnterprise\\nTemplates\\nApp Integrations\\n\\n\\n© 2025 Zapier Inc.\\nManage cookies\\n\\nLegal\\nPrivacy\\n\"]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tools"
      ],
      "metadata": {
        "id": "yJXO6NaNKVQk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fe2bbcf-8349-4540-ec0f-1b6f30d933d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[StructuredTool(name='multiply', description='use to multiply numbers', args_schema=<class '__main__.CalculatorInput'>, return_direct=True, func=<function multiply at 0x7fa0f8b6d8a0>),\n",
              " StructuredTool(name='search_web_extract_info', description='Search the web for a query. Userful for general information or general news', args_schema=<class 'langchain_core.utils.pydantic.search_web_extract_info'>, func=<function search_web_extract_info at 0x7fa0f8a223e0>),\n",
              " StructuredTool(name='get_weather', description='Search weatherapi to get the current weather.', args_schema=<class 'langchain_core.utils.pydantic.get_weather'>, func=<function get_weather at 0x7fa0f8a220c0>)]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    }
  ]
}